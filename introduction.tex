\chapter{INTRODUCTION}
\label{chap:introduction}

Mobile computing is the preferred method of personal computing
for millions of users.
%
Furthermore, the mobile device market has grown by over 400 million smartphones shipped
in 2011, and an estimated prediction of 1 billion shipments for
2015~\cite{dignan}.
%
This growth has been fueled by the hundreds of thousands of mobile
applications such as social networking, location-based services, image
processing, augmented reality, face and speech recognition.
%
In order to meet the increasing demands of these computationally-intense
applications, mobile platforms have been augmented with multi-core CPUs,
more powerful GPUs, and other specialized hardware accelerators.
%
Despite these many enhancements to the mobile platform's hardware,
the inescapable fact is that their limited batteries and small form
factor will be a bottleneck while hindering the mobile platforms from
utilizing their computing capabilities.\\
%
To address this issue, there have been research efforts in two different
areas: 1) heterogeneous computing, and 2) computation offloading.
%
Heterogeneous computing has been seen as a mechanism to increase the
dynamic range of executions by dynamically combining computing elements
with various power-performance characteristics, and opportunistically scheduling the
workloads onto the optimal computing element based on the workload
requirements and energy availability~\cite{chen}.
%
Heterogeneous architectures with big/small cores, CPU/GPU, and
CPU/hardware accelerators are now being offered by leading
processor/SoC (System on a Chip) vendors~\cite{bigprocessing, tegra}.
%
By allowing dynamic workload assignment onto the best processing
element, these architectures provide a wide dynamic range of executions
while improving performance when needed and optimizing for energy
performance.\\
%
In addition, computation offloading has been proposed as an alternative
for extending the capabilities of mobile platforms.
%
Since mobile networks have less throughput, and can exhibit higher
latencies and variances, computation offloading in mobile environments
is more constrained than previously studied cyber-foraging~\cite{cyber}
techniques in traditional computing systems.
%
In the past few years, there have been various mobile offloading
techniques proposed in the context of constrained mobile environments,
from application partitioning~\cite{maui} or process
migration~\cite{hung} to full mobile environment
cloning~\cite{clonecloud}.
%
Other works have also explored the implications of offloading to
different environments including the public cloud (i.e. Amazon
EC2)~\cite{hung}, to within private networks~\cite{shigeru}, and even to
other mobile devices~\cite{serendipity}.
%
All of these efforts seek for intelligent ways to enable mobile device
developers to leverage the computing capabilities of more powerful
external computing resources over the network.
%
Even though existing approaches provide core mechanisms to transform
typical mobile applications to offloading-enabled applications through
various granularities of partitioning and/or migration, they still lack
service discovery mechanisms while assuming the availability of remote
computing nodes with static endpoints.
%
Moreover, data privacy and secure communication between the mobile
client and remote resources are of increasing importance for secure
computing on mobile environments.\\
%
This dissertation presents a novel framework which addresses the challenge
of remote computation offloading by enabling mobile workload offloading
to external resources within a user-defined virtual private network.
%
For doing this, I establish \textit{Social Device Network} which is a social
virtual private network defined by the mobile user.
%
In this dissertation, Social Device Network is the user-defined virtual
private network in which trusted remote computing resources from
socially related peers (e.g. friend's workstation or family's laptop)
are aggregated in a virtual network regardless of user mobility.
%
On top of Social Device Network, I advocate a dynamic, distributed
approach for resource management where eligible computing resources are
discovered at runtime, allowing a more flexible design for distributed
computing environments.
%
By using an IP multicast-based discovery mechanism, the system
periodically locates remote computing resources available within its
private networks.
%
Furthermore, utilizing the virtual private networking technique as a
substrate for service management resolves concerns for data privacy and
secure communication between the mobile client and remore computing
resources due to its security capabilities such as packet tunneling and
encryption.\\
%
As part of this work, I also developed a remote offloading framework by
extending OpenCL framework, which is an open standard of parallel
programming for heterogeneous computing environments, to support remote
offloading using the TCP/IP networking stack.
%
Heterogeneous computing uses specialized hardware accelerators to
increase computing throughput and performance.
%
As a result, with the OpenCL-based remote offloading framework, I combine the
power of cloud offloading with the flexibility of
heterogeneous architectures to provide an end-to-end heterogeneous
architecture which expands the dynamic execution range of mobile
platforms, which are typically limited in available energy resources.
%
For example, many data centers and public cloud providers are
integrating GPUs in their structure~\cite{bowman}, and software
designers are able to get better processing performance by offloading
more computation to the on-board GPUs than the CPUs.
%
Heterogeneous computing is not just restricted to the data center or to
cloud providers; it is also available in most personal computing
devices (i.e. workstations, laptops, and mobile devices) of the common
user.
%
Some examples are web browsers utilizing GPUs on a user's machine for
HTML rendering~\cite{paul} or developers using mobile GPUs for face
recognition processing~\cite{burns}.
%
Since heterogeneous computing essentially offloads workloads to
different accelerators on the host platform, it is natural
to extend this concept to support accessing computing components that are not
on-board, but rather remote resources across the network.
%
%The proposed approach therefore is a remote offloading framework which
%broadens the range of heterogeneous computing to remote accelerators.
%
%I accomplish this by extending the OpenCL framework to support 1)
%service discovery and configuration using \lq\lq Social Device
%Network\rq\rq\  and 2) remote offloading over the TCP/IP networking stack
%and virtual networks.
%
OpenCL is a framework specifically designed to offload workloads in
heterogeneous platforms, but it currently does not support access to
accelerators across the network.
%
Hence, one contribution of this dissertation is the ability to remotely
offload and execute OpenCL kernels from a mobile device to either nearby
computing nodes or to cloud resources.
%
Although the proposed OpenCL-based remote offloading framework for mobile
platforms is not the first to propose the concept of offloading
heterogeneous workloads over the network (e.g. Remote CUDA~\cite{rcuda},
Virtual OpenCL~\cite{vocl}), the proposed design is the first to
consider an approach where offloading happens between a mobile device and a virtual
machine instance running in the cloud, or a trusted workstation in the
local network or across the Internet.\\
%
In the heterogeneous computing environment, a developer uses the OpenCL
API to discover the local hardware accelerators available for
computation.
%
The developers are able to offload computation to one or more of these
local accelerators through the OpenCL framework.
%
The proposed framework makes it possible for the developer to
dynamically discover accelerators located on remote computing resources,
even in the cloud.
%
For example, an application may query the OpenCL platform for available
accelerators, and the framework returns a virtual accelerator that
actually represents a graphics card running on Amazon EC2.
%
The application then seamlessly offloads a computation to this virtual
accelerator as if it were a regular accelerator located on-board
the local mobile device.
%
The proposed framework automatically offloads the computation to the
cloud, manages the transfer of necessary state over private network
links, and returns the result back to the application.
%
By integrating the design with the OpenCL offloading framework,
developers are not required to learn a new API.
%
The prototype implementation of the proposed offloading framework is evaluated,
with respect to end-to-end application performance and energy
consumption in mobile devices, through a variety of network
configurations representing local and wide area networks, and various
levels of remote computing capabilities such as typical CPUs, GPUs, as
well as Amazon EC2 instances.\\
%
%The second contribution is a distributed method of resource management
%which handles resource discovery, access control, and data privacy.
%
%Past mobile offloading solutions have not investigated a resource
%discovery mechanism and assume the availability of remote computing
%resources with fixed endpoints.
%
%I advocate a dynamic approach where eligible computing resources are
%discovered at runtime, allowing a more flexible design for distributed
%computing environments.
%
%By using an IP multicast-based discovery mechanism, the system periodically locates
%remote computing resources available within its private networks.\\
%
I also investigated the conditions where offloading is more beneficial
than local processing through experiments considering four types of OpenCL workloads:
sobelfilter, matrix multiplication, hidden Markov model, and {\it N}-body
physics. 
%
I use Computation to Communication ratio for those
workloads as a criterion for determining the effectiveness of remote
offloading.
%
In this work, computation to communication ratio for each workload is a
comprehensive measurement which consolidates three parameters such as the
computation complexity of workloads, the amount of data to be
transferred, and the network bandwidth.
%
The experiments show that it is more beneficial to offload computations
in cases of high computation to communication ratio, but for cases of
low computation to communication ratio, it is not always efficient to
perform remote offloading.\\
%
In addition, this dissertation proposes machine learning
techniques to runtime schedulers for mobile offloading framework.
%
By adopting machine learning techniques to remote offloading, a
scheduler can be automatically trained from previous offloading
performance measurements and make decisions on whether mobile workloads
should be offloaded or executed locally according to past behaviors and
current conditions.
%
While running various machine learning algorithms, the evaluation shows the
feasibility of adopting machine learning techniques into scheduling
problems for mobile offloading framework.
%
This dissertation studies both an offline machine
learning-based runtime scheduler and online machine learning techniques.
%
The latter provides an online training mechanism for the machine
learning-based runtime scheduler such that it supports a policy that
dynamically adapts scheduling decisions at runtime based upon the
observation of previous offloading decisions and their correctness.
%
To demonstrate its practical applicability, this work integrates the
online training machine learning-based mobile offloading scheduler with
an existing Java-based, offloading-capable code refactoring framework.\\ 
%
Lastly, I introduce SOLARE, a general-purpose peer-to-peer, utility
function-based self-organizing network latency-aware clustering system.
%
SOLARE allows nodes to self-organize in P2P clusters based on a
latency-based synthetic coordinate space, which can lead to possible
performance improvement (with respect to network latency) in conjunction
with mobile offloading framework by clustering a device with nearby
offloading candidates. 
%
In SOLARE, each node searches and joins the highest utility valued
cluster, and periodically monitors the status of the cluster that it is
currently involved in so that the node migrates to another cluster
whenever the utility is dropped below a threshold.
%
For this work, I select the distance between a virtual center of the
cluster and the coordinate of each node and the size of the cluster
(i.e. the number of current cluster members) as utility properties.
%
Even though mobile devices may not directly participate in the SOLARE
clustering process (such as searching, joining, monitoring, and creating
a cluster), it is possible that mobile devices assisted by SOLARE
are able to achieve an additional offloading performance improvement by
keeping a list of remote resources which guarantee similar network
performance in terms of latency (i.e. remote resources
having the same cluster membership) and seamlessly hopping into another
remote resource in the same cluster when a remote resource becomes
unavailable (e.g. server crashes and network partitions).
%

\section{Motivation}
\label{intro:motivation}
This section describes a few scenarios that motivate the proposed
approach.
%
Figure~\ref{fig:motivation} gives a general idea of one example deployment scenario.
%
Alice connects her smartphone to a virtual private network (VPN)
consisting of her laptop, Bob's desktop, and her virtual machine
instance running on Amazon EC2.
%
Since each of these devices is running SocialVPN~\cite{socialvpn}, they
automatically join the same virtual private network creating a pool of
trusted resources in Social Device Network.
%
With this secure IP layer consisting of trusted peers, the framework is
able to use IP multicasting over the VPN to discovery nodes that are
available for computation offloading.
%
During the discovery process, the system records the characteristics of
each node in the network, such as bandwidth, latency, and processing
capabilities.
%
When an application decides to offload some computation, the framework
dynamically determines the best node to use as the remote offloading
target.
%
In many cases, for example, if the bandwidth or remote processing
capabilities are too low, the framework may decide to simply run the
workload locally.\\
%
The goal of the design is to provide an intuitive offloading framework
that developers can integrate into their application using well-adopted
programming concepts.
%
Currently, many software developers utilize the OpenCL framework to
exploit on-board heterogeneous platforms.
%
In the scientific computing arena, supercomputers increasingly integrate
CPUs and GPUs in order to maximize performance/Watt~\cite{powertutor}.
%
Popular software projects, such as OpenCV and OpenSSL, are
re-implementing major portions of their functions to run on the OpenCL
platform.
%
Mobile SoC platforms, based on processors such as ARM and Intel, are
also starting to provide OpenCL support on their architecture.
%
The latest version of the OpenCL specification allows for devices
beyond CPUs and GPUs to be accesses through the API.
%
There is also an industry momentum building up behind OpenCL with the
formation of new industry foundations to foster fast
adoption~\cite{hsa}.
%
These considerations point to OpenCL API as the emerging defacto
standard for heterogeneous computing.\\
%
\begin{figure}
\centering
\epsfig{file=figs/motivation.eps, width=5.5in}
\caption{Private networking and node selection}
\label{fig:motivation}
\end{figure}
%
As another example of how a developer might take advantage of
OpenCL and the proposed platform in a seamless fashion, consider a
typical facial recognition application in a mobile device
used as a security feature, or for tagging friends on a social
networking application.
%
First, the developer utilizes a CPU implementation.
%
However, the developer quickly realizes the processing limitations of
doing image processing on the CPU and decides that such a workload is
better suited for the GPU or another hardware-based accelerator.
%
With this realization, the developer writes an OpenCL-based
implementation due to its wide support and adoption.
%
Using OpenCL, the developer is able to offload the computation from the
mobile CPU to the mobile GPU and therefore achieve better performance
with less battery consumption.\\
%
The proposed framework aims to extend the reach of heterogeneous
computing to include devices beyond the physical host platform.
%
By recompiling the application to link with the proposed framework, the
developer can transparently access remote resources available via the
SocialVPN, including GPUs running on computing resources more powerful
than a mobile device.
%
For instance, if the mobile device is connected to a virtual network
consisting of an Amazon EC2 GPU instance and the user's personal
workstation, the extensions to the OpenCL framework will automatically
select the best candidate based on available device capabilities, and
network conditions as the target compute node for remote execution.
%
Also, the use of SocialVPN, ensures that computation is offloaded
securely to socially trusted nodes.
%
This enhancement occurs transparently to the developer and the user
requiring only code recompilation. 
%

\section{Related Works on Remote Offloading Systems for Mobile Platforms}
\label{intro:relatedwork}

The research community has been investigating different methods to
offload computation for decades.
%
However, remote execution to the cloud has created new opportunities to
explore novel offloading solutions.
%
This section discusses the most recent proposals for mobile computation
offloading, which fall in the following categories: application
partitioning, thread or application migration, and distributed
offloading framework.
%
\subsection{Application Partitioning}
\label{intro:app_partitioning}
%
This approach involves selecting portions of an application to execute
remotely through the use of a static or dynamic scheduler.
%
In Spectra~\cite{spectra}, developers identify functions in the
application that  can be offloaded to a remote server over RPC.
%
By monitoring the CPU, file system, and bandwidth, Spectra dynamically
decides at runtime which portions of the application should run locally
or remotely.
%
MAUI~\cite{maui} takes a similar approach but alleviates the process by
using many of programming features in the .NET platform such as method
attributes, and the Reflection API.
%
Through the .NET Framework's virtual machine, MAUI is able to
dynamically serialize and ship \textit{remotable} methods and data to a
server proxy, thus leveraging the server's superior processing
capabilities while saving power on the mobile device.
%
Cuckoo~\cite{cuckoo} takes a slightly modified approach by focusing more
on integrating with the Eclipse IDE.
%
However, it requires developers to implement both local and remote
versions of their functionality, whereas MAUI only requires annotations
instead of a different implementation.
%
At runtime, Cuckoo does intelligent offloading by determining the
appropriate cases to run the code locally or remotely.
%
The approach of this dissertation may be classified as application partitioning
similar to Cuckoo because developers do not have to worry about the
complications of shipping the workload to the remote device.
%
\subsection{Thread Migration}
\label{intro:thr_migration}
%
The source code modification required for most partitioning schemes can
preclude adoption by many applications.
On the other hand, thread and process migration can be achieved without
any source code modification.
%
CloneCloud~\cite{clonecloud} achieves this by  employing thread
migration in the Dalvik Java Virtual Machine (JVM) by transferring all of
the thread state (thread stack, necessary heap objects and registers) to
the remote virtual machine.
%
When the remote thread completes, the results are merged back with the
local Dalvik JVM memory stack.
%
The authors of COMET~\cite{comet} developed a similar thread migration
technique by doing application VM synchronization through a distributed
shared memory (DSM) model.
%
The approach of this dissertation does not require any thread stack or heap
synchronization because the OpenCL framework requires explicit
declaration of input and output buffers for remote kernel execution.
%
Hence, the use of OpenCL alleviates the complexities of memory
synchronization since all of the necessary state is encapsulated as
contiguous memory arrays that are managed through a few OpenCL data
offloading functions (i.e. \textit{clEnqueueReadBeffer},
\textit{clEnqueueWriteBuffer}, \textit{clEnqueueMapBuffer}).
%
\subsection{Application Migration}
\label{intro:app_migration}
%
The previous thread migration techniques can be technically challenging
to implement since they require memory synchronization between the
remote thread and other threads running locally.
%
All local threads have to block on dirty region of the heap that has
been offloaded to the remote server until the remote execution finishes
and the memory is merged and released.
%
Application migration does not have such requirements.
%
Hung et al.~\cite{hung} describes an application migration design that
leverages the \textit{onResume} and \textit{onPause} events of an Android
application as the markers for process migration.
%
The \textit{onPause} event occurs when a user switches to another
application.
%
The Android system requires that application states are saved on
persistent storage in the case the operating system decides to shutdown
because of low memory.
%
Hence, Hung et al. create a solution which uses the \textit{onPause}
event to force the application to save its state.
%
The state is then copied to a cloned VM running on the cloud, resumed
there until completion, then transferred back.
%
The design of this dissertation automatically handles the state transfers between
the local and remote devices without relying on specific Android-based
events.
%
\subsection{Distributed Offloading Framework}
\label{intro:framework}
%
Various recent approaches have focused on a totally different model
requiring more effort from the developers.
%
Proposals such as Mobile Map Reduce (MMR)~\cite{mmr},
Sonora~\cite{sonora}, Serendipity~\cite{serendipity}, and
ThinkAir~\cite{thinkair} expose a distributed offloading framework for
developers to adopt.
%
For example, MMR is a MapReduce system optimized for the constrained
networking conditions of mobile devices by taking into account
bandwidth and latency for efficient mobile device performance.
%
Sonora exposes a distributed stream-based programming model which
handles workload distribution and failures in a mobile network.
%
Serendipity provides an offloading framework for intermittently
connected mobile devices and does not rely on cloud services.\\
%
The approach of this dissertation can be also classified as a distributed offloading
framework.
%
Instead of defining the system from scratch, however, the proposed
framework reuses the workload offloading paradigms of the OpenCL
framework which provides a more familiar and widely supported interface
for developers.
%
There also exist other heterogeneous offloading frameworks which are
quite similar to the proposed approach.
%
Remote CUDA~\cite{rcuda} is one approach that extends the NVIDIA CUDA
API to support remote offloading over the network.
%
Their goal is to minimize the network overhead and they analyze the
impact of using different networking technologies, such as Gigabit
Ethernet or InfiniBand.
%
Their research is aimed at cluster environments, such as a data center,
and it does not consider mobile environments with low bandwidth.
%
Virtual OpenCL\cite{vocl} provides a similar solution which uses
OpenCL instead of the proprietary CUDA protocol.
%
They also focus on cluster environments, but they leverage the MPI
library for memory and workload synchronization.
%
Since our solution is designed for mobile platforms and the cloud, it
differs greatly because it takes into account not only performance, but
also energy consumption, connectivity, and mobility.
%
\subsection{Lack of Privacy and Trust}
\label{intro:lack}
%
Mobile devices tend to process more private information about their
users.
%
Hence, dealing with trust and privacy is a fundamental requirement.
%
None of these previous works thoroughly deal with the issue of securing
the offloaded data over trusted channels, to trusted devices.
%
They also do not address the issue of verifying the integrity of the
results provided by the remote cloud server.
%
Hung et al. mentioned using L2TP VPN for security, but such as approach
does not ensure privacy among virtual resources within the cloud which
is a growing concern for many cloud applications~\cite{brodkin}.
%
Private communication and trusted results are addressed in this
dissertation through the use of a social peer-to-peer virtual
private network, SocialVPN~\cite{socialvpn} . 
%
The proposed approach guarantees that the user data is only offloaded to
trusted compute nodes and verified through non-repudiable, encrypted
channels.
%
I also analyze the cost associated with providing this privacy because
encryption on a mobile device is non-negligible; none of previous
approaches measured this cost.
%
UIA~\cite{uia} has considered ad-hoc virtual private networks connecting
mobile devices of social peers, and is closely related to SocialVPN, but
has not been evaluated in the context of computation offloading.\\
%
Apart from the aforementioned offload solutions, there has been related
research in the area of heterogeneity, which deals with
combining processing elements of varying power performance capabilities
to the local processor/platform.
%
Combining small/big cores, GPUs, and special purpose hardware and
scheduling this combination based on workload requirement provide a
wider dynamic execution range.
%

\section{Contributions}
\label{intro:contributions}
The key contributions of this dissertation can be summarized as
follows:\\
%
{\bf Novel framework for remote computation offloading.} The primary
contribution is a novel framework which addresses the challenge of
remote computation offloading to resources in the cloud through the paradigm of
extended hardware-layer heterogeneous computing.
%
Heterogeneous computing uses specialized hardware accelerators to
increase computing throughput and performance.
%
The proposed framework combines the power of cloud offloading with the
flexibility of heterogeneous architecture which expands the dynamic
execution range of mobile platforms, which are typically restricted by
power constraints.\\
%
{\bf Decentralized resource discovery mechanism.} Second
contribution of this proposal is a distributed method of resource
management which dynamically handles service discovery.
%
Previous mobile offloading solutions have not investigated a service
discovery mechanism by assuming the availability of remote computing
resources with static endpoints.
%
The proposed framework advocates a dynamic approach where candidate
computing nodes are discovered at runtime while allowing for a more
flexible design.
%
By using IP multicast-based discovery, the proposed system periodically
locates computing nodes which are available within their social device
network.\\
%
{\bf Workloads characterization using computation to communication
ratio.} 
%
I characterize mobile workloads for the suitability of offloading from
the perspective of {\it Computation to Communication ratio}, which is
calculated as the time for workloads to be executed locally, divided by
the data transfer time for remote offloading.
%
Thus, in this work, computation to communication ratio for mobile
workloads is a comprehensive measurement which consolidates three dynamic
parameters: the computation complexity of workloads, the amount of
data to be transferred, and the network bandwidth.\\
%
{\bf Machine learning-based runtime scheduler.} Prior studies have
primarily focused on core mechanisms for offloading.
%
However, adaptive scheduling in such system is important because
offloading effectiveness can be influenced by varying network
conditions, workload requirements, and load at the target device.
%
In this dissertation, a study on the feasibility of applying machine
learning techniques to address the adaptive scheduling problem in mobile
offloading framework is presented as a third contribution.
%
By taking the algorithm complexity and scheduling performance into
account, a few machine learning algorithms are selected to implement
offline runtime schedulers for mobile offloading framework.
%
Furthermore, I extend the offline machine learning-based runtime
scheduler to the mobile offloading scheduler with online machine
learning techniques.
%
This extension provides an online training mechanism for the machine
learning-based runtime scheduler such that it supports a policy that
dynamically adapts scheduling decisions at runtime based upon the
observation of previous offloading decision and their correctness.\\
%
{\bf Self-organizing latency-aware clustering system.} I introduce
SOLARE, a general-purpose peer-to-peer, utility function-based
self-organizing network latency-aware clustering system.
%
SOLARE allows nodes to self-organize in P2P clusters based on a network
latency-based synthetic coordinate space, which can lead to possible
performance improvement (with respect to network latency) in conjunction
with mobile offloading framework by clustering a device with nearby
remote computing resources.\\ 
%
The prototype implementation of the proposed framework is evaluated,
with respect to end-to-end application performance and energy consumption
in mobile devices, through a variety of network configurations
representing local and wide area network, and various levels of remote
computing capabilities such as typical CPUs, GPUs, as well as Amazon EC2
instances.
%
\section{Outline}
\label{intro:outline}
The remaining of this dissertation is organized as follows.
%
Chapter 2 provides the necessary background and context for this
proposal.
%
Chapter 3 presents the key idea of extending an OpenCL standard to
support remote offloading framework and integrating the OpenCL API with
RPC-based remote service.
%
Also, in Chapter 3, the decentralized resource discovery mechanism
through a peer-to-peer virtual private network is presented.
%
Chapter 4 characterizes mobile workloads the perspective of computation
to communication ratio.
%
Also, Chapter 5 explains machine learning techniques for a runtime
scheduler for remote offloading system.
%
Chapter 6 describes an online training mechanism for machine
learning-based mobile offloading scheduler.
%
Then, Chapter 7 introduces a proximity-aware clustering system which can
lead an additional performance improvement in conjunction with mobile
offloading framework.
%
Lastly, in Chapter 8, the dissertation is summarized and future work
is detailed.
%
