{\rtf1\ansi\uc1\deff13\deflang1024
{\fonttbl{\f0\fnil\fcharset0 Zapf Chancery;}
{\f1\fnil\fcharset204 Zapf Chancery;}
{\f2\fnil\fcharset204 Times;}
{\f3\fnil\fcharset204 Helvetica;}
{\f4\fnil\fcharset204 Helvetica;}
{\f5\fnil\fcharset204 Courier;}
{\f6\fnil\fcharset2 Symbol;}
{\f7\fnil\fcharset0 MT Extra;}
{\f8\fnil\fcharset238 Zapf Chancery;}
{\f9\fnil\fcharset238 Times;}
{\f10\fnil\fcharset238 Helvetica;}
{\f11\fnil\fcharset238 Helvetica;}
{\f12\fnil\fcharset238 Courier;}
{\f13\fnil\fcharset0 Times;}
{\f14\fnil\fcharset0 Helvetica;}
{\f15\fnil\fcharset0 Helvetica;}
{\f16\fnil\fcharset0 Courier;}
}
{\colortbl;
\red0\green0\blue0;
\red0\green0\blue255;
\red0\green255\blue255;
\red0\green255\blue0;
\red255\green0\blue255;
\red255\green0\blue0;
\red255\green255\blue0;
\red255\green255\blue255;
\red0\green0\blue128;
\red0\green128\blue128;
\red0\green128\blue0;
\red128\green0\blue128;
\red128\green0\blue0;
\red128\green128\blue0;
\red128\green128\blue128;
\red192\green192\blue192;
}
{\stylesheet
{\s0\fs20\snext0 Normal;}
{\s2\ql\sb240\sa60\keepn\f13\b\fs40 \sbasedon0\snext0 heading 1;}
{\s2\ql\sb240\sa60\keepn\f13\b\fs40\li0 \sbasedon0\snext0 heading 1;}
{\s1\ql\sb240\sa60\keepn\f13\b\fs40\li0 \sbasedon0\snext0 heading 1;}
{\s6\ql\sb240\sa60\keepn\f13\b\fs24\li2048 \sbasedon0\snext0 heading 5;}
{\s3\ql\sb240\sa60\keepn\f13\b\fs32\li512 \sbasedon0\snext0 heading 2;}
{\s7\ql\sb240\sa60\keepn\f13\b\fs24\li2560 \sbasedon0\snext0 heading 6;}
{\s4\ql\sb240\sa60\keepn\f13\b\fs32\li1024 \sbasedon0\snext0 heading 3;}
{\s5\ql\sb240\sa60\keepn\f13\b\fs24\li1536 \sbasedon0\snext0 heading 4;}
{\s6\ql\sb240\sa60\keepn\f13\b\fs24 \sbasedon0\snext0 heading 5;}
{\s1\qc\sb240\sa60\keepn\f13\b\fs40 \sbasedon0\snext0 part;}
{\s3\ql\sb240\sa60\keepn\f13\b\fs32 \sbasedon0\snext0 heading 2;}
{\s7\ql\sb240\sa60\keepn\f13\b\fs24 \sbasedon0\snext0 heading 6;}
{\s4\ql\sb240\sa60\keepn\f13\b\fs32 \sbasedon0\snext0 heading 3;}
{\s5\ql\sb240\sa60\keepn\f13\b\fs24 \sbasedon0\snext0 heading 4;}
}
{\info
{\title Original file was packages.tex}
{\doccomm Created using latex2rtf 1.9.19 (released Nov 20 2007) on Fri Sep 26 11:38:54 2014
}
}
{\footer\pard\plain\f13\fs24\qc\chpgn\par}
\paperw12280\paperh15900\margl2680\margr2700\margt2540\margb1760\pgnstart0\widowctrl\qj\ftnbj\f13\aftnnar
{\page{}\pard\qj\sl240\slmult1 \fi360 [keylist] [mathlist] \par
\page{}\pard\qj\sl240\slmult1 \fi360 {Heungsik Eom}{Dissertation} {Doctor of Philosophy}{December}{2014}{Electrical and Computer Engineering}{Renato J. Figueiredo}{Extending the capabilities of mobile platforms through remote offloading over social device networks}\par
\page{}\pard\qj\sl240\slmult1 \fi360 =-1\par

\par\pard\qc {\fs30 }
\par\pard\qc {\fs24 }
\par\pard\qc {\fs24 }
\par\pard\qc {\fs24 }
\par\pard\qc {\fs24 }\pard\qj\sl240\slmult1 \fi360 { {\par
\pard\qc\sl240\slmult1 \fi0 To my family and those have supported me.\par
}}\page{}\pard\qj\sl240\slmult1 \fi360 { First of all, my most sincere gratitude goes to my advisor, Dr. Renato Figueiredo. I deeply appreciate his support, encouragement, and guidance to complete my academic journey and this dissertation. This work would not have been possible without his patience and support of my effort. I have been very fortunate to have him as my advisor and mentor.\par
\pard\qj\sl240\slmult1 \fi360 I would also like to thank all the members of my advisory committee: Dr. Jos\'e9 A. B. Fortes, Dr. Xiaolin Li, and Dr. Ye Xia for their valuable time and interest in serving on my supervisory committee as well as their advice and comments, which helped improve the quality of this dissertation.\par
\pard\qj\sl240\slmult1 \fi360 I also wish to thank my ACIS lab colleagues and staff, especially, Dr. Taewoong Choi, Dr. David Wolinsky, Dr. Girish Venkatasubramanian, Dr. Pierre St Juste, Dr. Kyungyong Lee, Jiangyan Xu, Yonggang Liu, Kyuho Jeong, Kensworth Subratie, and Dina Quinn. I have had a wonderful time and enjoyed the lab atmosphere.\par
\pard\qj\sl240\slmult1 \fi360 I would also appreciate my master advisor, Dr. Keonwook Kim, for guiding me in building the path of my academic success and for motivating me to study abroad. Also, I wish to express my thanks to my best friends, Yeosang Yoon and Jeongryul Seo for their moral support during my doctoral studies.\par
\pard\qj\sl240\slmult1 \fi360 My special thanks go to my parents, brother, and his family for their love and support, which motivated me to complete my research. I could not have completed my dissertation without their support and patience. I owe much them all.\par
}\page{}\pard\qj\sl240\slmult1 \fi360 [0]{TABLE OF CONTENTS}{tableofcontents} \pard\qj\sl240\slmult1 \fi0 {\s1\ql\sb240\sa60\keepn\f13\b\fs40\li0 Contents\par
}{\field{\*\fldinst TOC \\o "1-3" }{\fldrslt }}
\page{}\pard\qj\sl240\slmult1 \sb60 \fi0 {\s1\ql\sb240\sa60\keepn\f13\b\fs40\li0 List of Tables\par
}{\field{\*\fldinst TOC \\f t }{\fldrslt }}
\page{}\pard\qj\sl240\slmult1 \sb60 \fi0 {\s1\ql\sb240\sa60\keepn\f13\b\fs40\li0 List of Figures\par
}{\field{\*\fldinst TOC \\f f }{\fldrslt }}
\page{}\pard\qj\sl240\slmult1 \sb60 \fi0 {toc}{{CHAPTER}\par
}\pard\qj\sl240\slmult1 \fi0 {}{\par
\page{}\pard\qj\sl240\slmult1 \fi0 \qc{\b Abstract}\par
\pard\qj\sl240\slmult1 \li1024\ri1024\fi0 Mobile computing is becoming the preferred method of personal computing environments for millions of users. In order to meet the increasing demands of computationally-intensive applications, recent mobile platforms have been augmented with multi-core CPUs, powerful GPUs, and special types of hardware accelerators. Despite of these enhancements to hardware of mobile platforms, however, their limited capacities of the batteries will always act as a bottleneck while hindering mobile platforms from utilizing their computing capabilities.\par
\pard\qj\sl240\slmult1 \li1024\ri1024\fi0 To address this restriction, there have been research efforts on remote offloading systems which seek for intelligent ways to enable mobile platforms to leverage computing capabilities of more powerful resources over the network. Even though existing approaches provide core mechanisms to transform typical mobile applications to offloading-enabled applications, they still lack considerations for service discovery mechanisms while assuming the availability of remote computing nodes with static endpoints. Moreover, they have not investigated data privacy and secure communication between the mobile client and remote resources, which can be a crucial flaw for mobile environments.\par
\pard\qj\sl240\slmult1 \li1024\ri1024\fi0 This dissertation presents a novel framework which enables remote workload offloading to external resources within a virtual private network of the mobile user in which trusted remote computing resources are aggregated regardless of user mobility. The proposed system accomplishes this by utilizing a peer-to-peer virtual private networking technique as a substrate for the discovery and configuration of trusted remote resources, and extending OpenCL framework, which is an open standard of parallel programming for heterogeneous computing environments, to support remote offloading using the TCP/IP networking stack.\par
\pard\qj\sl240\slmult1 \li1024\ri1024\fi0 Based upon the evaluation on the performance of the proposed offloading framework, various mobile workloads are characterized for the suitability of offloading from the perspective of computation to communication ratio which is a comprehensive measurement mirroring network conditions and workload characteristics. In addition, this dissertation proposes applying machine learning techniques to runtime schedulers for mobile offloading frameworks. By adopting machine learning techniques to remote offloading scheduling problems, the scheduler can learn the offloading effectiveness from previous offloading experiences automatically and make decisions on whether mobile workloads should be offloaded or executed locally according to the current conditions at runtime. \par
}\page{}\pard\qj\sl240\slmult1 \sb120 \fi0 {\s2\ql\sb240\sa60\keepn\f13\b\fs40 Chapter {\*\bkmkstart BMchap_introduction}1{\*\bkmkend BMchap_introduction}\par
\pard\qj\sl240\slmult1 \sb240 \fi0 INTRODUCTION\par
\par\par}\pard\qj\sl240\slmult1 \sb60 \fi0 Mobile computing is the preferred method of personal computing environments for millions of users. The mobile device market has grown over 400 million smartphones shipped in 2011 and an estimated prediction of 1 billion shipments for 2015\~dignan. This growth has been inspired by the hundreds of thousands of mobile applications such as social networking, location-based services, image processing, augmented reality, face and speech recognition. To meet the increasing demands of these computationally-intense applications, mobile platforms have been augmented with multi-core CPUs, more powerful GPUs and other specialized hardware accelerators. Despite of these many enhancements to the mobile platform\rquote s hardware, however, the inescapable fact is that their limited batteries will always serve as a bottleneck while hindering the mobile platforms from utilizing their computing capabilities.\par
\pard\qj\sl240\slmult1 \fi0 To address this issue, there have been research efforts in two different areas: 1) heterogeneous computing and 2) cloud offloading. Heterogeneous computing has been seen as a mechanism to increase the dynamic range of executions by combining computing elements of varying power performance characteristics and opportunistically scheduling the workloads onto the optimal computing element based on the workload requirements and energy availability\~chen. Heterogeneous architectures with big/small cores, CPU/GPU and CPU/Hardware accelerators are now being offered by leading processor/SoC vendors\~bigprocessing. By allowing dynamic scheduling of workloads onto the best processing element based on the requirements, these architectures provide a wide dynamic range of executions providing improved performance when needed, and optimizing for energy performance.\par
\pard\qj\sl240\slmult1 \fi0 In addition, computation offloading has been proposed as an alternative for extending the capabilities of mobile platforms. Computation offloading in mobile environments is more constrained than previously studied cyber-foraging\~cyber techniques in traditional computing systems, because mobile networks have less throughput, and can exhibit higher latencies and variances. In the past few years, there have been various mobile offloading techniques proposed in the context of constrained mobile environments, from application partitioning\~maui or process migration\~hung to full mobile environment cloning\~clonecloud. Other works have also explored the implications of offloading to different environments including the public cloud (i.e. Amazon EC2)\~hung to within private networks\~shigeru, and even to other mobile devices\~serendipity. All of these efforts seek intelligent ways to enable mobile device developers to leverage the computing capabilities of more powerful computing resources over the network. Even though existing approaches provide core mechanisms to transform typical mobile applications to offloading-enabled applications through various granularities of partitioning and/or migration, they still lack considerations for service discovery mechanisms while assuming the availability of remote computing nodes with static endpoints. Moreover, they have not investigated data privacy and secure communication between the mobile client and remote resources, which can be a crucial flaw for mobile computing environments.\par
\pard\qj\sl240\slmult1 \fi0 This proposal presents a novel framework which addresses the challenge of remote computation offloading to resources in the cloud through the paradigm of extended hardware-layer heterogeneous computing. Heterogeneous computing uses specialized hardware accelerators to increase computing throughput and performance. I combine the power of cloud offloading with the flexibility of heterogeneous architectures to provide an end-to-end heterogeneous architecture which expands the dynamic execution range of mobile platforms, which are typically limited in available energy resources. For example, many data centers and public cloud providers are integrating GPUs in their structure\~bowman, and software designers are able to get better processing performance by offloading more computation to the on-board GPUs than the CPUs. Heterogeneous computing is not just restricted to the data center or to cloud providers, it is also available in most personal computing devices (i.e. workstations, laptops, and mobile devices) of the common user. Some examples are web browsers utilizing GPUs on a user\rquote s machine for HTML rendering\~paul or developers using mobile GPUs for face recognition processing\~burns. Since heterogeneous computing is essentially the offloading of workloads to different accelerators on the host platform, I believe it is natural to extend this concept to support accessing components that are not on-board, but rather remote resources across the network. The proposed approach therefore is a remote offloading framework which broadens the range of heterogeneous computing to remote accelerators. I accomplish this by extending the OpenCL framework to support 1) service discovery and configuration using Social Device Networkand 2) remote offloading over the TCP/IP networking stack and virtual networks. OpenCL is a framework specifically designed to offload workloads in heterogeneous platforms, but it currently does not support access to accelerators across the network. Hence, the first contribution of this work is the ability to remotely offload and execute OpenCL kernels from a mobile device to either nearby computing nodes or to cloud resources. Although the proposed OpenCL-based remote offloading framework for mobile platforms is not the first to proposed the concept of offloading heterogeneous workloads over the network (e.g. Remote CUDA\~rcuda, Virtual OpenCL\~vocl), the proposed design is the first to consider an approach where offloading happens between a mobile device and a virtual machine instance running in the cloud or a trusted workstation across the Internet.\par
\pard\qj\sl240\slmult1 \fi0 In the heterogeneous computing environment, a developer uses the OpenCL API to discover the local hardware accelerators available for computation. The developers is able to offload computation to one or more of these local accelerators through the OpenCL framework. The proposed framework makes it possible for the developer to dynamically discover accelerators located on remote computing resources, even in the cloud. For example, an application may query the OpenCL platform for available accelerators, and the framework returns a virtual accelerator that actually represents a graphics card running on Amazon EC2. The application then seamlessly offloads a computation to this virtual accelerator as if it would be a regular accelerator located on-board the local mobile device. The proposed framework automatically offloads the computation securely to the cloud, manages the transfer of necessary state, and returns the result back to the application. By integrating the design with the OpenCL offloading framework, developers are not required to learn a new API.\par
\pard\qj\sl240\slmult1 \fi0 The second contribution is a distributed method of resource management which handles resource discovery, access control, and data privacy. Past mobile offloading solutions have not investigated a resource discovery mechanism and they assume the availability of remote computing resources with fixed endpoints. I advocate a dynamic approach where eligible computing resources are discovered at runtime, allowing for a more flexible design. By using IP multicast-based discovery, the system periodically locates computing resources available within its private networks.\par
\pard\qj\sl240\slmult1 \fi0 I also investigated the conditions where offloading is more beneficial than local processing by strategically selecting four types of OpenCL workloads: Sobelfilter, matrix multiplication, hidden Markov model, and {\scaps0\b0\i N}-body physics and by using Computation to Communication ratio for those workloads as a criterion for determining the effectiveness of remote offloading. The experiments show that it is more beneficial to offload computations in cases of high computation intensity (i.e. high computation to communication ratio), but for cases of low computation intensity (i.e. low computation to communication ratio), it is not always efficient to perform remote offloading.\par
\pard\qj\sl240\slmult1 \fi0 In addition, this dissertation proposes applying machine learning techniques to runtime schedulers for mobile offloading framework. By adopting machine learning techniques to remote offloading scheduling problems, a scheduler can be automatically trained from previous offloading performance and make decisions on whether mobile workloads should be offloaded or executed locally according to past behaviors and current conditions. While running various machine learning algorithms, the evaluation shows the feasibility of adopting machine learning techniques into scheduling problems for mobile offloading framework.\par
\pard\qj\sl240\slmult1 \sb240 \fi0 {\s3\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMintro_relatedwork}1{\*\bkmkend BMintro_relatedwork}  Related Works on Remote Offloading Systems for Mobile Platforms\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 The research community has been investigating different methods to offload computation for decades. However, remote execution to the cloud has created new opportunities to explore novel offloading solutions. This section discusses the most recent proposals for mobile computation offloading which fall in the following categories: application partitioning, thread or application migration, and distributed offloading frameworks. \par
\pard\qj\sl240\slmult1 \sb120 \fi0 {\s4\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMintro_app_partitioning}1.1{\*\bkmkend BMintro_app_partitioning}  Application Partitioning\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 This approach involves selecting portions of an application to execute remotely through the use of a static or dynamic scheduler. In Spectra\~spectra, developers identify functions in the application that can be offloaded to a remote server over RPC. By monitoring the CPU, file system, and bandwidth, Spectra dynamically decides at runtime which portions of the application should run locally or remotely. MAUI\~maui takes a similar approach but alleviates the process by using many of programming features in the.NET platform such as method attributes, and the Reflection API. Through the .NET Framework\rquote s virtual machine, MAUI is able to dynamically serialize and ship {\i remotable} methods and data to a server proxy, thus leveraging the server\rquote s superior processing capabilities while saving power on the mobile device. Cuckoo\~cuckoo takes a slightly modified approach by focusing more on integrating with the Eclipse IDE. However, it requires developers to implement both local and remote versions of their functionality, whereas MAUI only requires annotations instead of a different implementation. At runtime, Cuckoo does intelligent offloading by determining the appropriate cases to run the code locally or remotely. The proposed approach may be classified as application partitioning similar to Cuckoo because developers does not have to worry about the complications of shipping the workload to the remote device. \par
\pard\qj\sl240\slmult1 \sb120 \fi0 {\s4\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMintro_thr_migration}1.2{\*\bkmkend BMintro_thr_migration}  Thread Migration\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 The source code modification required for most partitioning schemes can preclude adoption by many applications. On the other hand, thread and process migration can be achieved without any source code modification. CloneCloud\~clonecloud achieves this by employing thread migration in the Dalvik Java Virtual Machine(JVM) by transferring all of the thread state(thread stack, necessary heap objects and registers) to the remote virtual machine. When the remote thread completes, the results are merged back with the local Dalvik JVM memory stack. The authors of COMET\~comet developed a similar thread migration technique by doing application VM synchronization through a distributed shared memory(DSM) model. The proposed solution does not require any thread stack or heap synchronization because the OpenCL framework requires explicit declaration of input and output buffers for remote kernel execution. Hence, the use of OpenCL alleviates the complexities of memory synchronizations since all of the necessary state is encapsulated as contiguous memory array that are managed through a few OpenCL data offloading fucntions(i.e. {\i clEnqueueReadBeffer}, {\i clEnqueueWriteBuffer}, {\i clEnqueueMapBuffer}). \par
\pard\qj\sl240\slmult1 \sb120 \fi0 {\s4\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMintro_app_migration}1.3{\*\bkmkend BMintro_app_migration}  Application Migration\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 The previous thread migration techniques can be technically challenging to implement since they require memory synchronization between the remote thread and other threads running locally. All local threads have to block on dirty region of the heap that has been offloaded to the remote server until the remote execution finishes and the memory is merged and released. Application migration does not have such requirements. Hung et al.\~hung describes an application migration design that leverages the {\i onResume} and{\i onPause} events of an Android application as the markers for process migration. The {\i onPause} event occurs when a user switches to another application. The Android system requires that application states are saved on persistent storage in the case the operating system decides to shutdown in the case of low memory situations. Hence, Hung et al. create a solution which uses the {\i onPause} event to force the application to save its state. The state is then copied to a cloned VM running on the cloud and resumed there until completion, then transferred back. The proposed design automatically handles the state transfers between the local and remote devices without relying on specific Android-based events. \par
\pard\qj\sl240\slmult1 \sb120 \fi0 {\s4\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMintro_framework}1.4{\*\bkmkend BMintro_framework}  Distributed Offloading Framework\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 Various recent approaches have focused on a totally different model requiring more effort from the developers. Proposals such as Mobile Map Reduce(MMR)\~mmr, Sonora\~sonora, Serendipity\~serendipity, and ThinkAir\~thinkair expose a distributed offloading framework for developers to adopt. For example, MMR is a MapReduce system optimized for the constrained networking conditions of mobile devices by taking into account bandwidth and latency for efficient mobile device performance. Sonora exposes a distributed stream-based programming model which handles workload distribution and failures in a mobile network. Serendipity provides an offloading framework for intermittently connected mobile devices and does not rely on cloud services.\par
\pard\qj\sl240\slmult1 \fi0 The proposed approach can be also classified as a distributed offloading framework. Instead of defining the system from scratch, however, the proposed framework reuses the workload offloading paradigms of the OpenCL framework which provides a more familiar and widely supported interface for developers. There also exist other heterogeneous offloading frameworks which are quite similar to the proposed approach. Remote CUDA\~rcuda is one approach that extends the NVIDIA CUDA API to support remote offloading over the network. Their goal is to minimize the network overhead and they analyze the impact of using different networking technologies such as Gigabit Ethernet or InfiniBand. Their research is aimed at cluster environment such as the data center and it does not consider mobile environments with low bandwidth. Virtual OpenCLvocl provides a similar solution which uses OpenCL instead of the proprietary CUDA protocol. They also focus on cluster environments but they leverage the MPI library for memory and workload synchronization. Since our solution is designed for mobile platforms and the cloud, it differs greatly because it takes into account not only performance, but also energy consumption, connectivity, and mobility. \par
\pard\qj\sl240\slmult1 \sb120 \fi0 {\s4\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMintro_lack}1.5{\*\bkmkend BMintro_lack}  Lack of Privacy and Trust\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 Mobile devices tend to process more private information about their users. Hence, dealing with trust and privacy is a fundamental requirement. None of these previous works thoroughly deal with the issue of securing the offloaded data in a trusted fashion. They also do not address the issue of verifying the integrity of the results provided by the remote cloud server. Hung et. al mentioned using L2TP VPN for security, but such as approach does not ensure privacy within the cloud which is a growing concern for many cloud applications\~brodkin. Private communication and trusted results are amongst the first issues that this proposal addresses through the use of a peer-to-peer virtual private network(P2PVPN). The proposed approach guarantees that the user data is only offloaded to trusted compute nodes and verified through non-repudiable, encrypted channels. I also analyze the cost associated with providing this privacy because encryption on a mobile device is non-negligible. However, none of previous approaches measured this cost. UIA\~uia has considered ad-hoc virtual private networks connecting mobile devices of social peers, and is closely related to SocialVPN, but has not been evaluated in the context of computation offloading.\par
\pard\qj\sl240\slmult1 \fi0 Apart from the above mentioned offload solutions, there has been a lot of research work going on in the area of heterogeneity which deals with combining processing elements of varying power performance capabilities to the local processor/platform. A combination of small and big cores, GPUs and special purpose hardware optimally scheduled for best energy efficient performance based on workload requirement and better power provides a wider dynamic execution range. \par
\pard\qj\sl240\slmult1 \sb240 \fi0 {\s3\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMintro_motivation}2{\*\bkmkend BMintro_motivation}  Motivation\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 This section describes a few scenarios that motivate the proposed approach in detail. Figure\~{\field{\*\fldinst{\lang1024 REF BMfig_motivation \\* MERGEFORMAT }}{\fldrslt{?}}} gives a general idea of one example deployment scenario. Alice connects her smartphone to a virtual private network(VPN) consisting of her laptop, Bob\rquote s desktop, and her virtual machine instance running on Amazon EC2. Since each of these devices is running SocialVPN\~socialvpn, they automatically join the same virtual private network creating a pool of trusted resources in a Social Device Network. With this secure IP layer consisting of trusted peers, the framework is able to use IP multicasting over the VPN to discovery nodes that are available for computation offloading. During the discovery process, the system records the characteristics of each node in the network such as bandwidth, latency, and processing capabilities. When an application decides to offload some computation, the framework dynamically determines the best node to use as the remote offloading target. In many cases, for example, if the bandwidth or remote processing capabilities are too low, the framework may decide to simply run the workload locally.\par
\pard\qj\sl240\slmult1 \fi0 The goal of the design is to provide an intuitive offloading framework that developers can integrate into their application using well-adopted programming concepts. Currently, many software developers utilize the OpenCL framework to exploit on-board heterogeneous platforms. In the scientific computing arena, supercomputers increasingly integrate CPUs and GPUs in order to maximize performance/Watt\~powertutor. Popular software projects, such as OpenCV and OpenSSL, are re-implementing major portions of their functions to run on the OpenCL platform. Mobile SoC platforms, based on processors such as ARM and Intel, are also starting to provide OpenCL support on their architecture. The latest version of the OpenCL specification allows for devices beyond CPUs and GPUs to be accesses through the API. There is also an industry momentum building up behind OpenCL with the formation of new industry foundations to foster fast adoption\~hsa. These considerations point to OpenCL API as the emerging defacto standard for heterogeneous computing.\par
{\pard\qj\sl240\slmult1 \sb240 \fi360 \par
\pard\qc\sl240\slmult1 \fi0 {file=figs/motivation.eps, width=5.5in} \par
\pard\qc\sl240\slmult1 \fi0 {Figure {\*\bkmkstart BMfig_motivation}1{\*\bkmkend BMfig_motivation}: Private Networking and Node Selection}{\field{\*\fldinst TC "1 Private Networking and Node Selection" \\f f}{\fldrslt }}\par
}\pard\qj\sl240\slmult1 \sb240 \fi0 Here is another example of how a developer might take advantage of OpenCL and the proposed platform in a seamless fashion. Consider a typical facial recognition application in a mobile device used as a security feature, or for tagging friends on a social networking application. First, the developer utilizes a CPU implementation. However, the developer quickly realizes the processing limitations of doing image processing on the CPU and decides that such a workload is better suited for the GPU or another hardware-based accelerator. With this realization, the developer writes an OpenCL-based implementation due to its wide support and adoption. Using OpenCL, the developer is able to offload the computation from the mobile CPU to the mobile GPU and therefore achieve better performance with less battery consumption.\par
\pard\qj\sl240\slmult1 \fi0 The proposed framework aims to extend the umbrella of heterogeneous computing to include devices beyond the physical host platform. By recompiling the application to link with the proposed framework, the developer can transparently access remote resources available via the SocialVPN, including GPUs running on computing resources more powerful than a mobile device. For instance, if the mobile device is connected to a virtual network consisting of an Amazon EC2 GPU instance, and the user\rquote s personal workstation, the extensions to the OpenCL framework will automatically select the best candidate based on available device capabilities, and network conditions as the target compute node for remote execution. Also, the use of SocialVPN, ensures that computation is offloaded securely to socially trusted nodes. This enhancement occurs transparently to the developer and the user requiring only code recompilation. \par
\pard\qj\sl240\slmult1 \sb240 \fi0 {\s3\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMintro_contributions}3{\*\bkmkend BMintro_contributions}  Contributions\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 The key contributions of this dissertation can be summarized as follows:\par
\pard\qj\sl240\slmult1 \fi0 {\i0\scaps0\b Novel framework for remote computation offloading.} The primary contribution is a novel framework which addresses the challenge of remote computation offloading to resources in the cloud the paradigm of extended hardware-layer heterogeneous computing. Heterogeneous computing uses specialized hardware accelerators to increase computing throughput and performance. The proposed framework combines the power of cloud offloading with the flexibility of heterogeneous architecture which expands the dynamic execution range of mobile platforms, which are typically restricted by power constraints.\par
\pard\qj\sl240\slmult1 \fi0 {\i0\scaps0\b Decentralized resource discovery mechanism.} Second contribution of this proposal is a distributed method of resource management which handles service discovery, access control, and data privacy. Previous mobile offloading solutions have not investigated a service discovery mechanism by assuming the availability of remote computing resources with static endpoints. The proposed framework advocates a dynamic approach where candidate computing nodes are discovered at runtime while allowing for a more flexible design. By using IP multicast-based discovery, the proposed system periodically locates computing nodes which are available within their social device network.\par
\pard\qj\sl240\slmult1 \fi0 {\i0\scaps0\b Workloads characterization using computation to communication ratio.} I characterize mobile workloads for the suitability of offloading from the perspective of {\scaps0\b0\i Computation to Communication ratio} which is calculated by the time for workloads to be executed locally divided by the data transfer time for remote offloading. Thus, in this work, computation to communication ratio for mobile workloads is a comprehensive measurement which mirrors three dynamic parameters such as the volume of computation of workloads, the amount of data to be transferred, and the network conditions.\par
\pard\qj\sl240\slmult1 \fi0 {\i0\scaps0\b Machine learning-based runtime scheduler.} Prior studies have primarily focused on core mechanisms for offloading. However, adaptive scheduling in such system is important because offloading effectiveness can be influenced by varying network conditions, workload requirements, and load at the target device. In this dissertation, a study on the feasibility of applying machine learning techniques to address the adaptive scheduling problem in mobile offloading framework is presented as a third contribution. By taking the algorithm complexity and scheduling performance into account, a few machine learning algorithms are selected to implement on/offline runtime schedulers for mobile offloading framework.\par
\pard\qj\sl240\slmult1 \fi0 The prototype implementation of the proposed framework is evaluated, with regard to end-to-end application performance and energy consumption in mobile devices, through a variety of network configurations representing local and wide area network, and various levels of remote computing capabilities such as typical CPUs, GPUs as well as Amazon EC2 instances. \par
\pard\qj\sl240\slmult1 \sb240 \fi0 {\s3\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMintro_outline}4{\*\bkmkend BMintro_outline}  Outline\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 The rest of the proposal is outlined as follows. Chapter 2 provides the necessary background and context for this proposal. Chapter 3 presents the key idea of extending an OpenCL standard to support remote offloading framework and integrating the OpenCL API with RPC-based remote service. Also, in this chapter, the decentralized resource discovery mechanism through a peer-to-peer virtual private network is presented. Chapter 4 characterizes mobile workloads the perspective of computation to communication ratio. Also, chapter 5 explains machine learning techniques for a runtime scheduler for remote offloading system. Finally, in chapter 6, the proposal is summarized and the future work is detailed.\par
\page{}\pard\qj\sl240\slmult1 \fi0 {\s2\ql\sb240\sa60\keepn\f13\b\fs40 Chapter {\*\bkmkstart BMchap_background}2{\*\bkmkend BMchap_background}\par
\pard\qj\sl240\slmult1 \sb240 \fi0 BACKGROUND\par
\par\par}\pard\qj\sl240\slmult1 \sb60 \fi0 This proposal brings together various technologies synergetically thus us to reuse various existing tools. By combining a heterogeneous computing framework, virtual private networking, and IP multicasting, I propose a solution that is based on well accepted academic and industry standard. In this section, I elaborate on some of these current standards and their impact on the proposed design. \par
\pard\qj\sl240\slmult1 \sb240 \fi0 {\s3\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMback_heterogeneous}1{\*\bkmkend BMback_heterogeneous}  Heterogeneous Computing\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 An increasing number of hardware platforms are incorporating heterogeneity for both energy and performance improvement. Therefore, it has been seen that primary host CPU cores are assisted by various computing units for specialized functions offloading. These specialized accelerators are aimed at speeding up specific portions of the application code and differ from the general purpose processor architectures. In this subsection, I explain some of the different types of heterogeneous platforms available nowadays.\par
\pard\qj\sl240\slmult1 \fi0 {\i0\scaps0\b Heterogeneous multi-core CPU combinations.} In this design, high performing CPU cores are integrated with low performing energy efficient cores. Currently, some mobile and server platforms employ this architecture\~atom. The goal is to match the application execution requirements to the smallest available core that can achieve the execution within required performance limitations with lowest energy consumption. In other cases, one big processor is utilized for controlling the scheduler of multiple smaller cores\~nvidia. Previous works have suggested that this combination can reduce energy consumption by 39%\~kumar.\par
\pard\qj\sl240\slmult1 \fi0 {\i0\scaps0\b CPU-GPU combinations.} These platforms aim to accelerate graphics performance by including specialized hardware to execute graphics functions. The Graphics Processor Units (GPUs) consist of multiple execution units and local memory that can efficiently process data and task parallel instruction streams at a very high throughput. This architectural property also make GPUs good candidates for executing non-graphical functions that can exploit the data and task parallelism at a large scale leading to evolution of a rich General-Purpose computing on Graphics Processing Units (GPGPU) based offloading frameworks.\par
\pard\qj\sl240\slmult1 \fi0 {\i0\scaps0\b Hardware accelerators.} Increasingly specialized hardware accelerators are being integrated on different platforms to provide faster execution times for domain-specific application bottleneck functions. The accelerators range from fixed function units to customizable and programmable hardware (e.g. FPGAs). In conjunction with the processor cores, the specialized hardware accelerators assist in making the targeted applications perform better by taking care of the functions that use a majority of core cycles. This model makes accelerator integration a natural choice for platforms that target specific usages and/or operate under real-time and power constraints.\par
\pard\qj\sl240\slmult1 \fi0 {\i0\scaps0\b Remote offloading.} With the advent of mobile era, remote execution to remote resources has been one of the most common solutions to achieve better application performance. Due to the compute, memory and power limitations on mobile devices, most of the applications divide their execution between the portions of local processing and remote execution handling much of the complex and heavy task.\par
\pard\qj\sl240\slmult1 \fi0 In this dissertation, one of the contributions is to extend the scope of heterogeneity to include a new kind of platform component \endash  the compute units on remote devices located in the private or public clouds. Therefore, the proposed multi-layer framework allows for application to discovery, enumerate, and use the remote devices as if they were local devices. The framework has the advantage of not only speeding application execution times and saving mobile device energy consumption but also allows for the construction of a virtual dynamic platform with tight integration of heterogeneous components both on and off the host platform. \par
\pard\qj\sl240\slmult1 \sb240 \fi0 {\s3\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMback_opencl}2{\*\bkmkend BMback_opencl}  OpenCL\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 Currently, OpenCL (Open Compute Language) is the most well-known framework for offloading tasks to different compute units within a heterogeneous host platform. This framework provides an APIs which make it possible for application developers to dispatch kernels for execution. It is supported by the top chip makers such as Intel, NVIDIA, AMD and ARM Holdings. At the moment, OpenCL is used in the scientific computing arena as means to leverage GPUs for high-throughput computations. I implement the proposed design as a subsystem of the OpenCL framework. The extension makes it possible for the OpenCL framework to discover accelerators located on remote resources across the network. Once these remote OpenCL devices are discovered, the application developers is able to use the same interface to transparently offload a kernel as if it was a part of the local platform. \par
\pard\qj\sl240\slmult1 \sb240 \fi0 {\s3\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMback_p2pvpn}3{\*\bkmkend BMback_p2pvpn}  Secure Offloading with P2PVPN\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 Previous works have not focused on dealing with the privacy implications of offloading mobile computation to the cloud for remote execution. One solution can be to use socket layer encryption such as SSL/TLS when sending data to the cloud, but the encryption overhead has not been studied in past research. As privacy concerns in mobile devices grow, this dissertation recognizes that a robust mechanism to ensure privacy is a fundamental requirement. To address the privacy issue, the proposed design integrates a social peer-to-peer virtual private network (SocialVPN\~socialvpn). SocialVPN automatically discovers social peers on the Internet and creates a private network consisting solely of these trusted peers. It also handles the cumbersome task of cryptographic key management in a distributed fashion thus enabling a robust, secure communication layer with no single point of failure. By emulating a virtual LAN, peers have private IP addresses to each other allowing them to freely communicate even if they are both behind network address translators (NAT) and certain firewalls. Hung et al.\~hung suggested using a centralized VPN to ensure data privacy. Centralized VPNs (e.g. OpenVPN, Cisco VPN, L2TP) typically function by allowing the VPN client to connect to a VPN gateway, which forwards UP requests to the internal private network on behalf of the clients. This model does not guarantee any privacy between the gateway and the destination machine because it does not provide end-to-end encryption. This is exacerbated in a public cloud where you have different and unknown organizations sharing the same private network. This lack of IP level data privacy is a primary reason that many organizations are hesitant to run their private workloads in the public cloudsbrodkin. SocialVPN addresses this issue by enforcing encrypted end-to-end communication.\par
\pard\qj\sl240\slmult1 \fi0 The other important aspect of SocialVPN is that it gives the user complete control over their virtual private network. A user can create a virtual network consisting only of his/her own personal computing devices. For example, Alice can create a VPN consisting of only 4 nodes: her mobile phone, her laptop, her workstation at the office, and a VM instance running on Amazon EC2. Alice can also expand the VPN to include her family members, and closest friends or scale it down dynamically. If Alice only trusts her own personal resources, she simply limits her VPN to allow connectivity to her machines, and the framework will only have private UP connectivity to her nodes thereby making them the only candidates for remote offloading. If she wants to access to more resources, she can allow private IP access to nodes belonging to her trusted friends and family, hence enabling the framework to utilize them as compute endpoints. The assumption here is that all of these nodes will be running SocialVPN software stack along with an OpenCL RPC-service to support the remote execution. \par
\pard\qj\sl240\slmult1 \sb240 \fi0 {\s3\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMback_ipmulticast}4{\*\bkmkend BMback_ipmulticast}  IP Multicasting in Private Networks\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 The Internet protocol does support multicasting; however, routers usually block multicast traffic; hence multicasting is widely used for decentralized service discovery within private networks. For example, most network printers support local network discovery through well-known standards such as Bonjour\~bonjour, or Universal Plug and Play (UPnP). Hence, all modern operating systems as well as smart appliances (i.e. TVs, NAS boxes) rely on these IP multicast-based services mechanisms to discover and communicate with each other. Through integration with SocialVPN, we are able to reuse these IP multicasting techniques as the foundation for the service discovery system and allow it to span across wide area networks. With this decentralized approach, it is no longer necessary to register services in a centralized directory. When the mobile device decides to offload some computation, it sends an IP multicast query over the network, SocialVPN automatically handles this packet and sends it to all trusted peers in the private network. Eligible peers are therefore able to advertise themselves to the requestor. This enables dynamic service discovery without having to hardcoded endpoints in configuration files or depend on a directory service. IP multicast support in SocialVPN enables us to follow the same decentralized services discovery techniques that are widely used in the private networks for network printers, DNLA compatible devices (e.g. TVs) and storage boxes.\par
\page{}\pard\qj\sl240\slmult1 \fi0 {\s2\ql\sb240\sa60\keepn\f13\b\fs40 Chapter {\*\bkmkstart BMchap_offloading}3{\*\bkmkend BMchap_offloading}\par
\pard\qj\sl240\slmult1 \sb240 \fi0 OFFLOADING FRAMEWORK\par
\par\par}\pard\qj\sl240\slmult1 \sb300 \fi0 {\s3\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMoffloading_motivation}1{\*\bkmkend BMoffloading_motivation}  Motivation\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 Heterogeneity is now the norm in commodity computing systems where platforms possess a mix of computing units such as CPUs, GPU, and other specialized accelerators. OpenCL has therefore emerged as the open standard for parallel programming for these heterogeneous platforms. By providing a common standard along with the necessary toolchain, OpenCL enables a uniform framework to discover, program, and distribute parallel workloads to the diverse set of compute units in the hardware. Graphics processing units (GPUs), in particular, have reached the extended coverage due to their rapidly expanding use in general purpose computing (GPGPU) with parallel programming of the OpenCL standard. For that reason, there have been efforts exploring the advantages of parallelism from the OpenCL framework by offloading GPGPU workloads within an HPC cluster environment\~rcuda. The primary motivation for offloading within a cluster is for more efficient utilization of resources by allowing multiple compute nodes to share the same GPU for general purpose computing. These researchers clearly demonstrate that OpenCL (and CUDA)-based remote offloading is a viable option which saves power through more efficient sharing of heterogeneous compute units over the network despite the communication overheads.\par
\pard\qj\sl240\slmult1 \fi0 In this work, I shift this motivation from the HPC cluster environment to mobile platforms by considering a different perspective to this expanding body of research by adapting the OpenCL offloading approach to a mobile cloud computing scenario. Since previous works focused mainly on offloading OpenCL workloads in HPC cluster environments with high bandwidth and low latency between the nodes, it was easy to realize and assess the advantages. However, the advantages are not as clear in the mobile cloud computing scenario where OpenCL workloads are sent over the wide area on network links with much lower bandwidth and higher latencies than cluster environments. Moreover, since workloads are traversing untrusted networks in the wide-area, a layer of network encryption is necessary to ensure privacy and some level of the trust of the results from the remote compute node.\par
\pard\qj\sl240\slmult1 \fi0 This section presents an OpenCL-based remote offloading framework designed specifically for mobile cloud computing where OpenCL workloads can be exported from a mobile node (i.e. an Android device) to the cloud (i.e. an Amazon EC2 instance with GPU access). This remote offloading framework consists of the following components: 1) a customized RPC system with optimizations for network tasking and data marshalling, 2) a service discovery mechanism which selects the compute node with the lowest latency, and 3) a virtual private networking layer which provides transparent network encryption without any modification at the application layer. The proposed system is implemented as a wrapper library around the OpenCL API; thus allowing transparent integration of the OpenCL API with our framework without any code modification. The offloading framework also makes it possible for the developer to dynamically discover accelerators located on remote computing nodes (i.e. in the cloud), virtualize these accelerators as if they would be regular accelerators located on-board the local mobile device, and then seamlessly offload computation to the virtual accelerators.\par
\pard\qj\sl240\slmult1 \fi0 An additional contribution of this work is a distributed method of resource management which handles service discovery, access control, and data privacy. Past mobile offloading solutions have not investigated a service discovery mechanism and they assume the static availability of remote computing nodes with fixed endpoints. Instead, we advocate a dynamic approach where eligible compute nodes are discovered at runtime, allowing for a more flexible design. We achieved this by using IP multicast-based discovery so that the system periodically locates compute nodes available within their networks.\par
\pard\qj\sl240\slmult1 \fi0 Furthermore, the proposed approach supports accessing resources beyond the local private network, broadening the accessibility to trusted compute nodes across the Internet and the cloud. In fact, previous offloading research has focused on sending workloads only to resources within private local area networks where there is some guarantee that the data is contained within the network. This is accomplished by utilizing a social peer-to-peer virtual private network, SocialVPN\~socialvpn. The use of a peer-to-peer VPN with social features has several benefits. First of all, by providing virtual private IP addresses only to social peers, the endpoints discovered through the VPN are deemed trustworthy by the user -\'E2\'93 creating a Social Area Networks. Secondly, the IP layer security ensures data privacy and frees us from having to handle the cumbersome tasks of cryptographic key management, and socket layer encryption. Through SocialVPN, the state and functions (called kernels in OpenCL) necessary for remote execution are sent privately and the results are authenticated and verified at the virtual networking layer.\par
\pard\qj\sl240\slmult1 \sb240 \fi0 {\s3\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMoffloading_structure}2{\*\bkmkend BMoffloading_structure}  The Structure of Remote Offloading Framework\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 The overall architecture of the proposed framework consists of 5 main modules: integration with the OpenCL API, RPC-based offloading mechanism, decentralized resource discovery feature, runtime scheduler, and trusted and private IP communication layer. {\par
\pard\qj\sl240\slmult1 \sb240 \fi360 \par
\pard\qc\sl240\slmult1 \fi0 {file=figs/overall\\s\\do5({\fs16 a})rch.eps, width=4.5in} \par
\pard\qc\sl240\slmult1 \fi0 {Figure {\*\bkmkstart BMfig_architecture}1{\*\bkmkend BMfig_architecture}: Overall System Architecture}{\field{\*\fldinst TC "1 Overall System Architecture" \\f f}{\fldrslt }}\par
}\pard\qj\sl240\slmult1 \sb360 \fi0 {\s4\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMoffloading_API}2.1{\*\bkmkend BMoffloading_API}  Transparent OpenCL API Extension\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 Since OpenCL is an open standard currently supported by most of the major device manufacturers, I chose not to deviate from their API in order to minimize the learning curve for developers. Latest revisions of the OpenCL specification are extending the coverage to enable integration of a large pool of heterogeneous hardware under the API. Because OpenCL is also a hardware level offloading framework, it provides various functions which can leverage in the proposed system. The OpenCL API possesses a set of these features: device discovery and enumeration, device selection and customization, buffer management, job offload and status queries. With these features, the application developer has a full control on the use of specific accelerators necessary to optimize their application performance. In this section, I explain how these features can be extended to support remote offloading framework. Table\~{\field{\*\fldinst{\lang1024 REF BMtable_functionality \\* MERGEFORMAT }}{\fldrslt{?}}} summaries OpenCL API functionalities and extensions.\par
\pard\qj\sl240\slmult1 \fi0 {\i0\scaps0\b Device discovery and enumeration.} In the initialization phase of the interface, the developer queries the platform for on-board OpenCL-capable accelerators. The OpenCL framework returns a list of accessible compute devices located on the board along with their computing capabilities (e.g. graphics cards, video decoders, cryptographic devices). In the case of a mobile device, this might include only a graphics processor, or a specialized stream processor\~stemcell. I extend this portion of the API by allowing the developer to discover other OpenCL-enabled accelerators located on remote computing resources over the network. Hence, when the developer performs this type of the query on a mobile device, it discovers not only the local mobile GPU but also another GPUs running on the remote workstations or on the cloud, as long as they are part of the same virtual private network.\par
\pard\qj\sl240\slmult1 \fi0 {\i0\scaps0\b Device selection and configuration.} In the standard OpenCL framework, once presented with a list of devices, the developer selects one or more targets for computation offloading. This selection is usually based on the characteristics of each particular device (e.g. the number of compute units of the accelerator, maximum number of work items, architecture, latency or network bandwidth). Based on the characteristics the developer is also able to configure the workload appropriately for better performance. Since this is a hardware layer offloading mechanism, the OpenCL framework handles the support for different architectures by using a compiler which converts the code from a modified version of C99 to the target instruction set of the accelerator. Hence, the API also provides access to compiler-based customizations for both local and remote devices. By extending the discovery process to include remote OpenCL devices over the network, this selection and configuration process can become cumbersome to the developer. Thus, the proposed system can present only one virtual device handle which represents the best offloading node according to network conditions.\par
\pard\qj\sl240\slmult1 \fi0 {\i0\scaps0\b Workload state transfer.} Having selected a device, the next phase is the actual offloading of the data and code necessary to run workloads remotely. The function to be executed (i.e. a kernel) is first sent either as C99 source code or an LLVM-based intermediate language. Once transferred, the code is compiled for the target accelerator. In order to execute the kernel on the accelerator, the necessary state has to be transferred to the device regardless of whether it is local or remote. If the device resides on the host platform, the task of buffer management simply involves copying data from main memory to local storage accessible by the accelerator. However, if the workload is being offloaded to a remote accelerator, then the buffers have to be managed slightly differently. First, the data has to be marshalled and copied into the buffers of the networking stack, then transported over the network to the appropriate remote host. The data is then copied from the networking stack of the remote host unto the accelerator\rquote s own local storage. For example, in the case of a mobile device offloading computation to a GPU hosted in the cloud, the necessary input and output buffers have to be created and copied in the GPU\rquote s local memory in the cloud. Upon completion, the output buffers are copied back from the GPU\rquote s local memory to the mobile device\rquote s memory over the network. Also, it is possible to perform data compression at the networking layer to minimize the networking overhead and energy consumption.\par
\pard\qj\sl240\slmult1 \fi0 {\i0\scaps0\b Resouce and failure management.} The final phase of the OpenCL API is the ability to discover errors and release its state and resources in a graceful manner. Each function has its error parameter which keeps the developer aware of the proper execution of the remote job. If an error occurs due to an issue with the source code, the workload configuration, or any other hardware issues, an appropriate error code is returned to the developer. In return, the developer can release the various resources (i.e. buffers, device handles) that are associated with the job. Once again, I extend this functionality to support network failures as well. In the case of a disconnection, the appropriate error code is returned ro the developer who then performs the necessary actions to clean up the state belonging to the job. On the server, the necessary clean-up is taken as well by the framework.\par
\pard\qj\sl240\slmult1 \fi0 The decision to utilize the OpenCL framework for computation offloading in mobile devices allows us to leverage all of the functionalities already in place for offloading computation locally from the CPU to an on-board hardware accelerator. I added the required extensions to the framework by creating a wrapper library around the OpenCL API. Since the offloading framework provides an identical interface, developers can integrate their OpenCL code with the system without any code modification. { {\par
\pard\qj\sl240\slmult1 \sb240 \fi0 \par
\pard\qc\sl240\slmult1 \fi0 {Table {\*\bkmkstart BMtable_functionality}1{\*\bkmkend BMtable_functionality}: OpenCL API Functionalities and Extensions}{\field{\*\fldinst TC "1 OpenCL API Functionalities and Extensions" \\f t}{\fldrslt }}\par
{\pard\qc\sl240\slmult1 \fi0 \par
\trowd\clbrdrt\brdrs\clbrdrb\brdrs\clbrdrr\brdrs\cellx1150\clbrdrt\brdrs\clbrdrb\brdrs\clbrdrr\brdrs\cellx2300\clbrdrt\brdrs\clbrdrb\brdrs\clbrdrr\brdrs\cellx3450\clbrdrt\brdrs\clbrdrb\brdrs\clbrdrr\brdrs\cellx4600\clbrdrt\brdrs\clbrdrb\brdrs\cellx5750
\pard\intbl\ql {{\par
\pard\qc\sl240\slmult1 \fi0 Feature}}\cell
\pard\intbl\ql {{\par
\pard\qc\sl240\slmult1 \fi0 API Function}}\cell
\pard\intbl\ql {{\par
\pard\qc\sl240\slmult1 \fi0 OpenCL Feature}}\cell
\pard\intbl\ql {{\par
\pard\qc\sl240\slmult1 \fi0 Extension Capabilities}}\cell
\pard\intbl\ql {{\par
\pard\qc\sl240\slmult1 \fi0 Future Capabilities}}\cell
\pard\intbl\qr \cell
\row
\trowd\clbrdrr\brdrs\cellx1150
\pard\intbl\ql { {\par
\pard\qc\sl240\slmult1 \fi0 Discovery and}\cell
\pard\intbl\ql \cell
\row
\trowd\clbrdrr\brdrs\cellx1150\clbrdrr\brdrs\cellx2300
\pard\intbl\ql {Enumeration}}\cell
\pard\intbl\ql {{\par
{\i }\pard\qc\sl240\slmult1 \fi0 }\cell
\pard\intbl\ql \cell
\row
\trowd\clbrdrr\brdrs\cellx1150
\pard\intbl\ql {{\i clGetPlatformIDs},}\cell
\pard\intbl\ql \cell
\row
\trowd\clbrdrr\brdrs\cellx1150
\pard\intbl\ql {{\i clGetDeviceIDs},}\cell
\pard\intbl\ql \cell
\row
\trowd\clbrdrr\brdrs\cellx1150
\pard\intbl\ql {{\i clGetDeviceInfo}}\cell
\pard\intbl\ql \cell
\row
\trowd\clbrdrr\brdrs\cellx1150\clbrdrr\brdrs\cellx2300\clbrdrr\brdrs\cellx3450\clbrdrr\brdrs\cellx4600
\pard\intbl\ql {{\i }}}\cell
\pard\intbl\ql {{\par
\pard\qc\sl240\slmult1 \fi0 List on-board devices}}\cell
\pard\intbl\ql {{\par
\pard\qc\sl240\slmult1 \fi0 List the best remote resource}}\cell
\pard\intbl\ql {{\par
\pard\qc\sl240\slmult1 \fi0 List all remote resources}}\cell
\pard\intbl\ql \cell
\row
\trowd\clbrdrr\brdrs\cellx1150
\pard\intbl\ql {{\par
\pard\qc\sl240\slmult1 \fi0 Selection and}\cell
\pard\intbl\ql \cell
\row
\trowd\clbrdrr\brdrs\cellx1150\clbrdrr\brdrs\cellx2300
\pard\intbl\ql {Configuration}}\cell
\pard\intbl\ql {{\par
{\i }\pard\qc\sl240\slmult1 \fi0 }\cell
\pard\intbl\ql \cell
\row
\trowd\clbrdrr\brdrs\cellx1150
\pard\intbl\ql {{\i clCreateContext},}\cell
\pard\intbl\ql \cell
\row
\trowd\clbrdrr\brdrs\cellx1150
\pard\intbl\ql {{\i clBuildProgram},}\cell
\pard\intbl\ql \cell
\row
\trowd\clbrdrr\brdrs\cellx1150
\pard\intbl\ql {{\i clCreateKernel},}\cell
\pard\intbl\ql \cell
\row
\trowd\clbrdrr\brdrs\cellx1150
\pard\intbl\ql {{\i clSetKernelArgs},}\cell
\pard\intbl\ql \cell
\row
\trowd\clbrdrr\brdrs\cellx1150
\pard\intbl\ql {{\i clEnqueueNDRangeKernel}}\cell
\pard\intbl\ql \cell
\row
\trowd\clbrdrr\brdrs\cellx1150\clbrdrr\brdrs\cellx2300
\pard\intbl\ql {{\i }}}\cell
\pard\intbl\ql {{\par
\pard\qc\sl240\slmult1 \fi0 Enables job compilation}\cell
\pard\intbl\ql \cell
\row
\trowd\clbrdrr\brdrs\cellx1150\clbrdrr\brdrs\cellx2300
\pard\intbl\ql {and configuration}}\cell
\pard\intbl\ql {{\par
\pard\qc\sl240\slmult1 \fi0 Configuration for the}\cell
\pard\intbl\ql \cell
\row
\trowd\clbrdrr\brdrs\cellx1150\clbrdrr\brdrs\cellx2300
\pard\intbl\ql {remote resource}}\cell
\pard\intbl\ql {{\par
\pard\qc\sl240\slmult1 \fi0 Configure}\cell
\pard\intbl\ql \cell
\row
\trowd\clbrdrr\brdrs\cellx1150
\pard\intbl\ql {multiple resources}}\cell
\pard\intbl\ql \cell
\row
\trowd\clbrdrr\brdrs\cellx1150
\pard\intbl\ql {{\par
\pard\qc\sl240\slmult1 \fi0 Workload}\cell
\pard\intbl\ql \cell
\row
\trowd\clbrdrr\brdrs\cellx1150\clbrdrr\brdrs\cellx2300
\pard\intbl\ql {State Transfer}}\cell
\pard\intbl\ql {{\par
{\i }\pard\qc\sl240\slmult1 \fi0 }\cell
\pard\intbl\ql \cell
\row
\trowd\clbrdrr\brdrs\cellx1150
\pard\intbl\ql {{\i clCreateProgramWithSource},}\cell
\pard\intbl\ql \cell
\row
\trowd\clbrdrr\brdrs\cellx1150
\pard\intbl\ql {{\i clCreateBuffer},}\cell
\pard\intbl\ql \cell
\row
\trowd\clbrdrr\brdrs\cellx1150
\pard\intbl\ql {{\i clEnqueueReadBuffer},}\cell
\pard\intbl\ql \cell
\row
\trowd\clbrdrr\brdrs\cellx1150
\pard\intbl\ql {{\i clEnqueueWriteBuffer}}\cell
\pard\intbl\ql \cell
\row
\trowd\clbrdrr\brdrs\cellx1150\clbrdrr\brdrs\cellx2300
\pard\intbl\ql {{\i }}}\cell
\pard\intbl\ql {{\par
\pard\qc\sl240\slmult1 \fi0 Source code and data transfer}\cell
\pard\intbl\ql \cell
\row
\trowd\clbrdrr\brdrs\cellx1150\clbrdrr\brdrs\cellx2300
\pard\intbl\ql {over the internal}}\cell
\pard\intbl\ql {{\par
\pard\qc\sl240\slmult1 \fi0 Source code and data transfer}\cell
\pard\intbl\ql \cell
\row
\trowd\clbrdrr\brdrs\cellx1150\clbrdrr\brdrs\cellx2300
\pard\intbl\ql {over the network}}\cell
\pard\intbl\ql {{\par
\pard\qc\sl240\slmult1 \fi0 Explore caching for}\cell
\pard\intbl\ql \cell
\row
\trowd\clbrdrr\brdrs\cellx1150
\pard\intbl\ql {better performance}}\cell
\pard\intbl\ql \cell
\row
\trowd\clbrdrr\brdrs\cellx1150
\pard\intbl\ql {{\par
\pard\qc\sl240\slmult1 \fi0 Resource and}\cell
\pard\intbl\ql \cell
\row
\trowd\clbrdrr\brdrs\cellx1150\clbrdrr\brdrs\cellx2300
\pard\intbl\ql {Failure Management}}\cell
\pard\intbl\ql {{\par
{\i }\pard\qc\sl240\slmult1 \fi0 }\cell
\pard\intbl\ql \cell
\row
\trowd\clbrdrr\brdrs\cellx1150
\pard\intbl\ql {{\i clWaitEvents},}\cell
\pard\intbl\ql \cell
\row
\trowd\clbrdrr\brdrs\cellx1150
\pard\intbl\ql {{\i clFlush},}\cell
\pard\intbl\ql \cell
\row
\trowd\clbrdrr\brdrs\cellx1150
\pard\intbl\ql {{\i clFinish},}\cell
\pard\intbl\ql \cell
\row
\trowd\clbrdrr\brdrs\cellx1150
\pard\intbl\ql {{\i clReleaseBuffer},}\cell
\pard\intbl\ql \cell
\row
\trowd\clbrdrr\brdrs\cellx1150
\pard\intbl\ql {{\i clReleaseProgram}}\cell
\pard\intbl\ql \cell
\row
\trowd\clbrdrr\brdrs\cellx1150\clbrdrr\brdrs\cellx2300
\pard\intbl\ql {{\i }}}\cell
\pard\intbl\ql {{\par
\pard\qc\sl240\slmult1 \fi0 Check on status and}\cell
\pard\intbl\ql \cell
\row
\trowd\clbrdrr\brdrs\cellx1150\clbrdrr\brdrs\cellx2300
\pard\intbl\ql {cleanup}}\cell
\pard\intbl\ql {{\par
\pard\qc\sl240\slmult1 \fi0 Add support for network failure}\cell
\pard\intbl\ql \cell
\row
\trowd\clbrdrr\brdrs\cellx1150\clbrdrr\brdrs\cellx2300
\pard\intbl\ql {and cleanup}}\cell
\pard\intbl\ql {{\par
\pard\qc\sl240\slmult1 \fi0 Enable timeout for}\cell
\pard\intbl\ql \cell
\row
\trowd\clbrdrb\brdrs\clbrdrr\brdrs\cellx1150
\pard\intbl\ql {slow responses}}\cell
\pard\intbl\ql \cell
\row
}  \par
}}\pard\qj\sl240\slmult1 \sb360 \fi0 {\s4\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMoffloading_rpc}2.2{\*\bkmkend BMoffloading_rpc}  Integration of OpenCL API with RPC Service\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 In order to support offloading on the remote resources, the offloading framework utilizes an RPC-based service which handles offloading requests received over the virtual private network. As an first attempt, SunRPC service was utilized to provide the remote procedure calling interface, serialization, and networking capabilities. However, SunRPC provides many extra features that are not necessarily efficient(for example, the use of a portmapper daemon to discover the listening port of the RPC service). SunRPC also initiates a new TCP connection for each function call which incurs extra delay and the overhead on network performance. In contrast, the proposed design uses a single TCP connection per offloading job thus achieving lower latencies. By running an RPC service which exposes the OpenCL API over the network, the offloading framework provides a computation offloading design that is lightweight in terms of argument serialization and buffer management. Other approaches\~vocl rely on more sophisticated communication primitives such as MPI which require extra processing and memory resulting in poor battery performance.\par
{\pard\qj\sl240\slmult1 \sb240 \fi360 \par
\pard\qc\sl240\slmult1 \fi0 {file=figs/code\\s\\do5({\fs16 r})pc.eps, width=4.0in} \par
\pard\qc\sl240\slmult1 \fi0 {Figure {\*\bkmkstart BMfig_code_rpc}2{\*\bkmkend BMfig_code_rpc}: Code Snippet for Integration of OpenCL API with RPC}{\field{\*\fldinst TC "2 Code Snippet for Integration of OpenCL API with RPC" \\f f}{\fldrslt }}\par
}\pard\qj\sl240\slmult1 \sb240 \fi0 The integration of OpenCL API with RPC-based service is realized as a wrapper library which provides an identical interface to the original OpenCL API. Figure\~{\field{\*\fldinst{\lang1024 REF BMfig_code_rpc \\* MERGEFORMAT }}{\fldrslt{?}}} presents the codes snippet for wrapped OpenCL API, {\scaps0\b0\i {clGetPlatformIDs}}, which is used to retrieve the information of a targeted remote resource. In the wrapper library, if a runtime scheduler, which will be described in section\~{\field{\*\fldinst{\lang1024 REF BMoffloading_scheduler \\* MERGEFORMAT }}{\fldrslt{?}}}, decides to execute the workload locally, the wrapper library routes these API invocations to the original local OpenCL library to invoke the corresponding OpenCL API. On the other hand, the wrapper library serializes the API name and arguments for remote execution, invokes the corresponding remote procedure call, and sends them to the targeted remote resource over the network. For data serialization, I use the efficient serialization package called TPL data serialization\~tpl. TPL can be used to serialize files, memory buffers and file descriptors so that it is appropriate for use as a file format or inter-process communication message format.\par
\pard\qj\sl240\slmult1 \sb120 \fi0 {\s4\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMoffloading_scheduler}2.3{\*\bkmkend BMoffloading_scheduler}  Runtime Scheduler\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 As previously mentioned, the OpenCL-based remote offloading framework provides developers with a list of accelerators(both local and remote) to select as offloading targets. However, some application developers may not want to bother with this cumbersome task of figuring out which devices would make a good candidate for local execution versus remote offloading. The proposed framework provides a runtime scheduler which plays a key role in the framework because it is the module that profiles performance dynamically at runtime and codifies the policies designed to maximize energy performance by selectively offloading tasks.\par
\pard\qj\sl240\slmult1 \fi0 In the current implementation, the runtime scheduler consists of two main parts: a resource profiler and a decision maker. The default behavior of the resource profiler is to keep track of network latencies to available remote resources and when the decision maker requests the target to offload the workload, the profiler provides the remote resource with lowest latency. Then, the decision maker determines dynamically if the workload should be offloaded or executed locally. I will explain the machine learning-based decision making in section\~{\field{\*\fldinst{\lang1024 REF BMchap_scheduler \\* MERGEFORMAT }}{\fldrslt{?}}} in detail.\par
\pard\qj\sl240\slmult1 \fi0 Currently, the resource profiler is an overly simplistic model by considering only network latency as a feature for remote resource selection. Therefore, I plan on supporting a more complex set of conditions to select the best remote resource. For instance, by periodically recording the network conditions (i.e. latency and bandwidth) measured at the VPN layer, and others features such as computing capabilities of remote resources, the profiler can automatically make the selection for which remote resources would yield the best performance for a given condition while maximizing a certain objective function on the utilization for remote resources.\par
\pard\qj\sl240\slmult1 \fi0 This utility function-based resource profiler is motivated by my previous research work on a peer-to-peer self-organizing and self-managing cluster system based upon network coordinates and utility function called SOLARE which stands for self-organizing latency-aware resource ensemble\~solare. In the system, a peer-to-peer node periodically monitors the status of the cluster in which it is currently involved, and whenever the utility of the cluster is dropped below a threshold, it migrates to another cluster to maximize the utility value. In order to make use of utility function, I used two features: the virtual distance to the cluster in artificial network coordinate system and the number of current members in the cluster.\par
\pard\qj\sl240\slmult1 \fi0 By adopting the module of utility function with different features for utility function (i.e. latency, bandwidth, computing capabilities of remote resources and so on), I expect that the remote resource profiler provides more flexible ability of on-demand resource selection. \par
\pard\qj\sl240\slmult1 \sb120 \fi0 {\s4\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMoffloading_discovery}2.4{\*\bkmkend BMoffloading_discovery}  Decentralized Resource Discovery\par
}{\pard\qj\sl240\slmult1 \sb300 \fi0 \par
\pard\qc\sl240\slmult1 \fi0 {file=figs/discovery.eps, width=6.5in} \par
\pard\qc\sl240\slmult1 \fi0 {Figure {\*\bkmkstart BMfig_discovery}3{\*\bkmkend BMfig_discovery}: Decentralized Resource Discovery}{\field{\*\fldinst TC "3 Decentralized Resource Discovery" \\f f}{\fldrslt }}\par
}\pard\qj\sl240\slmult1 \sb240 \fi0 Current mobile offloading solutions rely on a centralized service ot provide the resource discovery capability. A key differentiation of my approach is the decentralized IP multicast-based remote resource discovery subsystem. A challenge with IP multicast resource discovery mechanism is that the vast majority of ISPs prevent multicast traffic from traversing their network. I sidestep this issue with network virtualization through SocialVPN. Because the OpenCL-based remote offloading system is deployed on top of SocialVPN which enables IP multicasting over the Internet, it is possible to leverage existing LAN-based service discovery techniques for wide area environment as well as local area environment. The way that the decentralized remote resource discovery mechanism works is illustrated in Figure\~{\field{\*\fldinst{\lang1024 REF BMfig_discovery \\* MERGEFORMAT }}{\fldrslt{?}}}. The client periodically polls the network for eligible remote resources by sending a UDP datagram to the multicast IP address. The SocialVPN router distributes the IP packet to every remote resource in the private network. The RPC-based service described in section\~{\field{\*\fldinst{\lang1024 REF BMoffloading_rpc \\* MERGEFORMAT }}{\fldrslt{?}}} has a UDP listening thread that waits for resource discovery request and responds to the request with its computing capabilities using the requestor\rquote s unicast IP address. The requester waits for a certain amount of time and accumulates all the replies that it receives within that time window. The requestor records their latency, bandwidth, and processing capabilities and provides them to the scheduler. The scheduler then determines which remote resource will provide the best performance and selects that resource as the targeted candidate. \par
\pard\qj\sl240\slmult1 \sb120 \fi0 {\s4\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMoffloading_vpn}2.5{\*\bkmkend BMoffloading_vpn}  Trusted Communication via VPN\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 The virtual networking component addresses two key challenges of remote workload offloading \endash  privacy and peer discovery \endash  while supporting unmodified TCP/IP applications to offload computation to remote computing resources. In order to augment the mobile platform\rquote s computing capabilities, it is important to find trusted resources that are not only in the same local area network, but also geographically-dispersed peers, and to do so dynamically an transparently to the mobile user\rquote s application. While many VPN tunneling techniques would be applicable (e.g. OpenVPN\~openvpn, Hamachi\~hamachi), the approach chosen in the prototype is SocialVPN\~socialvpn, due to its ability to autonomously create and manage VPN links to social peers, and its support for tunneling IP multicast and discovery. SocialVPN is a decentralized, self-configuring VPN based on online social networking services such as XMPP and a public-key based security model where certificates are exchanged and used to setup VPN links automatically using online social network APIs. SocialVPN ensures that friends anywhere on the Internet appear to be on the same virtual LAN and end-to-end encrypted peer-to-peer tunnels are abstracted as virtual IP links among friends. SocialVPN autonomously and continuously takes care of discovering and exchanging public-key VPN certificates with social peers through online social network infrastructures, handling the assignment and mapping of virtual private IP addresses to devices, creating security associations among peers, and capturing/routing IP packets for secure end-to-end tunneling leveraging a peer-to-peer overlay. By leveraging SocialVPN as a trusted peer-to-peer messaging substrate, it is possible to use the Berkeley sockets networking interface to offload the mobile workloads without any direct linking to SocialVPn itself. Most peer-to-peer systems require integration with a peer-to-peer library as well as a learning curve for learning curve for learning its API. Because SocialVPN provides virtual private IP addresses to friends instead of P2P addresses, it supports unmodified applications.\par
\pard\qj\sl240\slmult1 \sb240 \fi0 {\s3\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMoffloading_evaluation}3{\*\bkmkend BMoffloading_evaluation}  Evaluation\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 In this section, I evaluate the implementation of the OpenCL-based remote offloading framework for mobile platforms in terms of the performance and energy consumption for mobile devices through real deployment over local and wide area environments. Firstly, I quantitatively analyze the overhead of the proposed resource discovery mechanism in terms of energy consumption as a function of number of remote resources. Next, I will demonstrate the ability of the decentralized resource discovery mechanism to dynamically discover remote resources under the situation where the client offloads the workload iteratively to the best resource with respect to the network latency while a few remote resources occasionally join or leave the private network. Also, I examine the overhead of adopting SocialVPN to the secure communication between the client and the remote resource since SocialVPN utilizes its own encryption and IP tunneling. \par
\pard\qj\sl240\slmult1 \sb120 \fi0 {\s4\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMoffloading_setup}3.1{\*\bkmkend BMoffloading_setup}  Experimental Setup\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 In order to evaluate our remote offloading framework under a variety of possible use case scenarios, I setup the experiments using various hardware and network configurations. First of all, for the mobile client, I utilized an Android table, Samsung GalaxyTab 10.1 equipped with 1Ghz dual-core processor and 1GB RAM, and running Android 3.1. For the SocialVPN-connected remote resources, I utilized a workstation with Intel 3.0Ghz Core2 Duo processor and 8GB memory installed with Linux OS. In addition, I used Amazon EC2 GPU cluster resource\~amazonec2 and FutureGrid virtual instances\~futuregrid for the remote resources which have different computing capabilities and network performance.\par
\pard\qj\sl240\slmult1 \fi0 For offloaded OpenCL workloads, I utilized OpenCL SDK code samples provided by AMD SDK\~amd and NVIDIA\~nvidia such as Sobelfilter, matrix multiplication, N-body physics, and hidden Markov model{\up16\chftn}
{\*\footnote \pard\plain\s246\f13\fs24 {\up16\chftn}I will describe more details about OpenCL workloads that I chose for the experiments in Section\~{\field{\*\fldinst{\lang1024 REF BMchap_character \\* MERGEFORMAT }}{\fldrslt{?}}}}
. Also, I emulate different wide area network conditions between the client and the remote resource by controlling the network latency using Traffic Control(TC)\~tc. TC is a network tool which provides functionality to control network traffic by prioritizing network resources and using concepts of traffic classification, queue disciplines and quality of service. To profile energy consumption of the mobile device, I used PowerTutor\~powertutor which is an application for the variants of Android devices that displays the power consumed by major components such as CPU, network interface, LCD display, and GPS receiver. \par
\pard\qj\sl240\slmult1 \sb120 \fi0 {\s4\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMoffloading_overhead_discovery}3.2{\*\bkmkend BMoffloading_overhead_discovery}  Overhead of Resource Discovery\par
}{\pard\qj\sl240\slmult1 \sb300 \fi0 \par
\pard\qc\sl240\slmult1 \fi0 {file=figs/energy\\s\\do5({\fs16 d})iscovery.eps, width=4.5in} \par
\pard\qc\sl240\slmult1 \fi0 {Figure {\*\bkmkstart BMfig_energy_discovery}4{\*\bkmkend BMfig_energy_discovery}: Energy Consumption for Resource Discovery Mechanism}{\field{\*\fldinst TC "4 Energy Consumption for Resource Discovery Mechanism" \\f f}{\fldrslt }}\par
}\pard\qj\sl240\slmult1 \sb240 \fi0 A novelty of this work is to allow the users to dynamically and transparently discovery the remote resources through IP multicasting on top of SocialVPN. However, as the number of remote resources increases, the burden that the mobile device has to handle also increases as well. For that reason, it is essential to quantify the costs of the resource discovery mechanism in terms of energy consumption of the mobile devices. I have conducted a WAN experiment in which 10 remote resources bound to SocialVPN overlay are deployed in virtual machines at FutureGrid\~futuregrid resources at University of Chicago, University of California San Diego, and University of Texas, while a client located at University of Florida requests the resource discovery. Figure\~{\field{\*\fldinst{\lang1024 REF BMfig_energy_discovery \\* MERGEFORMAT }}{\fldrslt{?}}} shows energy consumption while the mobile device performs the resource discovery for the given number of resources. As shown in Figure\~{\field{\*\fldinst{\lang1024 REF BMfig_energy_discovery \\* MERGEFORMAT }}{\fldrslt{?}}}, up to six resources, energy consumption of the resource discovery does not vary significantly, at less than 0.1J, regardless of the number of the resources. However, from seven resources onward, energy consumption jumps up to 0.7J. This is because a typical Wi-Fi networking device can tolerate the number of packets to multicast resource request messages and to receive the replies from up to six resources without entering into high power mode. However, the number of packets with seven resources crosses this threshold pushing the Wi-Fi device into high power mode and resulting in more energy consumption. It is worth noting that the overhead of energy consumption from resource discovery mechanism depends on the interval of resource discovery which means the trade-off between energy consumption and the accuracy of the network characteristics of available remote resources. In current setting, the resource discovery is repeated every 60 seconds which means only 4.6% of the overhead compared with when matrix multiplication is executed locally consuming 15J of energy for 60 seconds(Section\~{\field{\*\fldinst{\lang1024 REF BMcharacter_energy \\* MERGEFORMAT }}{\fldrslt{?}}}). \par
\pard\qj\sl240\slmult1 \sb120 \fi0 {\s4\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMoffloading_dynamic}3.3{\*\bkmkend BMoffloading_dynamic}  Dynamic Resource Discovery\par
}{\pard\qj\sl240\slmult1 \sb300 \fi0 \par
\pard\qc\sl240\slmult1 \fi0 {file=figs/dynamic\\s\\do5({\fs16 d})iscovery1.eps, width=4.5in} \par
\pard\qc\sl240\slmult1 \fi0 {Figure {\*\bkmkstart BMfig_dynamic_discovery1}5{\*\bkmkend BMfig_dynamic_discovery1}: Bandwidth Measurement}{\field{\*\fldinst TC "5 Bandwidth Measurement" \\f f}{\fldrslt }}\par
}{\pard\qj\sl240\slmult1 \sb480 \fi0 \par
\pard\qc\sl240\slmult1 \fi0 {file=figs/dynamic\\s\\do5({\fs16 d})iscovery2.eps, width=4.5in} \par
\pard\qc\sl240\slmult1 \fi0 {Figure {\*\bkmkstart BMfig_dynamic_discovery2}6{\*\bkmkend BMfig_dynamic_discovery2}: Performance Measurement of Remote Offloading}{\field{\*\fldinst TC "6 Performance Measurement of Remote Offloading" \\f f}{\fldrslt }}\par
}\pard\qj\sl240\slmult1 \sb240 \fi0 In this section, I demonstrate the ability to dynamically discover the remote resources while the resources join or leave the networks and the client offloads hidden Markov model to the remote resource. For this, I designed a simple experiment in which the client offloads the workload iteratively to the best resource with respect to the network latency while a few servers occasionally join or leave the network.\par
\pard\qj\sl240\slmult1 \fi0 Firstly, when the client starts executing the workload, it has only one available remote resource launched at Amazon EC2 GPU cluster and offloads the workloads until a better remote resource is available. At the fourth iteration, a new remote resource, which has less than 10ms latency, joins the network. As a result, the client connects to the new remote resource and switches offloading to the new resource. And then, at the sixth iteration, the remote server leaves the network but Amazon EC2 resource still remains, so the client offloads the workload to Amazon EC2 resource again. Finally, another new remote resource with 50ms latency joins and the client contacts to that resource. Figure\~{\field{\*\fldinst{\lang1024 REF BMfig_dynamic_discovery1 \\* MERGEFORMAT }}{\fldrslt{?}}} and \~{\field{\*\fldinst{\lang1024 REF BMfig_dynamic_discovery2 \\* MERGEFORMAT }}{\fldrslt{?}}} show the available bandwidth to the best resource and the performance of remote offloading respectively while resources join or leave the private network. \par
\pard\qj\sl240\slmult1 \sb120 \fi0 {\s4\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMoffloading_overhead_socialvpn}3.4{\*\bkmkend BMoffloading_overhead_socialvpn}  Overhead of SocialVPN\par
}{\pard\qj\sl240\slmult1 \sb300 \fi0 \par
\pard\qc\sl240\slmult1 \fi0 {file=figs/overhead\\s\\do5({\fs16 s})ocialvpn1.eps, width=4.5in} \par
\pard\qc\sl240\slmult1 \fi0 {Figure {\*\bkmkstart BMfig_overhead_socialvpn1}7{\*\bkmkend BMfig_overhead_socialvpn1}: Overhead of SocialVPN in terms of Execution Time}{\field{\*\fldinst TC "7 Overhead of SocialVPN in terms of Execution Time" \\f f}{\fldrslt }}\par
}{\pard\qj\sl240\slmult1 \sb480 \fi0 \par
\pard\qc\sl240\slmult1 \fi0 {file=figs/overhead\\s\\do5({\fs16 s})ocialvpn2.eps, width=4.5in} \par
\pard\qc\sl240\slmult1 \fi0 {Figure {\*\bkmkstart BMfig_overhead_socialvpn2}8{\*\bkmkend BMfig_overhead_socialvpn2}: Overhead of SocialVPN in terms of Energy Consumption}{\field{\*\fldinst TC "8 Overhead of SocialVPN in terms of Energy Consumption" \\f f}{\fldrslt }}\par
}\pard\qj\sl240\slmult1 \sb240 \fi0 In the OpenCL-based remote offloading framework, SocialVPN enables mobile devices and remote resources to securely communicate. This incurs overheads due to encryption and tunneling. In this section, I investigate the overhead of SocialVPN with respect to the performance and energy consumption. In order to measure the overhead of SocialVPN, I have conducted the experiments in local area networks in which the client offloads OpenCL workloads to the remote resource both with and without SocialVPN. I do not consider the wide area network experiments for this comparison because the resource discovery mechanism without SocialVPN is not viable in wide area networks due to hindering of multicast by ISPs.\par
\pard\qj\sl240\slmult1 \fi0 Figure\~{\field{\*\fldinst{\lang1024 REF BMfig_overhead_socialvpn1 \\* MERGEFORMAT }}{\fldrslt{?}}} and \~{\field{\*\fldinst{\lang1024 REF BMfig_overhead_socialvpn2 \\* MERGEFORMAT }}{\fldrslt{?}}} show the offloading execution time and energy consumption with and without SocialVPN, respectively. Even though I carried out the experiments using all of the workloads we implemented, I only present the results from Sobelfilter because other workloads also show the similar trend as Sobelfilter. As shown in Figure\~{\field{\*\fldinst{\lang1024 REF BMfig_overhead_socialvpn1 \\* MERGEFORMAT }}{\fldrslt{?}}}, as the image size increases, the execution time and energy consumption also increase. In the case of 480{{{\f6\'B4}}}270 of image size, for instance, offloading with SocialVPN takes 0.05s more than without SocialVPN while it takes 0.6s more to offload 1920{{{\f6\'B4}}}1080 of image size which means that the overhead ranges from 2.8% to 5.6%. In Figure\~{\field{\*\fldinst{\lang1024 REF BMfig_overhead_socialvpn2 \\* MERGEFORMAT }}{\fldrslt{?}}}, I also observed the overhead ranging from 2.6% to 8.1% in terms of energy consumption. SocialVPN appends 80 bytes of an additional header to every packet to tunnel regular TCP/IP packets where the default MTU(Maximum Transmission Unit) for the experimental setup is 1,480 bytes. For that reason, I approximately calculated about 6% of the overhead coming from SocialVPN.\par
\pard\qj\sl240\slmult1 \sb240 \fi0 {\s3\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMoffloading_summary}4{\*\bkmkend BMoffloading_summary}  Summary\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 In this section, I presented the OpenCL-based remote offloading framework designed specifically for mobile cloud computing where OpenCL workloads can be exported from a mobile device (i.e. an Android device) to trusted remote resource (i.e. friend\rquote s desktop or family\rquote s laptop) or even to the cloud resource (i.e. an Amazon EC2 instance with GPU access). The proposed remote offloading system consists of the following main components: 1) a customized RPC-based service with optimizations for network tasking and data marshalling, 2) a resource discovery mechanism which selects the remote resource with the lowest latency, and 3) a virtual private networking layer which provides transparent network encryption without any modification at the application layer.\par
\pard\qj\sl240\slmult1 \fi0 First of all, the prototype of the proposed remote offloading framework is implemented as a wrapper library around the OpenCL API while allowing transparent integration of the OpenCL API with RPC-based service without any code modification. Also, the IP multicast-based resource discovery mechanism makes it possible for the mobile device to dynamically discover remote resources during runtime and to utilize the computing capabilities of remote resources. Furthermore, the proposed approach supports accessing resources beyond the local private network, broadening the accessibility to trusted remote resources across the Internet and the cloud. This is accomplished by utilizing a social peer-to-peer virtual private network, SocialVPN and creating a Social Area Network in which only trusted social peers\rquote  computing resources are involved.\par
\pard\qj\sl240\slmult1 \fi0 The evaluation showed that the use of SocialVPN resulted in a reasonably acceptable overhead while guaranteeing secure connections and data privacy. Also, I demonstrated the ability to dynamically discover remote resources while remote resources join or leave the network.\par
\page{}\pard\qj\sl240\slmult1 \fi0 {\s2\ql\sb240\sa60\keepn\f13\b\fs40 Chapter {\*\bkmkstart BMchap_character}4{\*\bkmkend BMchap_character}\par
\pard\qj\sl240\slmult1 \sb240 \fi0 WORKLOAD CHARACTERIZATION\par
\par\par}\pard\qj\sl240\slmult1 \sb60 \fi0 In this section, I characterize mobile workloads for the suitability of offloading from the perspective of {\scaps0\b0\i Computation to Communication ratio} which is calculated by the time for workloads to be executed locally divided by the data transfer time for the remote offloading. Thus in this work, computation to communication ratio for each workload is a comprehensive measurement which mirrors three parameters such as the volume of computation of workloads, the amount of data to be transferred, and the network conditions. As part of the workload characterization effort through computation to communication ratio, I strategically selected four quantitatively and qualitatively different OpcnCL workloads for the analysis: Sobelfilter, matrix multiplication, {\scaps0\b0\i N}-body physics and hidden Markov model, each being classified into two categories: the computation-intensive workload (matrix multiplication and {\scaps0\b0\i N}-body physics) and the communication-intensive workload (Sobelfilter and hidden Markov model) in terms of computation to communication ratio. Note that though computation- or communication-intensity of four OpenCL workloads is not absolute, it is worth to categorize the relative computation- or communication-intensity of the workloads so it is possible to explore the workload conditions where offloading is more beneficial than local processing.\par
\pard\qj\sl240\slmult1 \fi0 Furthermore, I deployed the offloading framework into the various types of network such as local area networks, campus networks and Amazon EC2 instance which have different network restrictions, and set various levels of the computing capability of the remote resource from general purpose CPUs to Amazon EC2 GPUs cluster instance to analyze the behavior of the remote offloading framework in terms of the performance and energy implication of mobile devices in accordance with environmental factors such as network conditions and the computing capabilities of offloadable resources. For example, I configure local area networks by directly connecting a mobile device and a remote resource with Nvidia graphics processing units via a wireless router which represents the best resource of our experimental setups. The experimental results show that although not all types of workloads benefit from mobile workload offloading, there clearly exist a class of workloads and environmental conditions that can leverage the remote offloading. \par
\pard\qj\sl240\slmult1 \sb240 \fi0 {\s3\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMcharacter_relatedworks}1{\*\bkmkend BMcharacter_relatedworks}  Related Works\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 In\~fullsystem, the authors characterize the microarchitectural behavior of smartphone applications through several representative benchmarks including the following areas: an interactive game, a streaming player and mp3 audio player. Also, they developed a new benchmark to characterize the performance of a web browser called BBench. Through those benchmarks, they show how they differ from SPEC CPU2006 benchmarks which are widely used for the measurement of compute-intensive performance. MEVBench\~mevbench presented a custom benchmark suite for full mobile vision applications such as augmented reality as well as components of common vision algorithms such as SIFT feature extraction and SVM classification. The authors evaluated the performance of MEVBench with various platforms for the direction of future mobile embedded vision architectures. Also, they show that mobile vision architectures require to be supported from heterogeneous computation for performance improvement. In\~characterization, the performance and energy benefits of mobile heterogeneous computing are characterized by using 2D FFT(Fast Fourier Transformation), matrix multiplication, and 2D Stencil as the benchmarks. In this study, the authors demonstrated that fully utilizing available computing cores to complete a task can achieve an 3.7{{{\f6\'B4}}} speed-up over the case of the single-threaded CPU, consequently, reduce the energy consumption for the mobile platform.\par
\pard\qj\sl240\slmult1 \fi0 In contrast with the aforementioned studies which designed new benchmarks or characterized the performance and energy benefits of the mobile platforms via the various workloads from a perspective of mobile computing capabilities, we characterized the behavior of mobile offloading framework while considering the network conditions as well as the computing capabilities of remote resources. \par
\pard\qj\sl240\slmult1 \sb240 \fi0 {\s3\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMcharacter_methodology}2{\*\bkmkend BMcharacter_methodology}  Methodology\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 The main contribution of this work is to analyze the behavior of the remote offloading in accordance with the characteristics of the workloads and environmental factors such as network conditions and the level of computing capabilities of remote resources. In this section, I explain the workloads used for the analysis and the way to characterize the workloads as well as experimental setup. Every experiment is repeated 5 times and the results presented in the paper are the averaged values. \par
\pard\qj\sl240\slmult1 \sb120 \fi0 {\s4\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMcharacter_setup}2.1{\*\bkmkend BMcharacter_setup}  Hardware Setup\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 In order to analyze the behavior of our remote offloading framework under the various network conditions and the computing capabilities of remote resources, we setup the experiment using various hardwares and network configurations. First of all, the hardware setup consists of a client and various types of remote resources. I have utilized Android tabletPC, Samsung GalaxyTab 10.1 equipped with 1GHz dual-core process and 1GB RAM, and running Android honeycomb as the mobile client. I adopted three types of servers: CPU only-installed server, GPU-installed server, and Amazon EC2 GPU cluster. For CPU only-installed server without a graphics card, we utilized a workstation with Intel 3.0GHz Core 2 Duo processor and 8GB memory installed with Linux OS. Also, I installed GeForce GT 640 graphics card with 2GB RAM for the server with GPUs. In addition, I used Amazon EC2 GPU cluster resource with two Nvidia GPUs equipped with 3GB on-board memory per GPU.\par
\pard\qj\sl240\slmult1 \fi0 In order to analyze the impact of network conditions on offloading performance, I configured both local and wide area networks using following setup. Firstly, I built a local area network by directly connecting the mobile client and the server through a wireless router supporting 802.11 b/g/n network standard. Secondly, I used our campus network in order to configure a wide-area network in which the client and the server connect to a wireless and wired network respectively. Also, I emulate different wide-area network conditions between the client and the server by controlling the network latency using Traffic Control (TC)\~tc. TC is a network tool which provides functionalities to control network traffic by prioritizing network resources and using concepts of traffic classification, queue disciplines and quality of service(QoS). Furthermore, utilizing an instance of Amazon EC2 GPU cluster gives us one more option for the server located in wide-area network. Table\~{\field{\*\fldinst{\lang1024 REF BMtable_network_summary \\* MERGEFORMAT }}{\fldrslt{?}}} shows the average and standard deviation of network latency and bandwidth for our network configurations. {\par
\pard\qj\sl240\slmult1 \sb240 \fi0 \par
\pard\qc\sl240\slmult1 \fi0 {Table {\*\bkmkstart BMtable_network_summary}1{\*\bkmkend BMtable_network_summary}: Average and Standard Deviation of Network Latency and Bandwidth for Local and Wide Area Networks including Amazon EC2.}{\field{\*\fldinst TC "1 Average and Standard Deviation of Network Latency and Bandwidth for Local and Wide Area Networks including Amazon EC2." \\f t}{\fldrslt }}\par
{\pard\qc\sl240\slmult1 \fi0 \par
\trowd\clbrdrt\brdrs\clbrdrb\brdrs\clbrdrr\brdrs\cellx985\clmgf\clbrdrt\brdrs\clbrdrb\brdrs\cellx1970\clmrg\clbrdrt\brdrs\clbrdrb\brdrs\clbrdrr\brdrs\cellx2955\clmgf\clbrdrt\brdrs\clbrdrb\brdrs\cellx3940\clmrg\clbrdrt\brdrs\clbrdrb\brdrs\clbrdrr\brdrs\cellx4925\clmgf\clbrdrt\brdrs\clbrdrb\brdrs\cellx5910\clmrg\clbrdrt\brdrs\clbrdrb\brdrs\cellx6895
\pard\intbl\qc {}\cell
\pard\intbl\qc {LAN}\cell
\pard\intbl{}\cell
\pard\intbl\qc {Campus network}\cell
\pard\intbl{}\cell
\pard\intbl\qc {Amazon EC2}\cell
\pard\intbl{}\cell
\row
\trowd\clbrdrr\brdrs\cellx985\cellx1970\clbrdrr\brdrs\cellx2955\cellx3940\clbrdrr\brdrs\cellx4925\cellx5910\cellx6895
\pard\intbl\qc {Latency}\cell
\pard\intbl\qc {Avg.}\cell
\pard\intbl\qc {Stdev.}\cell
\pard\intbl\qc {Avg.}\cell
\pard\intbl\qc {Stdev.}\cell
\pard\intbl\qc {Avg.}\cell
\pard\intbl\qc {Stdev.}\cell
\row
\trowd\clbrdrr\brdrs\cellx985\cellx1970\clbrdrr\brdrs\cellx2955\cellx3940\clbrdrr\brdrs\cellx4925\cellx5910\cellx6895
\pard\intbl\qc {({\i ms})}\cell
\pard\intbl\qc {10.833}\cell
\pard\intbl\qc {2.684}\cell
\pard\intbl\qc {15.465}\cell
\pard\intbl\qc {4.189}\cell
\pard\intbl\qc {74.036}\cell
\pard\intbl\qc {17.737}\cell
\row
\trowd\clbrdrr\brdrs\cellx985\cellx1970\clbrdrr\brdrs\cellx2955\cellx3940\clbrdrr\brdrs\cellx4925\cellx5910\cellx6895
\pard\intbl\qc {Bandwidth}\cell
\pard\intbl\qc {Avg.}\cell
\pard\intbl\qc {Stdev.}\cell
\pard\intbl\qc {Avg.}\cell
\pard\intbl\qc {Stdev.}\cell
\pard\intbl\qc {Avg.}\cell
\pard\intbl\qc {Stdev.}\cell
\row
\trowd\clbrdrb\brdrs\clbrdrr\brdrs\cellx985\clbrdrb\brdrs\cellx1970\clbrdrb\brdrs\clbrdrr\brdrs\cellx2955\clbrdrb\brdrs\cellx3940\clbrdrb\brdrs\clbrdrr\brdrs\cellx4925\clbrdrb\brdrs\cellx5910\clbrdrb\brdrs\cellx6895
\pard\intbl\qc {({\i MB/s})}\cell
\pard\intbl\qc {6.523}\cell
\pard\intbl\qc {0.177}\cell
\pard\intbl\qc {2.461}\cell
\pard\intbl\qc {0.238}\cell
\pard\intbl\qc {0.178}\cell
\pard\intbl\qc {0.023}\cell
\row
}  \par
}\pard\qj\sl240\slmult1 \sb360 \fi0 {\s4\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMcharacter_workloads}2.2{\*\bkmkend BMcharacter_workloads}  OpenCL Workloads\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 I utilized OpenCL SDK code samples provided by AMD APP SDK\~amd and Nvidia\~nvidia to choose appropriate workloads for the remote offloading framework. In order to choose appropriate OpenCL workloads among a set of samples, we expected that the more computation- and less communication-intensive the workloads, the more potential gain from remote offloading. Furthermore, we characterized the workloads in accordance with the amount of computation and communication for input and output data transfer by considering computation to communication ratio calculated by the execution time required to process a certain amount of data locally divided by the data transfer time for the remote offloading. Thus, computation to communication ratio is a comprehensive measurement which mirrors three parameters such as the the volume of computation of workloads, the amount of data to be transferred, and the network conditions. After careful considerations, we chose four qualitatively and quantitatively different workloads: Sobelfilter, matrix multiplication, hidden Markov model, and {\scaps0\b0\i N}-body physics used by a variety of areas such as image processing, physics simulation, and mathematical modeling. Also, since it is necessary to consider an additional interaction between the client and the server such as device configuration or state transfer which occurs further overhead for communication, we examined the programming flow of the application-layer for each workload. The programming flow and structure of workloads are described in the following:\par
\pard\qj\sl240\slmult1 \fi0 {\i0\scaps0\b Sobelfilter:} Sobelfilter is an image processing filter used for image edge detection. It consists of both a 3{{{\f6\'B4}}}3 horizontal and vertical filter. The edge detection process applies two filters into an input image in a sequence and adds final results to a form of the image. Accordingly, the client transfers the horizontal, vertical filter, and the input image as states for the execution and sets them as arguments accessed by the kernel. In the server side, the image is iteratively processed based upon the filters that the client has sent. Once the execution is completed, the server sends back an output image into the client. Figure\~{\field{\*\fldinst{\lang1024 REF BMfig_program_flow1 \\* MERGEFORMAT }}{\fldrslt{?}}} shows the program flow diagram for the data transfer, the argument setup, and the kernel execution for Sobelfilter.\par
\pard\qj\sl240\slmult1 \fi0 {\i0\scaps0\b Matrix multiplication:} Similarly to Sobelfilter, the client sends to floating-point matrices ({\scaps0\b0\i n} by {\scaps0\b0\i n} and {\scaps0\b0\i n} by {\scaps0\b0\i 2n}) and additional small states and arguments necessary for the remote kernel execution. Then, server executes multiplication of two matrices and returns one {\scaps0\b0\i n} by {\scaps0\b0\i 2n} matrix to the client as the result. Therefore, matrix multiplication also follows the program flow of Figure\~{\field{\*\fldinst{\lang1024 REF BMfig_program_flow1 \\* MERGEFORMAT }}{\fldrslt{?}}}.\par
\pard\qj\sl240\slmult1 \fi0 {\i0\scaps0\b {\scaps0\b0\i N}-body physics:} {\scaps0\b0\i N}-body physics is a mathematical simulation for predicting a dynamic system consisting of particles or astronomical objects under physical forces such as gravity. In the OpenCL workload for {\scaps0\b0\i N}-body physics, the client generates initial states for a certain number of objects and sends them as an input data. However, the kernel takes additional arguments for each iteration of kernel execution incurring additional communication between the client and the server for the arguments setup. The program flow for {\scaps0\b0\i N}-body physics is shown in Figure\~{\field{\*\fldinst{\lang1024 REF BMfig_program_flow2 \\* MERGEFORMAT }}{\fldrslt{?}}}.\par
\pard\qj\sl240\slmult1 \fi0 {\i0\scaps0\b Hidden Markov model:} Hidden Markov model is a popular statistical tool for modeling generative sequences which can be characterized by observable sequences. It has been applied to a wide range of applications such as image, speech recognition, computational biology, bioinformatics and environment engineering. The OpenCL workload for hidden Markov model follows similar steps for kernel execution as {\i N}-body physics. The client generates and transfers initial states and transition probabilities to the server. Also, the client sends different arguments in order to execute each iteration of the kernel. Therefore, the program flow for hidden Markov model is represented by Figure\~{\field{\*\fldinst{\lang1024 REF BMfig_program_flow2 \\* MERGEFORMAT }}{\fldrslt{?}}}. {\par
\pard\qj\sl240\slmult1 \sb240 \fi360 \par
\pard\qc\sl240\slmult1 \fi0 {file=figs/program\\s\\do5({\fs16 f})low1.eps, width=4.0in} \par
\pard\qc\sl240\slmult1 \fi0 {Figure {\*\bkmkstart BMfig_program_flow1}1{\*\bkmkend BMfig_program_flow1}: Program Flow Diagram for OpenCL Workloads(A)}{\field{\*\fldinst TC "1 Program Flow Diagram for OpenCL Workloads(A)" \\f f}{\fldrslt }}\par
}{\pard\qj\sl240\slmult1 \sb480 \fi0 \par
\pard\qc\sl240\slmult1 \fi0 {file=figs/program\\s\\do5({\fs16 f})low2.eps, width=4.0in} \par
\pard\qc\sl240\slmult1 \fi0 {Figure {\*\bkmkstart BMfig_program_flow2}2{\*\bkmkend BMfig_program_flow2}: Program Flow Diagram for OpenCL Workloads(B)}{\field{\*\fldinst TC "2 Program Flow Diagram for OpenCL Workloads(B)" \\f f}{\fldrslt }}\par
}\pard\qj\sl240\slmult1 \sb480 \fi0 {\s3\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMcharacter_analysis}3{\*\bkmkend BMcharacter_analysis}  Analysis\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 In this section, I analyze the behavior of the OpenCL-based remote offloading framework for mobile platforms in accordance with the workload characteristics and environmental factors such as network conditions and the computing capabilities of remote resources through real deployment over local and wide area environments. First of all, I observe computation to communication ratios of the workloads for the given size of data and network configurations. Next, I evaluate the efficacy and the cost of the remote offloading framework with the various network configurations and the remote resources. Table\~{\field{\*\fldinst{\lang1024 REF BMtable_workload_summary_sm \\* MERGEFORMAT }}{\fldrslt{?}}} and\~{\field{\*\fldinst{\lang1024 REF BMtable_workload_summary_hn \\* MERGEFORMAT }}{\fldrslt{?}}} summarize the amount of input and output data transfer, and the number of additional API calls for argument setup required to execute the OpenCL kernel code for the given size of data or the number of iterations. { {\par
\pard\qj\sl240\slmult1 \sb240 \fi0 \par
\pard\qc\sl240\slmult1 \fi0 {Table {\*\bkmkstart BMtable_workload_summary_sm}2{\*\bkmkend BMtable_workload_summary_sm}: Summary of Input, Output, and Number of API Calls for Sobelfilter and Matrix Multiplication}{\field{\*\fldinst TC "2 Summary of Input, Output, and Number of API Calls for Sobelfilter and Matrix Multiplication" \\f t}{\fldrslt }}\par
{\pard\qc\sl240\slmult1 \fi0 \par
\trowd\clmgf\clbrdrt\brdrs\clbrdrb\brdrs\clbrdrr\brdrs\cellx690\clmrg\clbrdrt\brdrs\clbrdrb\brdrs\cellx1380\clmrg\clbrdrt\brdrs\clbrdrb\brdrs\cellx2070\clmrg\clbrdrt\brdrs\clbrdrb\brdrs\cellx2760\clmrg\clbrdrt\brdrs\clbrdrb\brdrs\clbrdrr\brdrs\cellx3450\clmgf\clbrdrt\brdrs\clbrdrb\brdrs\clbrdrr\brdrs\cellx4140\clmrg\clbrdrt\brdrs\clbrdrb\brdrs\cellx4830\clmrg\clbrdrt\brdrs\clbrdrb\brdrs\cellx5520\clmrg\clbrdrt\brdrs\clbrdrb\brdrs\cellx6210\clmrg\clbrdrt\brdrs\clbrdrb\brdrs\cellx6900
\pard\intbl\qc {Sobelfilter}\cell
\pard\intbl{}\cell
\pard\intbl{}\cell
\pard\intbl{}\cell
\pard\intbl{}\cell
\pard\intbl\qc {Matrix multiplication}\cell
\pard\intbl{}\cell
\pard\intbl{}\cell
\pard\intbl{}\cell
\pard\intbl{}\cell
\row
\trowd\clbrdrr\brdrs\cellx690\clbrdrr\brdrs\cellx1380\clbrdrr\brdrs\cellx2070\clbrdrr\brdrs\cellx2760\clbrdrr\brdrs\cellx3450\clbrdrr\brdrs\cellx4140\clbrdrr\brdrs\cellx4830\clbrdrr\brdrs\cellx5520\clbrdrr\brdrs\cellx6210\cellx6900
\pard\intbl\qc {Image size}\cell
\pard\intbl\qc {480{{{\f6\'B4}}}270}\cell
\pard\intbl\qc {960{{{\f6\'B4}}}540}\cell
\pard\intbl\qc {1440{{{\f6\'B4}}}810}\cell
\pard\intbl\qc {1920{{{\f6\'B4}}}1080}\cell
\pard\intbl\qc {Matrix size}\cell
\pard\intbl\qc {160{{{\f6\'B4}}}320}\cell
\pard\intbl\qc {400{{{\f6\'B4}}}800}\cell
\pard\intbl\qc {560 {{{\f6\'B4}}}1120}\cell
\pard\intbl\qc {720{{{\f6\'B4}}}1440}\cell
\row
\trowd\clbrdrr\brdrs\cellx690\clbrdrr\brdrs\cellx1380\clbrdrr\brdrs\cellx2070\clbrdrr\brdrs\cellx2760\clbrdrr\brdrs\cellx3450\clbrdrr\brdrs\cellx4140\clbrdrr\brdrs\cellx4830\clbrdrr\brdrs\cellx5520\clbrdrr\brdrs\cellx6210\cellx6900
\pard\intbl\qc {Input}\cell
\pard\intbl\qc {390}\cell
\pard\intbl\qc {1,520}\cell
\pard\intbl\qc {3,310}\cell
\pard\intbl\qc {5,790}\cell
\pard\intbl\qc {Input}\cell
\pard\intbl\qc {270}\cell
\pard\intbl\qc {1,720}\cell
\pard\intbl\qc {3,390}\cell
\pard\intbl\qc {4,610}\cell
\row
\trowd\clbrdrr\brdrs\cellx690\clbrdrr\brdrs\cellx1380\clbrdrr\brdrs\cellx2070\clbrdrr\brdrs\cellx2760\clbrdrr\brdrs\cellx3450\clbrdrr\brdrs\cellx4140\clbrdrr\brdrs\cellx4830\clbrdrr\brdrs\cellx5520\clbrdrr\brdrs\cellx6210\cellx6900
\pard\intbl\qc {Output}\cell
\pard\intbl\qc {110}\cell
\pard\intbl\qc {370}\cell
\pard\intbl\qc {720}\cell
\pard\intbl\qc {1,190}\cell
\pard\intbl\qc {Output}\cell
\pard\intbl\qc {170}\cell
\pard\intbl\qc {1,280}\cell
\pard\intbl\qc {2,010}\cell
\pard\intbl\qc {3,340}\cell
\row
\trowd\clbrdrb\brdrs\clbrdrr\brdrs\cellx690\clbrdrb\brdrs\clbrdrr\brdrs\cellx1380\clbrdrb\brdrs\clbrdrr\brdrs\cellx2070\clbrdrb\brdrs\clbrdrr\brdrs\cellx2760\clbrdrb\brdrs\clbrdrr\brdrs\cellx3450\clbrdrb\brdrs\clbrdrr\brdrs\cellx4140\clbrdrb\brdrs\clbrdrr\brdrs\cellx4830\clbrdrb\brdrs\clbrdrr\brdrs\cellx5520\clbrdrb\brdrs\clbrdrr\brdrs\cellx6210\clbrdrb\brdrs\cellx6900
\pard\intbl\qc {# of API calls}\cell
\pard\intbl\qc {7(0.5)}\cell
\pard\intbl\qc {7(0.5)}\cell
\pard\intbl\qc {7(0.5)}\cell
\pard\intbl\qc {7(0.5)}\cell
\pard\intbl\qc {# of API calls}\cell
\pard\intbl\qc {8(0.05)}\cell
\pard\intbl\qc {8(0.05)}\cell
\pard\intbl\qc {8(0.05)}\cell
\pard\intbl\qc {8(0.05)}\cell
\row
}  \par
}}{{\pard\qj\sl240\slmult1 \sb480 \fi0 \par
\pard\qc\sl240\slmult1 \fi0 {Table {\*\bkmkstart BMtable_workload_summary_hn}3{\*\bkmkend BMtable_workload_summary_hn}: Summary of Input, Output, and Number of API Calls for Hidden Markov model and N-body physics}{\field{\*\fldinst TC "3 Summary of Input, Output, and Number of API Calls for Hidden Markov model and N-body physics" \\f t}{\fldrslt }}\par
{\pard\qc\sl240\slmult1 \fi0 \par
\trowd\clmgf\clbrdrt\brdrs\clbrdrb\brdrs\clbrdrr\brdrs\cellx690\clmrg\clbrdrt\brdrs\clbrdrb\brdrs\cellx1380\clmrg\clbrdrt\brdrs\clbrdrb\brdrs\cellx2070\clmrg\clbrdrt\brdrs\clbrdrb\brdrs\cellx2760\clmrg\clbrdrt\brdrs\clbrdrb\brdrs\clbrdrr\brdrs\cellx3450\clmgf\clbrdrt\brdrs\clbrdrb\brdrs\clbrdrr\brdrs\cellx4140\clmrg\clbrdrt\brdrs\clbrdrb\brdrs\cellx4830\clmrg\clbrdrt\brdrs\clbrdrb\brdrs\cellx5520\clmrg\clbrdrt\brdrs\clbrdrb\brdrs\cellx6210\clmrg\clbrdrt\brdrs\clbrdrb\brdrs\cellx6900
\pard\intbl\qc {Hidden Markov model}\cell
\pard\intbl{}\cell
\pard\intbl{}\cell
\pard\intbl{}\cell
\pard\intbl{}\cell
\pard\intbl\qc {{\i N}-body physics}\cell
\pard\intbl{}\cell
\pard\intbl{}\cell
\pard\intbl{}\cell
\pard\intbl{}\cell
\row
\trowd\clbrdrb\brdrs\clbrdrr\brdrs\cellx690\clbrdrb\brdrs\clbrdrr\brdrs\cellx1380\clbrdrb\brdrs\clbrdrr\brdrs\cellx2070\clbrdrb\brdrs\clbrdrr\brdrs\cellx2760\clbrdrb\brdrs\clbrdrr\brdrs\cellx3450\clbrdrb\brdrs\clbrdrr\brdrs\cellx4140\clbrdrb\brdrs\clbrdrr\brdrs\cellx4830\clbrdrb\brdrs\clbrdrr\brdrs\cellx5520\clbrdrb\brdrs\clbrdrr\brdrs\cellx6210\clbrdrb\brdrs\cellx6900
\pard\intbl\qc {number of states}\cell
\pard\intbl\qc {288}\cell
\pard\intbl\qc {640}\cell
\pard\intbl\qc {928}\cell
\pard\intbl\qc {1,216}\cell
\pard\intbl\qc {number of iterations}\cell
\pard\intbl\qc {5}\cell
\pard\intbl\qc {10}\cell
\pard\intbl\qc {50}\cell
\pard\intbl\qc {100}\cell
\row
\trowd\clbrdrb\brdrs\clbrdrr\brdrs\cellx690\clbrdrb\brdrs\clbrdrr\brdrs\cellx1380\clbrdrb\brdrs\clbrdrr\brdrs\cellx2070\clbrdrb\brdrs\clbrdrr\brdrs\cellx2760\clbrdrb\brdrs\clbrdrr\brdrs\cellx3450\clbrdrb\brdrs\clbrdrr\brdrs\cellx4140\clbrdrb\brdrs\clbrdrr\brdrs\cellx4830\clbrdrb\brdrs\clbrdrr\brdrs\cellx5520\clbrdrb\brdrs\clbrdrr\brdrs\cellx6210\clbrdrb\brdrs\cellx6900
\pard\intbl\qc {Input}\cell
\pard\intbl\qc {290}\cell
\pard\intbl\qc {1,470}\cell
\pard\intbl\qc {3,200}\cell
\pard\intbl\qc {5,500}\cell
\pard\intbl\qc {Input}\cell
\pard\intbl\qc {15}\cell
\pard\intbl\qc {15}\cell
\pard\intbl\qc {15}\cell
\pard\intbl\qc {15}\cell
\row
\trowd\clbrdrb\brdrs\clbrdrr\brdrs\cellx690\clbrdrb\brdrs\clbrdrr\brdrs\cellx1380\clbrdrb\brdrs\clbrdrr\brdrs\cellx2070\clbrdrb\brdrs\clbrdrr\brdrs\cellx2760\clbrdrb\brdrs\clbrdrr\brdrs\cellx3450\clbrdrb\brdrs\clbrdrr\brdrs\cellx4140\clbrdrb\brdrs\clbrdrr\brdrs\cellx4830\clbrdrb\brdrs\clbrdrr\brdrs\cellx5520\clbrdrb\brdrs\clbrdrr\brdrs\cellx6210\clbrdrb\brdrs\cellx6900
\pard\intbl\qc {Output}\cell
\pard\intbl\qc {78}\cell
\pard\intbl\qc {108}\cell
\pard\intbl\qc {110}\cell
\pard\intbl\qc {119}\cell
\pard\intbl\qc {Output}\cell
\pard\intbl\qc {30}\cell
\pard\intbl\qc {30}\cell
\pard\intbl\qc {30}\cell
\pard\intbl\qc {30}\cell
\row
\trowd\clbrdrb\brdrs\clbrdrr\brdrs\cellx690\clbrdrb\brdrs\clbrdrr\brdrs\cellx1380\clbrdrb\brdrs\clbrdrr\brdrs\cellx2070\clbrdrb\brdrs\clbrdrr\brdrs\cellx2760\clbrdrb\brdrs\clbrdrr\brdrs\cellx3450\clbrdrb\brdrs\clbrdrr\brdrs\cellx4140\clbrdrb\brdrs\clbrdrr\brdrs\cellx4830\clbrdrb\brdrs\clbrdrr\brdrs\cellx5520\clbrdrb\brdrs\clbrdrr\brdrs\cellx6210\clbrdrb\brdrs\cellx6900
\pard\intbl\qc {# of API calls}\cell
\pard\intbl\qc {1000(66)}\cell
\pard\intbl\qc {1000(66)}\cell
\pard\intbl\qc {1000(66)}\cell
\pard\intbl\qc {1000(66)}\cell
\pard\intbl\qc {# of API calls}\cell
\pard\intbl\qc {20(0.16)}\cell
\pard\intbl\qc {40(0.32)}\cell
\pard\intbl\qc {200(1.6)}\cell
\pard\intbl\qc {400(3.2)}\cell
\row
}  \par
}}\pard\qj\sl240\slmult1 \sb360 \fi0 {\s4\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMcharacter_ctoc}3.1{\*\bkmkend BMcharacter_ctoc}  Computation to Communication Ratio\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 In this section, I present computation to communication ratio for each workload for the given size of data and network configurations. As we expected, it is evident that for all workloads, computation to communication ratio with local area networks is higher than other network configurations, since the data transfer time becomes shorter by higher network bandwidth from local area networks. On the contrary to local area networks, the cases of Amazon EC2 have the lowest computation to communication ratio because of its most restricted network conditions. As shown in Figure\~{\field{\*\fldinst{\lang1024 REF BMfig_ctoc_sobelfilter \\* MERGEFORMAT }}{\fldrslt{?}}} and\~{\field{\*\fldinst{\lang1024 REF BMfig_ctoc_hmm \\* MERGEFORMAT }}{\fldrslt{?}}}, however, computation to communication ratios for Sobelfilter and hidden Markov model are relatively uniform regardless of the size of data, while those for matrix multiplication and {\scaps0\b0\i N}-body physics increase as the size of data or the number of iterations increases. This is because that the volume of computation for Sobelfilter and hidden Markov model is linearly proportional to the amount of data to be processed, while that for matrix multiplication and {\scaps0\b0\i N}-body physics increases exponentially as the size of data or the number of iteration increases, which means that the benefits from offloading such workloads to more powerful resources also increase exponentially. Therefore, I expect it is highly likely that more gain is available from offloading matrix multiplication and {\i N}-body physics as the size of data increases. {\par
\pard\qj\sl240\slmult1 \sb240 \fi360 \par
\pard\qc\sl240\slmult1 \fi0 {file=figs/ctoc\\s\\do5({\fs16 s})obelfilter.eps, width=5.0in} \par
\pard\qc\sl240\slmult1 \fi0 {Figure {\*\bkmkstart BMfig_ctoc_sobelfilter}3{\*\bkmkend BMfig_ctoc_sobelfilter}: Computation to Communicaiton Ratio for Sobelfilter}{\field{\*\fldinst TC "3 Computation to Communicaiton Ratio for Sobelfilter" \\f f}{\fldrslt }}\par
}{\pard\qj\sl240\slmult1 \sb480 \fi0 \par
\pard\qc\sl240\slmult1 \fi0 {file=figs/ctoc\\s\\do5({\fs16 m})atrix.eps, width=5.0in} \par
\pard\qc\sl240\slmult1 \fi0 {Figure {\*\bkmkstart BMfig_ctoc_matrix}4{\*\bkmkend BMfig_ctoc_matrix}: Computation to Communication Ratio for Matrix Multiplication}{\field{\*\fldinst TC "4 Computation to Communication Ratio for Matrix Multiplication" \\f f}{\fldrslt }}\par
}{\pard\qj\sl240\slmult1 \sb480 \fi0 \par
\pard\qc\sl240\slmult1 \fi0 {file=figs/ctoc\\s\\do5({\fs16 h})mm.eps, width=5.0in} \par
\pard\qc\sl240\slmult1 \fi0 {Figure {\*\bkmkstart BMfig_ctoc_hmm}5{\*\bkmkend BMfig_ctoc_hmm}: Computation to Communication Ratio for Hidden Markov Model}{\field{\*\fldinst TC "5 Computation to Communication Ratio for Hidden Markov Model" \\f f}{\fldrslt }}\par
}{\pard\qj\sl240\slmult1 \sb480 \fi0 \par
\pard\qc\sl240\slmult1 \fi0 {file=figs/ctoc\\s\\do5({\fs16 n})body.eps, width=5.0in} \par
\pard\qc\sl240\slmult1 \fi0 {Figure {\*\bkmkstart BMfig_ctoc_nbody}6{\*\bkmkend BMfig_ctoc_nbody}: Computation to Communication Ratio for {\scaps0\b0\i N}-body Physics}{\field{\*\fldinst TC "6 Computation to Communication Ratio for {\scaps0\b0\i N}-body Physics" \\f f}{\fldrslt }}\par
}\pard\qj\sl240\slmult1 \sb360 \fi0 {\s4\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMcharacter_perf}3.2{\*\bkmkend BMcharacter_perf}  Offloading Performance\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 First of all, for Sobelfilter, I observed better performance from only a few cases where offloading 1440{{{\f6\'B4}}}810 and 1920{{{\f6\'B4}}}1080 of the image size to remote resources located in local area network than local processing in Figure\~{\field{\*\fldinst{\lang1024 REF BMfig_time_sobelfilter \\* MERGEFORMAT }}{\fldrslt{?}}}. In fact, Sobelfilter has lower computation to communication ratios that other workloads ranging from 0.28 to 11.68 in the experimental setup which indicates it is less computation-intensive (i.e. more communication-intensive) workload. Especially, the cases of offloading to the resources located in campus network and Amazon EC2 cluster instance whose computation to communication ratio is fairly low have the worst performance in Sobelfilter. Also, I observed that better performance comes from more powerful computing power by comparing CPU only-installed server and GPU installed server in the same network configuration. In contrast to Sobelfilter, in all the cases of matrix multiplication except for 160{{{\f6\'B4}}}320 of matrix size, offloading is much faster than local processing showing the speed-up which ranges from 1.2{{{\f6\'B4}}} to 9.2{{{\f6\'B4}}} as shown in Figure\~{\field{\*\fldinst{\lang1024 REF BMfig_time_matrix \\* MERGEFORMAT }}{\fldrslt{?}}}. As mentioned in previous section, computation to communication ratios of matrix multiplication with the experimental setup range from 0.41 to 97.42 which means that matrix multiplication is more computation-intensive workload than Sobelfilter. Therefore, it is highly likely that matrix multiplication is able to gain more from offloading to the remote resources with more powerful computing capabilities. In fact, the computation for matrix multiplication has higher complexity than Sobelfilter (the computation for matrix multiplication is {\scaps0\b0\i O}({{\field{\*\fldinst{ EQ {{\i n}\\s\\up5({\fs16 3})}}}{\fldrslt }}}) while Sobelfilter is {\scaps0\b0\i O}({{\field{\*\fldinst{ EQ {{\i n}\\s\\up5({\fs16 2})}}}{\fldrslt }}})). For the smallest matrix size of the setup, 160{{{\f6\'B4}}}320, however, I observed similar results as 1440{{{\f6\'B4}}}810 and 1920{{{\f6\'B4}}}1080 of image size in Sobelfilter where only offloading to resources in local area networks has better performance than local processing. With the results of Sobelfilter and matrix multiplication, it is evident that as computation to communication ratio becomes higher, it is possible to gain more from offloading.\par
\pard\qj\sl240\slmult1 \fi0 Interestingly, in the cases of hidden Markov model, the extremely worst performance is shown when the workload is offloaded to Amazon EC2 GPU cluster which has the worst restrictions in terms of network latency and bandwidth. Though hidden Markov model has higher computation to communication ratio than Sobelfilter, the kernel for hidden Markov model is repeatedly executed requiring additional argument setups for each execution which incur more communication overhead as described in section\~{\field{\*\fldinst{\lang1024 REF BMcharacter_workloads \\* MERGEFORMAT }}{\fldrslt{?}}}. In the Amazon EC2 setup, packets are sent or received at higher latencies compared to the local area networks which impacts performance especially since the OpenCL-based offloading framework requires that each RPC call is acknowledged with a response from the server. Consequently, offloading to Amazon EC2 GPU cluster, which has the highest latency among the experimental setups, takes the longest time. However, though {\scaps0\b0\i N}-body physics has the similar programming flow as hidden Markov model, all the cases of offloading have better performance than local processing because it is the most computation-intensive workload among the experimental setup whose computation to communication ratios range from 40.36 to 29323.68. {\par
\pard\qj\sl240\slmult1 \sb240 \fi360 \par
\pard\qc\sl240\slmult1 \fi0 {file=figs/time\\s\\do5({\fs16 s})obelfilter.eps, width=5.0in} \par
\pard\qc\sl240\slmult1 \fi0 {Figure {\*\bkmkstart BMfig_time_sobelfilter}7{\*\bkmkend BMfig_time_sobelfilter}: Total Execution Time for Sobelfilter with Various Configurations}{\field{\*\fldinst TC "7 Total Execution Time for Sobelfilter with Various Configurations" \\f f}{\fldrslt }}\par
}{\pard\qj\sl240\slmult1 \sb480 \fi0 \par
\pard\qc\sl240\slmult1 \fi0 {file=figs/time\\s\\do5({\fs16 m})atrix.eps, width=5.0in} \par
\pard\qc\sl240\slmult1 \fi0 {Figure {\*\bkmkstart BMfig_time_matrix}8{\*\bkmkend BMfig_time_matrix}: Total Execution Time for Matrix Multiplication with Various Configurations}{\field{\*\fldinst TC "8 Total Execution Time for Matrix Multiplication with Various Configurations" \\f f}{\fldrslt }}\par
}{\pard\qj\sl240\slmult1 \sb480 \fi0 \par
\pard\qc\sl240\slmult1 \fi0 {file=figs/time\\s\\do5({\fs16 h})mm.eps, width=5.0in} \par
\pard\qc\sl240\slmult1 \fi0 {Figure {\*\bkmkstart BMfig_time_hmm}9{\*\bkmkend BMfig_time_hmm}: Total Execution Time for Hidden Markov Model with Various Configurations}{\field{\*\fldinst TC "9 Total Execution Time for Hidden Markov Model with Various Configurations" \\f f}{\fldrslt }}\par
}{\pard\qj\sl240\slmult1 \sb480 \fi0 \par
\pard\qc\sl240\slmult1 \fi0 {file=figs/time\\s\\do5({\fs16 n})body.eps, width=5.0in} \par
\pard\qc\sl240\slmult1 \fi0 {Figure {\*\bkmkstart BMfig_time_nbody}10{\*\bkmkend BMfig_time_nbody}: Total Execution Time for {\scaps0\b0\i N}-body Physics with Various Configurations}{\field{\*\fldinst TC "10 Total Execution Time for {\scaps0\b0\i N}-body Physics with Various Configurations" \\f f}{\fldrslt }}\par
}\pard\qj\sl240\slmult1 \sb360 \fi0 {\s4\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMcharacter_energy}3.3{\*\bkmkend BMcharacter_energy}  Energy Consumption\par
}{\pard\qj\sl240\slmult1 \sb300 \fi0 \par
\pard\qc\sl240\slmult1 \fi0 {file=figs/energy\\s\\do5({\fs16 s})obelfilter.eps, width=5.0in} \par
\pard\qc\sl240\slmult1 \fi0 {Figure {\*\bkmkstart BMfig_energy_sobelfilter}11{\*\bkmkend BMfig_energy_sobelfilter}: Energy Consumption for Sobelfilter with Various Configurations}{\field{\*\fldinst TC "11 Energy Consumption for Sobelfilter with Various Configurations" \\f f}{\fldrslt }}\par
}{\pard\qj\sl240\slmult1 \sb480 \fi0 \par
\pard\qc\sl240\slmult1 \fi0 {file=figs/energy\\s\\do5({\fs16 m})atrix.eps, width=5.0in} \par
\pard\qc\sl240\slmult1 \fi0 {Figure {\*\bkmkstart BMfig_energy_matrix}12{\*\bkmkend BMfig_energy_matrix}: Energy Consumption for Matrix Multiplication with Various Configurations}{\field{\*\fldinst TC "12 Energy Consumption for Matrix Multiplication with Various Configurations" \\f f}{\fldrslt }}\par
}{\pard\qj\sl240\slmult1 \sb480 \fi0 \par
\pard\qc\sl240\slmult1 \fi0 {file=figs/energy\\s\\do5({\fs16 h})mm.eps, width=5.0in} \par
\pard\qc\sl240\slmult1 \fi0 {Figure {\*\bkmkstart BMfig_energy_hmm}13{\*\bkmkend BMfig_energy_hmm}: Energy Consumption for Hidden Markov Model with Various Configurations}{\field{\*\fldinst TC "13 Energy Consumption for Hidden Markov Model with Various Configurations" \\f f}{\fldrslt }}\par
}{\pard\qj\sl240\slmult1 \sb480 \fi0 \par
\pard\qc\sl240\slmult1 \fi0 {file=figs/energy\\s\\do5({\fs16 n})body.eps, width=5.0in} \par
\pard\qc\sl240\slmult1 \fi0 {Figure {\*\bkmkstart BMfig_energy_nbody}14{\*\bkmkend BMfig_energy_nbody}: Energy Consumption for {\scaps0\b0\i N}-body Physics with Various Configurations}{\field{\*\fldinst TC "14 Energy Consumption for {\scaps0\b0\i N}-body Physics with Various Configurations" \\f f}{\fldrslt }}\par
}\pard\qj\sl240\slmult1 \sb240 \fi0 To profile energy consumption of the mobile device, I used PowerTutor\~powertutor which is an application for the variants of Android devices that displays the power consumed by major components such as GPU, network interfaces, LCD display, and GPS receiver.\par
\pard\qj\sl240\slmult1 \fi0 A similar pattern for energy consumption of the mobile device as the execution time is shown in Figure\~{\field{\*\fldinst{\lang1024 REF BMfig_energy_sobelfilter \\* MERGEFORMAT }}{\fldrslt{?}}},\~{\field{\*\fldinst{\lang1024 REF BMfig_energy_matrix \\* MERGEFORMAT }}{\fldrslt{?}}},\~{\field{\*\fldinst{\lang1024 REF BMfig_energy_hmm \\* MERGEFORMAT }}{\fldrslt{?}}}, and\~{\field{\*\fldinst{\lang1024 REF BMfig_energy_nbody \\* MERGEFORMAT }}{\fldrslt{?}}}. As computation to communication value is higher, it is likely that offloading is more beneficial than local processing. However, it is worth noting that though some cases for Sobelfilter showed the benefits from offloading in terms of the execution time, offloading consumes more than local processing as shown in Figure 3(a). This dissimilar result comes from the discrepancy in the amount of power consumed by CPU and the Wi-Fi networking card. According to the measurement data profiled by PowerTutor, CPU consumes 200{{{\f6\'7E}}}220{\scaps0\b0\i mW} per second in active mode, while the Wi-Fi networking card consumes about 710{{{\f6\'7E}}}720{\scaps0\b0\i mW} per second in high power mode. For that reason, even though offloading is faster than local processing, it is possible that offloading consumes more energy than local processing. However, in matrix multiplication and {\scaps0\b0\i N}-body physics which result in extremely high speed-up by offloading, it is observed that offloading also improves energy implication as shown in Figure\~{\field{\*\fldinst{\lang1024 REF BMfig_energy_matrix \\* MERGEFORMAT }}{\fldrslt{?}}} and\~{\field{\*\fldinst{\lang1024 REF BMfig_energy_nbody \\* MERGEFORMAT }}{\fldrslt{?}}}. \par
\pard\qj\sl240\slmult1 \sb240 \fi0 {\s3\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMcharacter_summary}4{\*\bkmkend BMcharacter_summary}  Summary\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 In this section, I analyzed the behavior of the mobile offloading framework in terms of the offloading performance and energy implication of the mobile devices with respect to characteristics of workloads and environmental factors such as network conditions or the computing capabilities of remote resources. In order to characterize the workloads, I used computation to communication ratio calculated by the local processing time divided by the time for data transfer. Furthermore, as part of the workload characterization effort, I developed the OpenCL-based remote offloading framework by broadening the scope of heterogeneity to include a new kind of platform component: the computing capabilities on remote resources. I accomplish this by extending the well-defined hardware-level offloading standard, OpenCL framework to support the various range of remote computing elements over the network using the regular TCP/IP network stack. Also, I configured both local and wide area networks in which the various computing capabilities are deployed to evaluate the impact of environmental factors into the offloading performance.\par
\pard\qj\sl240\slmult1 \fi0 According to the analysis, the benefits and the costs of the remote offloading depend on computation to communication ratio as well as the programming flow related to how the client and the server interact each other. In fact, as computation to communication ratio becomes higher, the performance improvement and the conservation of energy consumption also increase. Moreover, in the cases of hidden Markov model and {\scaps0\b0\i N}-body physics, we observed that the programming flow is also important to analyze the behavior of the remote offloading for the mobile platforms. I believe that the numerical results presented in this section guides the basis of designing the intelligent runtime offloading scheduler which is described in next section.\par
\page{}\pard\qj\sl240\slmult1 \fi0 {\s2\ql\sb240\sa60\keepn\f13\b\fs40 Chapter {\*\bkmkstart BMchap_scheduler}5{\*\bkmkend BMchap_scheduler}\par
\pard\qj\sl240\slmult1 \sb240 \fi0 MACHINE LEARNING-BASED RUNTIME SCHEDULER\par
\par\par}\pard\qj\sl240\slmult1 \sb60 \fi0 Rapid enhancements in computing capabilities of mobile platforms have been driving the increased adopting and use of mobile computing platforms by increasing numbers of users. Today\rquote s mobile platforms are able to deliver capabilities that are close to those of non-mobile platforms such as desktops or workstations. For instance, a mobile phone equipped with a GPU core is able to achieve approximately 10GFLOPS/Watt of compute-power, which is identical as a 4-core desktop with GPU\~soyata. Despite of these significant advancements, mobile platforms remain significantly limited by resources such as memory size, storage capacity, and especially battery lifespan. To alleviate the problem of the resource limitations in mobile platforms, computation offloading techniques have been proposed as a way to extend the capabilities of mobile platforms to more powerful resources. These may include personal computers, servers, or even public cloud resource over the network\~snarf.\par
\pard\qj\sl240\slmult1 \fi0 However, the benefits from these systems can vary due to different requirements for data transfer among various types of mobile applications and dynamic network conditions including latency and bandwidth. As a result, offloading is not always beneficial, and poor offloading decisions can result in the degradation of performance or energy consumption. Therefore, offloading frameworks need to consider the scheduling or workloads onto remote or local processing resources adaptively, as a function of network conditions and application requirements.\par
\pard\qj\sl240\slmult1 \fi0 In this section, I address these challenges by considering machine learning techniques for a runtime adaptive scheduler for mobile offloading framework. Machine learning technique is a branch of artificial intelligence through which a system can learn from previous data and adapt to unseen situations dynamically. By applying machine learning techniques to the remote offloading scheduling problem, a scheduler can be automatically trained from previous offloading behaviors and make decisions on whether the mobile workload should be offloaded or executed locally informed by past behavior and current conditions. There have been a number of related studies proposing adaptive offloading mechanisms for mobile platforms. To the best of our knowledge, this work is the first to systematically study machine learning techniques for a mobile offloading scheduler. To this end, I utilized the OpenCL-based offloading framework that I explained in section\~{\field{\*\fldinst{\lang1024 REF BMchap_offloading \\* MERGEFORMAT }}{\fldrslt{?}}} and detailed measurement results under various network conditions and mobile applications shown in section\~{\field{\*\fldinst{\lang1024 REF BMchap_character \\* MERGEFORMAT }}{\fldrslt{?}}}.\par
\pard\qj\sl240\slmult1 \fi0 One of the contributions described in section\~{\field{\*\fldinst{\lang1024 REF BMchap_character \\* MERGEFORMAT }}{\fldrslt{?}}} is the combination of dynamic network conditions and application requirements into {\scaps0\b0\i Computation to Communication ratio}, which considers the local processing time for the workload, the amount of data transfer, and network bandwidth. The computation to communication ratio is a composite measurement which coalesces three dynamic features into one parameter, and is used as an attribute of the machine learning technique. For the evaluation on the feasibility of applying machine learning techniques to the adaptive scheduling problem, I utilized {\scaps0\b0\i Weka}\~weka, a Java-based open source package.\par
\pard\qj\sl240\slmult1 \fi0 After investigating the scheduling accuracy of several machine learning algorithms using Weka, I choose a few machine learning algorithms which have relatively high scheduling accuracy to implement an {\scaps0\b0\i offline} offloading scheduler. Further, by taking the complexity and scheduling performance into account, I select Instance-Based Learning algorithm for an {\scaps0\b0\i online} scheduler for mobile offloading framework. In the evaluation, I show that although Instance-Based Learning online offloading scheduler is fairly simple and has low overhead, it provides performance advantages over non-adaptive schedulers for mobile offloading system.\par
\pard\qj\sl240\slmult1 \sb240 \fi0 {\s3\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMscheduler_relatedwork}1{\*\bkmkend BMscheduler_relatedwork}  Related Works\par
}\pard\qj\sl240\slmult1 \sb180 \fi0 {\s4\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMscheduler_adaptive}1.1{\*\bkmkend BMscheduler_adaptive}  Adaptive Mobile Offloading\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 Many studies have considered adaptive mobile offloading to provide performance improvements and energy savings for mobile platforms. In\~xiaohui, the authors focus on relieving the memory limitation of a mobile device by dynamically making the offloading decision with Offloading Inference Engine (OLIE) which is based on the fuzzy control model. In particular, OLIE profiles the available memory size of a mobile device and network bandwidth, and maps them into the offloading decision specifications by the application developer, such that when the current condition matches any specified rule, an offloading action is triggered. In MAUI\~maui, the authors assume that offloading is always preferable to local processing; however, it depends on three factors to determine which methods should be offloaded to the remote server: the device\rquote s energy consumption characteristics, the program characteristics, and the network characteristics. It established the relationship between CPU utilization and the energy consumption of the mobile device and profiled the state transfer requirement, runtime duration, and the number of CPU cycles. Specifically, MAUI used the lightweight throughput measurement to profile network condition. With these information, MAUI schedules the local execution or offloading at the beginning of the application to conserve the energy consumption of the mobile platform. In\~shigeru, a prediction model for the performance of distributed mobile applications is evaluated through a sample image processing application (i.e. face detection). The prediction model heuristic uses linear functions to approximate the time for local, remote execution and data transfer. The server updates these functions using least-squares method, and returns the updated heuristic linear functions to the client so that those updated functions are used for the performance comparison between local processing and offloading. Kovachev et al.\~dejan propose a simple cost function of a service-based mobile cloud computing middleware for Android platforms under three restrictions: minimized memory usage, minimized energy usage, and minimized execution time.\par
\pard\qj\sl240\slmult1 \fi0 Mobile "Grid" systems, where mobile devices participate as resource users or providers, also consider scheduling techniques to improve the performance of the systems while tackling the resource constraints of mobile platforms. In\~waleed, a novel energy-aware scheduling is formulated and the level-based list scheduling heuristic is proposed for a Mobile wireless Ad hoc NETwork (MANET). The authors predefine the models for the task, the processor or the mobile device, and the cost function, and the scheduler tries to minimize the cost function by mapping each task to the resource through the level-based list scheduling heuristic.\par
\pard\qj\sl240\slmult1 \fi0 Ali et al.\~hesham present a mobile computing scheduling mechanism based on self-ranking algorithm. Each worker node in the mobile grid system measures the connectivity and utilization metrics and ranks itself using the ranking metric map from the system developer. Then, distributed tasks are assigned into high ranker nodes.\par
\pard\qj\sl240\slmult1 \fi0 The distributed scheduler for the mobile grid system has been proposed in\~karin. Each mobile node self-evaluates its current and future capabilities using static and dynamic device information such as connection quality, remaining capacity and processor utilization, and it does matchmaking the task requests similar to the Condor ClassAds.\par
\pard\qj\sl240\slmult1 \fi0 Although the above-mentioned approaches take into account dynamic parameters from the application level or the network level (i.e. CPU cycles, network bandwidth) to predict system performance and schedule the mobile workload execution, they still rely on predefined decision rules or cost models, preventing the scheduler from adapting to dynamic conditions during runtime. In contrast, in this paper I consider approaches that do not rely on any predefined specifications or prior knowledge of the mobile application. Instead, I consider machine learning techniques for adaptive runtime mobile offloading schedulers. Once trained with training data, the scheduler predicts the behavior of an incoming task when deciding on local or remote execution.\par
\pard\qj\sl240\slmult1 \sb120 \fi0 {\s4\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMscheduler_machine}1.2{\*\bkmkend BMscheduler_machine}  Machine Learning Techniques for Dynamic Scheduling Problems\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 Machine learning techniques have been used to address dynamic scheduling problems in various areas, such as heterogeneous computing platforms, grid computing systems as well as in data center. In\~zheng, machine learning techniques are used to provide a compiler-based, automatic and portable predictor for multi-core processors. In order to determine the best number of threads and scheduling policy, the authors used a feed-forward artificial neural network and a multi-class support vector machine model, respectively. Berral et al.\~josep propose an energy-aware data center through server consolidation by turning off idle servers with assistance from machine learning based scheduling. The scheduler predicts the future performance of the jobs and power consumption in the resulting job allocation using linear regression algorithms. The novel Adaptively Scheduled parallel R (ASpR) framework, which transparently parallelizes scripts in the popular R language, is presented in\~jiangtian. This framework uses artificial neural networks for the performance modeler which predicts task computation and data communication costs, and this modeler is used by the directed acyclic graph to determine an appropriate schedule.\par
\pard\qj\sl240\slmult1 \fi0 In this work, I further consider the appropriateness of adopting machine learning techniques to the scheduler of our mobile offloading framework with respect to the complexity to construct the predictor and ability to capture dynamic characteristics of mobile environments. \par
\pard\qj\sl240\slmult1 \sb240 \fi0 {\s3\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMscheduler_challenge}2{\*\bkmkend BMscheduler_challenge}  Challenge for Adaptive Scheduling for Remote Offloading Framework\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 In this section, I demonstrate the need for support from adaptive runtime schedulers by conducting an experiment in which I deploy the OpenCL-based remote offloading framework subject to various network configurations and collect measurements to show the performance disparity between different network configurations.\par
\pard\qj\sl240\slmult1 \fi0 In the experiments, I utilized an Android TabletPC equipped with 1GHz dual-core processor and 1GB RAM as a mobile client. In order to observe the impact of different network conditions on the offloading performance, I deployed a remote server equipped with GeForce GT 640 graphics card into three different network configurations: local area network, campus network, and Amazon EC2 instance. In the local area network, I connected the mobile client and the remote server through a wireless router supporting 802.11 b/g/n network standard. The campus network is used to represent a wide area network in which the mobile client and the remote server are involved in different networks: the mobile client connects to the campus wireless router and the server connects to the laboratory internal router. They communicate each other through multiple routers in the campus. I used an Amazon EC2 GPU cluster as another option for the remote server located in a wide area network, but for more restricted network condition than the campus network. Once again, Table\~{\field{\*\fldinst{\lang1024 REF BMtable_network_summary \\* MERGEFORMAT }}{\fldrslt{?}}} in section\~{\field{\*\fldinst{\lang1024 REF BMcharacter_setup \\* MERGEFORMAT }}{\fldrslt{?}}} summarizes the average and standard deviation of latency and bandwidth of the network configurations that I setup for the experiments.\par
\pard\qj\sl240\slmult1 \fi0 The benchmarks used in the experiment are Sobelfilter, floating-point matrix multiplication, Hidden Markov Model, and {\i N}-body physics provided by AMD APP SDK\~amd and Nvidia\~nvidia sample code. These execution kernels are used by a variety of applications in areas such as image processing, physics simulation, and mathematical modeling.\par
{\pard\qj\sl240\slmult1 \sb240 \fi360 \par
\pard\qc\sl240\slmult1 \fi0 {file=figs/challenge\\s\\do5({\fs16 n})etwork.eps, width=4.5in} \par
\pard\qc\sl240\slmult1 \fi0 {Figure {\*\bkmkstart BMfig_challenge_network}1{\*\bkmkend BMfig_challenge_network}: Performance for Sobelfilter with Various Network Configurations}{\field{\*\fldinst TC "1 Performance for Sobelfilter with Various Network Configurations" \\f f}{\fldrslt }}\par
}{\pard\qj\sl240\slmult1 \sb480 \fi0 \par
\pard\qc\sl240\slmult1 \fi0 {file=figs/challenge\\s\\do5({\fs16 w})orkload.eps, width=4.5in} \par
\pard\qc\sl240\slmult1 \fi0 {Figure {\*\bkmkstart BMfig_challenge_workload}2{\*\bkmkend BMfig_challenge_workload}: Performance Differences between Various Workloads}{\field{\*\fldinst TC "2 Performance Differences between Various Workloads" \\f f}{\fldrslt }}\par
}\pard\qj\sl240\slmult1 \sb240 \fi0 Figure\~{\field{\*\fldinst{\lang1024 REF BMfig_challenge_network \\* MERGEFORMAT }}{\fldrslt{?}}} and\~{\field{\*\fldinst{\lang1024 REF BMfig_challenge_workload \\* MERGEFORMAT }}{\fldrslt{?}}} shows the offloading performance in terms of the execution time compared to the case of local processing according to the data size and network configurations for four OpenCL execution kernels. As shown in Figure\~{\field{\*\fldinst{\lang1024 REF BMfig_challenge_network \\* MERGEFORMAT }}{\fldrslt{?}}}, for Sobelfilter, we observed that different network conditions result in significantly different offloading performance. Particularly, offloading to the remote server located in a local area network has better performance than local processing. In contrast, offloading to the remote servers located in the campus network and Amazon EC2 instance, where we have more restricted network conditions than a local area network, takes longer time than local processing. Figure\~{\field{\*\fldinst{\lang1024 REF BMfig_challenge_workload \\* MERGEFORMAT }}{\fldrslt{?}}} shows the performance difference among various execution workloads due to different computational requirements of workloads even though they process or offload the similar size of data ranging from 3.49MB to 3.74MB. For Sobelfilter, offloading to the GPU server located in LAN is only more beneficial than local processing. On the other hand, offloading floating-point matrix multiplication has always better performance than local processing in our setup due to heavier computational requirement of floating-point matrix multiplication. In fact, the computation complexity for floating-point matrix multiplication is {\scaps0\b0\i O}({{\field{\*\fldinst{ EQ {{\i n}\\s\\up5({\fs16 3})}}}{\fldrslt }}}) while that for Sobelfilter is {\scaps0\b0\i O}({{\field{\*\fldinst{ EQ {{\i n}\\s\\up5({\fs16 2})}}}{\fldrslt }}}).\par
\pard\qj\sl240\slmult1 \fi0 It is also worth noting that, for Hidden Markov Model, offloading to Amazon EC2 instance shows the worst performance among other cases. This is because that Hidden Markov Model requires extra communications between the mobile client and the remote server to setup additional arguments for workload execution. Packets are exchanged at higher latencies in the Amazon EC2 setup compared with a local area network, which causes performance degradation since our offloading framework requires that each RPC call is acknowledged with a response from the remote server. Consequently, offloading to Amazon EC2 GPU instance, which has the highest latency among our experimental setups, takes the longest time. These results show that there is variation in offloading performance between different network conditions and execution workloads. Accordingly, proper scheduling can have a significant impact on the offloading performance, and remote offloading framework requires the support from the runtime scheduler. \par
\pard\qj\sl240\slmult1 \sb240 \fi0 {\s3\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMscheduler_ml}3{\*\bkmkend BMscheduler_ml}  Machine Learning-Based Runtime Scheduler\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 In order to apply machine learning techniques to any decision making problems, it is first required to select a subset of relevant attributes. These need to comprehensively represent a set of problem instances in terms of internal and external conditions which have an effect on making a decision. In this section, I describe the attributes of machine learning techniques considered in this work, and how the proposed scheduler can extract these attributes. Then, using this subset of attributes, I investigate the scheduling accuracy of machine learning techniques using a dataset collected from experimental data using benchmark executions. By taking the text-based dataset as an input, I can train the classifier of various machine learning algorithms and examine the accuracy of the trained classifiers. {\par
\pard\qj\sl240\slmult1 \sb240 \fi360 \par
\pard\qc\sl240\slmult1 \fi0 {file=figs/scheduler.eps, width=5.5in} \par
\pard\qc\sl240\slmult1 \fi0 {Figure {\*\bkmkstart BMfig_scheduler}3{\*\bkmkend BMfig_scheduler}: Structure of Machine Learning-based Runtime Scheduler}{\field{\*\fldinst TC "3 Structure of Machine Learning-based Runtime Scheduler" \\f f}{\fldrslt }}\par
}\pard\qj\sl240\slmult1 \sb240 \fi0 Figure\~{\field{\*\fldinst{\lang1024 REF BMfig_scheduler \\* MERGEFORMAT }}{\fldrslt{?}}} illustrates the structure of the machine learning-based runtime scheduler and how it generates and uses the subset of attributes to make a decision for remote offloading framework. \par
\pard\qj\sl240\slmult1 \sb120 \fi0 {\s4\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMscheduler_attributes}3.1{\*\bkmkend BMscheduler_attributes}  Selection of Machine Learning Attributes\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 Since offloading performance can vary as a function of network conditions, the size of data to be processed, and computation requirements, the scheduler has to take these factors into account to make an accurate decision on offloading or local processing. I focus on four features to establish the subset of attributes which is the representation of the scheduling problem for the remote offloading framework:1) computation amount of the workload, 2) size of data, 3) network bandwidth, and 4) additional communication between the mobile client and the remote resource to setup extra arguments.\par
\pard\qj\sl240\slmult1 \fi0 {\i0\scaps0\b Local execution time({\scaps0\b0\i {t{{\field{\*\fldinst{ EQ {\\s\\do5({\fs16 {\i l}{\i o}{\i c}{\i a}{\i l}_{\i e}{\i x}{\i e}{\i c}{\i u}{\i t}{\i i}{\i o}{\i n}})}}}{\fldrslt }}}}}).} I regard the time for a workload to be executed in the mobile client locally as the computation amount. There are a variety of methods to measure the computation amount of the execution, such as counting the number of assembly instructions of loop iterations, some of which require additional assistance from the special hardware or compiler. Instead, in the proposed approach, runtime measurements are taken by the offloading framework as it executes the workload with the given data locally, and the scheduler profiles the execution time for the workload.\par
\pard\qj\sl240\slmult1 \fi0 {\i0\scaps0\b Data size to be transferred({\scaps0\b0\i {d{{\field{\*\fldinst{ EQ {\\s\\do5({\fs16 {\i t}{\i r}{\i a}{\i n}{\i s}{\i f}{\i e}{\i r}})}}}{\fldrslt }}}}}).} In addition to the computation cost of a workload depending on the size of the data, the data size also affects the communication cost to transfer the data from the mobile client to the remote resource. In the OpenCL-based remote offloading framework, the APIs for buffer management such as {\scaps0\b0\i clEnqueueWriteBuffer} and {\scaps0\b0\i clEnqueueReadBuffer} are used to profile the size of data to be transferred.\par
\pard\qj\sl240\slmult1 \fi0 {\i0\scaps0\b Network bandwidth({\scaps0\b0\i BW}).} I integrate the network bandwidth measurement into the offloading framework so that it is able to measure network bandwidth between the mobile client and the remote resource during runtime. In the implementation, network bandwidth is simply measured by the size of probing packets divided by the elapsed time to send those packets\~bandwidth.\par
\pard\qj\sl240\slmult1 \fi0 {\i0\scaps0\b Number of the invocations for argument setup({\scaps0\b0\i {n{{\field{\*\fldinst{ EQ {\\s\\do5({\fs16 {\i a}{\i r}{\i g}{\i u}{\i s}{\i e}{\i t}})}}}{\fldrslt }}}}}).} I count the number of the invocations of the specific OpenCL API called {\scaps0\b0\i clSetKernelArgs}, which causes additional communication overhead between the client and the remote resource to setup the extra arguments for kernel executions in addition to the primary data setup. The reason why I distinguish communications between main data transfer and additional arguments setup is that, though the latter incurs minor amount of data, it can cause significant communication costs due to protocol round-trip messages between the client and the remote resource.\par
\pard\qj\sl240\slmult1 \fi0 Not that, rather than considering the local execution time, the size of data transfer, and network bandwidth as individual attributes separately, I use {\scaps0\b0\i Computation to Communication ratio} in which three features are merged into on attribute as Equation\~{\field{\*\fldinst{\lang1024 REF BMequ_ctoc \\* MERGEFORMAT }}{\fldrslt{?}}}. {\par\par
\pard\tqc\tx3450\tqr\tx6900\tab {\field{\*\fldinst{ EQ {{\pard\qj\sl240\slmult1 \fi0 \qc [Sorry. Ignored {\plain\f16\\begin\{split\} ... \\end\{split\}}]} {\*\bkmkstart BMequ_ctoc}{\*\bkmkend BMequ_ctoc} }}}{\fldrslt }}\tab{\b0 (1)}\par
}\pard\qj\sl240\slmult1 \fi0 where {\scaps0\b0\i t{{\field{\*\fldinst{ EQ {\\s\\do5({\fs16 {\i d}{\i a}{\i t}{\i a}_{\i t}{\i r}{\i a}{\i n}{\i s}{\i f}{\i e}{\i r}})}}}{\fldrslt }}}} is the time for data transfer. Thus, in this work, computation to communication ratio is a composite measurement which combines three dynamic features into one parameter. As a result, the proposed machine learning-based classifier accepts two attributes: computation to communication ratio({\scaps0\b0\i CtoC}) and the number of invocations of {\scaps0\b0\i clSetKernelArgs}({\scaps0\b0\i n{{\field{\*\fldinst{ EQ {\\s\\do5({\fs16 {\i a}{\i r}{\i g}{\i s}{\i e}{\i t}})}}}{\fldrslt }}}}) to make a decision on scheduling a new workload(i.e. local processing or remote offloading).\par
\pard\qj\sl240\slmult1 \sb120 \fi0 {\s4\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMscheduler_accuracy}3.2{\*\bkmkend BMscheduler_accuracy}  Scheduling Accuracy of Machine Learning techniques\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 In this subsection, I investigate the scheduling accuracy of the machine learning-based scheduler for remote offloading framework. First of all, by running the OpenCL-based offloading framework into the experimental setup with various network configurations, execution kernels, and data sizes as described in section\~{\field{\*\fldinst{\lang1024 REF BMcharacter_methodology \\* MERGEFORMAT }}{\fldrslt{?}}}, I gathered a total 640 data instances to train and test the classifiers of various machine learning algorithms. Each data instance means one execution of the offloading framework, and consists of \{{\scaps0\b0\i d{{\field{\*\fldinst{ EQ {\\s\\do5({\fs16 {\i t}{\i r}{\i a}{\i n}{\i s}{\i f}{\i e}{\i r}})}}}{\fldrslt }}}}, {\scaps0\b0\i BW}, and {\scaps0\b0\i n{{\field{\*\fldinst{ EQ {\\s\\do5({\fs16 {\i a}{\i r}{\i g}{\i s}{\i e}{\i t}})}}}{\fldrslt }}}}\}. Next, I aggregate collected data instances to create the training and test datasets, each consisting of a 3-tuple, {{\scaps0\b0\i CtoC}, {\scaps0\b0\i n{{\field{\*\fldinst{ EQ {\\s\\do5({\fs16 {\i a}{\i r}{\i g}{\i s}{\i e}{\i t}})}}}{\fldrslt }}}} ; {\scaps0\b0\i label}}, with two attributes explained in section\~{\field{\*\fldinst{\lang1024 REF BMscheduler_attributes \\* MERGEFORMAT }}{\fldrslt{?}}} and the label which is a decision on {\scaps0\b0\i offload} or {\scaps0\b0\i local}. {\par
\pard\qj\sl240\slmult1 \sb240 \fi360 \par
\pard\qc\sl240\slmult1 \fi0 {file=figs/scheduling\\s\\do5({\fs16 a})ccuracy.eps, width=4.5in} \par
\pard\qc\sl240\slmult1 \fi0 {Figure {\*\bkmkstart BMfig_scheduling_accuracy}4{\*\bkmkend BMfig_scheduling_accuracy}: Offloading Scheduling Accuracy of Various Machine Learning Algorithms}{\field{\*\fldinst TC "4 Offloading Scheduling Accuracy of Various Machine Learning Algorithms" \\f f}{\fldrslt }}\par
}\pard\qj\sl240\slmult1 \sb240 \fi0 Then I labeled each data instance by comparing offloading performance to the local execution in terms of the execution time. For example, if offloading Sobelfilter with a 1920{{{\f6\'B4}}}1080 image into a machine with a GPU in LAN takes a shorter time than local processing, the instance is labeled as {\i offload}. In the collected dataset, 65% of instances are labeled as {\i offload}. Note that it is possible to use another performance metric: mobile device\rquote s energy consumption, such that each instance can be labeled based on energy consumption between remote offloading and local processing. Lastly, I separated the collected dataset with 70% of them for the training dataset and the rest for the test dataset, so they have the identical distribution for instance properties.\par
\pard\qj\sl240\slmult1 \fi0 Using the training dataset, I trained the classifiers of various categories of machine learning algorithms such as Decision Tree, Bayesian Networks, Instance-Based Learning, and Perceptron-Based Learning. In the training phase, Weka takes the text-based training dataset as the input and automatically generates the classifier associated with each machine learning algorithm. Once each classifier is trained, I tested the accuracy of the trained classifier with the test dataset by observing whether the trained classifier labels each instance in the test dataset correctly or not.\par
\pard\qj\sl240\slmult1 \fi0 Figure\~{\field{\*\fldinst{\lang1024 REF BMfig_scheduling_accuracy \\* MERGEFORMAT }}{\fldrslt{?}}} shows the scheduling accuracy of various machine learning algorithms. In this evaluation, the scheduling accuracy is calculated through the number of the correct decisions made by the classifier out of the test dataset. I observed that two Instance-Based Learning classifiers performed the most accurate prediction, showing greater than 90% of the scheduling accuracy. The basis for the classification of Instance-Based Learning is the instances database, where previously seen instances are stored. Instead of building the explicit classifier as other machine learning algorithms, Instance-Based Learning compares a new problem instance with the stored instances in the database to select {\scaps0\b0\i k} most similar instances from the database and votes the majority of the selected instances to predict the label of the new problem instance. {\scaps0\b0\i k} set to 1 (IB1) and 3 (IB3) as higher values showed the same performance. The classification of probabilistic machine learning techniques such as Bayesian Networks is based on the statistics of attributes of the previous instances such as the mean and the variance values. Thus it is possible that probability-based machine learning algorithms overlook the edge of previously seen instances, which causes the performance degradation for the prediction problem. In fact, Naive Bayes has the worst performance among machine learning algorithms used for the evaluation, showing 64.4% scheduling accuracy.\par
\pard\qj\sl240\slmult1 \sb240 \fi0 {\s3\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMscheduler_offline}4{\*\bkmkend BMscheduler_offline}  Performance Evaluation for Offline Scheduler\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 In this section, using the classifiers from various machine learning algorithms, I implement an offline offloading scheduler and evaluate the performance and penalty of the offline runtime schedulers.\par
\pard\qj\sl240\slmult1 \sb120 \fi0 {\s4\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMschedeulr_offline_setup}4.1{\*\bkmkend BMschedeulr_offline_setup}  Experimental Setup\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 Based on scheduling performance and algorithm complexity, we selected three machine learning algorithms: RandomTree, Instance-Based Learning and Rule-Based Learning, and built them onto our remote offloading framework for the offline runtime scheduler. For RandomTree, I used the classifier that Weka generated using the training dataset described in section\~{\field{\*\fldinst{\lang1024 REF BMscheduler_accuracy \\* MERGEFORMAT }}{\fldrslt{?}}}. Total depth of the tree is 101 and the scheduling accuracy simulated through Weka is 89.5%.\par
\pard\qj\sl240\slmult1 \fi0 Though I do not need any classifier for Instance-Based Learning algorithm, it is required to define the similarity between a new problem instance and previously stored instances in the database. To do this, I used Euclidean distance, which is common to measure the similarity for Instance-Based Learning\~instance. The closer the distance between instances is, the more similar they are. I stored the training dataset with 448 instances to build the database for the Instance-Based Learning algorithm. For simplicity, we use {\scaps0\b0\i k=1} for Instance-Based Learning algorithm. For Rule-Based Learning, we establish a simple rule based on computation to communication ratio threshold, in which the scheduler decides to offload the mobile computation only if computation to communication ratio is higher than the threshold. Based on our observation, it is most likely that the benefits from offloading are more promising when computation to communication ratio is higher than 1.5. For that reason, I setup the threshold with 1.5 and 3.\par
\pard\qj\sl240\slmult1 \fi0 Also, I emulate various network configurations in which the client and the server connect directly through a wireless router, but have 9 different network bandwidths ranging from 6.5MB/s to 0.3MB/s controlled by Traffic Control (TC)\~tc. TC is a network tool which provides functionalities to control network traffic by prioritizing network resources and using concepts of traffic classification, queue disciplines and quality of service (QoS). While setting different network bandwidths, we ran our offloading framework with four benchmark kernels 720 times (9 network bandwidths {{{\f6\'B4}}} 4 kernels {{{\f6\'B4}}} 4 data sizes {{{\f6\'B4}}} 5 repeats for average) per each offline scheduler.\par
\pard\qj\sl240\slmult1 \sb120 \fi0 {\s4\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMscheduler_offline_perf}4.2{\*\bkmkend BMscheduler_offline_perf}  Performance Comparison\par
}{\pard\qj\sl240\slmult1 \sb300 \fi0 \par
\pard\qc\sl240\slmult1 \fi0 {file=figs/offline\\s\\do5({\fs16 a})ccuracy.eps, width=5.5in} \par
\pard\qc\sl240\slmult1 \fi0 {Figure {\*\bkmkstart BMfig_offline_accuracy}5{\*\bkmkend BMfig_offline_accuracy}: Scheduling Accuracy of Offline Schedulers}{\field{\*\fldinst TC "5 Scheduling Accuracy of Offline Schedulers" \\f f}{\fldrslt }}\par
}\pard\qj\sl240\slmult1 \sb240 \fi0 Figure\~{\field{\*\fldinst{\lang1024 REF BMfig_offline_accuracy \\* MERGEFORMAT }}{\fldrslt{?}}} shows the scheduling accuracy for various machine learning algorithms with four benchmark kernels. Similarly as the result shown in Figure\~{\field{\*\fldinst{\lang1024 REF BMfig_scheduling_accuracy \\* MERGEFORMAT }}{\fldrslt{?}}}, I observed that Instance-Based Learning has the most accurate scheduling performance among various schedulers showing 92% of the scheduling accuracy. Even though in matrix multiplication and Hidden Markov Model, other machine learning algorithms have better performance than Instance- Based Learning, Instance-Based Learning shows the best performance on average. It is also observed that, even though the schedulers based on Rule-Based Learning algorithm consider only one attribute, computation to communication ratio, they have better performance than RandomTree. An interpretation is that the computation to communication ratio is a more dominant attribute than the number of argument setup, {\scaps0\b0\i n{{\field{\*\fldinst{ EQ {\\s\\do5({\fs16 {\i a}{\i r}{\i g}{\i s}{\i e}{\i t}})}}}{\fldrslt }}}}. Interestingly, for {\scaps0\b0\i N}-body physics, all machine learning algorithms show the perfect scheduling accuracy. It is because that computation to communication ratio for {\scaps0\b0\i N}-body physics is extremely high in our experimental setup so that it is easy for the scheduler to differentiate the conditions where offloading or the local execution for {\scaps0\b0\i N}-body physics is more beneficial than the other. In fact, offloading {\scaps0\b0\i N}-body physics always had better performance that the local execution in the experimental setup.\par
\pard\qj\sl240\slmult1 \fi0 Figure\~{\field{\*\fldinst{\lang1024 REF BMfig_penalty_time \\* MERGEFORMAT }}{\fldrslt{?}}} and\~{\field{\*\fldinst{\lang1024 REF BMfig_penalty_energy \\* MERGEFORMAT }}{\fldrslt{?}}} present the penalty for various schedulers normalized to the case of the oracle scheduler which always makes the right decision to offload or run locally as Equation\~{\field{\*\fldinst{\lang1024 REF BMequ_penalty \\* MERGEFORMAT }}{\fldrslt{?}}}. {\par
\pard\qj\sl240\slmult1 \sb240 \fi360 \par
\pard\qc\sl240\slmult1 \fi0 {file=figs/penalty\\s\\do5({\fs16 t})ime.eps, width=5.5in} \par
\pard\qc\sl240\slmult1 \fi0 {Figure {\*\bkmkstart BMfig_penalty_time}6{\*\bkmkend BMfig_penalty_time}: Penalty(Execution Time) Normalized to the Oracle Scheduler}{\field{\*\fldinst TC "6 Penalty(Execution Time) Normalized to the Oracle Scheduler" \\f f}{\fldrslt }}\par
}{\pard\qj\sl240\slmult1 \sb480 \fi0 \par
\pard\qc\sl240\slmult1 \fi0 {file=figs/penalty\\s\\do5({\fs16 e})nergy.eps, width=5.5in} \par
\pard\qc\sl240\slmult1 \fi0 {Figure {\*\bkmkstart BMfig_penalty_energy}7{\*\bkmkend BMfig_penalty_energy}: Penalty(Energy Consumption) Normalized to the Oracle Scheduler}{\field{\*\fldinst TC "7 Penalty(Energy Consumption) Normalized to the Oracle Scheduler" \\f f}{\fldrslt }}\par
}{\par\par
\pard\tqc\tx3450\tqr\tx6900\tab {\field{\*\fldinst{ EQ {{\pard\qj\sl240\slmult1 \sb240 \fi0 \qc [Sorry. Ignored {\plain\f16\\begin\{split\} ... \\end\{split\}}]} {\*\bkmkstart BMequ_penalty}{\*\bkmkend BMequ_penalty} }}}{\fldrslt }}\tab{\b0 (2)}\par
}\pard\qj\sl240\slmult1 \fi0 where {\scaps0\b0\i execution_time{{\field{\*\fldinst{ EQ {\\s\\do5({\fs16 {\i M}{\i L}})}}}{\fldrslt }}}} and {\scaps0\b0\i execution_time{{\field{\*\fldinst{ EQ {\\s\\do5({\fs16 {\i o}{\i r}{\i a}{\i c}{\i l}{\i e}})}}}{\fldrslt }}}} are the processing times of the workload scheduled by the machine learning-based scheduler and the oracle scheduler, respectively. For the penalty in terms of energy consumption, {\scaps0\b0\i execution_time{{\field{\*\fldinst{ EQ {\\s\\do5({\fs16 {\i M}{\i L}})}}}{\fldrslt }}}} and {\scaps0\b0\i execution_time{{\field{\*\fldinst{ EQ {\\s\\do5({\fs16 {\i o}{\i r}{\i a}{\i c}{\i l}{\i e}})}}}{\fldrslt }}}} should be replaced with the mobile device\rquote s energy consumption to execute or offload the workload scheduled by each scheduler. In this evaluation, the penalty implies the extra costs that the mobile device or user has to pay additionally over the oracle scheduler when the machine learning-based scheduler makes a wrong decision. To profile energy consumption of the mobile device, I used PowerTutor\~powertutor which is an application for the variants of Android devices that displays the power consumed by major components such as CPU, network interface, LCD display, and GPS receiver.\par
\pard\qj\sl240\slmult1 \fi0 As you can see, the Instance-Based Learning scheduler has the smallest penalty in terms of the execution time because it has the highest scheduling accuracy. For energy consumption, moreover, the Instance-Based Learning scheduler has a fairly small penalty compared with Rule-Based Learning scheduler. Note that, for Sobelfilter, the penalty in terms of both the execution time and energy consumption is lower than other execution kernels, because the gap of the execution time and energy consumption for Sobelfilter between offloading and the local execution is relatively small. Therefore, the penalty for Sobelfilter is less significant than the cases of other execution kernels when the scheduler makes a wrong decision on offloading or the local execution.\par
\pard\qj\sl240\slmult1 \sb240 \fi0 {\s3\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMscheduler_online}5{\*\bkmkend BMscheduler_online}  Online Runtime Scheduler\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 In the previous section, I demonstrated the offline runtime offloading scheduler based on various machine learning algorithms by illustrating the scheduling accuracy and the penalty with regard to the execution time and energy consumption of the mobile platforms. In this section, I explore the potential possibility and benefits of an online runtime scheduler for mobile offloading framework in which the online scheduler can be trained through the previous experiences automatically and adapt to the dynamic situation. \par
\pard\qj\sl240\slmult1 \sb120 \fi0 {\s4\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMscheduler_online_impl}5.1{\*\bkmkend BMscheduler_online_impl}  Implementation of the Online Offloading Scheduler\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 I first implemented the prototype of the online runtime scheduler based on the Instance-Based Learning algorithm for the mobile offloading framework. The reason why I chose the Instance-Based Learning algorithm for the online runtime scheduler is due to the simplicity of the algorithm and the ability to quickly apply newly seen data to its future decisions. Usually, other machine learning algorithms such as neural networks or linear regression have its own the classification model and it is required to be completely modified when a new instance data is added to the training dataset. However, Instance-Based Learning simply stores the new instance to the training dataset, and the new instance is used to predict a next coming problem instance along with previous stored instances. In addition to its simplicity, in the evaluation for the offline offloading scheduler, Instance-Based Learning showed the best performance among various machine learning algorithms I used for the evaluation.\par
\pard\qj\sl240\slmult1 \fi0 The following is the scheduling process of our prototype of the online scheduler. Once the application starts, the online scheduler executes the application locally at once to figure out the information which is required for profiling the workload such as the local execution time, the size of data, or the number of the invocations for argument setup. Then, the scheduler enters the training phase by unconditionally offloading the execution to the remote server {\scaps0\b0\i N} times, and each case is labeled according to the performance comparison between offloading and the local execution. The labeled instance is stored into the training database. For the prototype, we set {\scaps0\b0\i N} with 16. After the training phase, the scheduler starts the scheduling process by measuring the Euclidean distance between a new scheduling problem and the instances stored in the training database. When offloading is scheduled, the scheduler offloads the workload and measures the execution time for offloading. If offloading takes shorter than the local execution, then that instance is added to the training database as {\scaps0\b0\i offload}. On the other hand, it is stored as {\scaps0\b0\i local}.\par
\pard\qj\sl240\slmult1 \fi0 To update the training database, the scheduler keeps adding the new instance to the database without removing any previous stored instance until the database is full. Only if the database is full, the oldest instance will be replaced with the new one. For the implementation, I try to find the optimal number of instances for the database which covers as many cases as possible, but takes reasonably small memory space (less than 1MB) and time (less than 0.01sec) to schedule a new problem. I chose 5,000 instances database which requires only 0.1MB of memory. This memory usage occupies only less than 0.0007% of typical memory sizes of contemporary mobile devices (e.g 16GB or 32GB). Also, even though it takes 5{{{\f6\'7E}}}6msec to measure the Euclidean distance of the new scheduling problem with 5,000 instances, we believe that instance generalization or clustering techniques for the database such as\~domingos can help the scheduler significantly reduce the measurement time. \par
\pard\qj\sl240\slmult1 \sb120 \fi0 {\s4\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMscheduler_online_eval}5.2{\*\bkmkend BMscheduler_online_eval}  Evaluation for the Online Offloading Scheduler\par
}{\pard\qj\sl240\slmult1 \sb300 \fi0 \par
\pard\qc\sl240\slmult1 \fi0 {file=figs/online.eps, width=5.5in} \par
\pard\qc\sl240\slmult1 \fi0 {Figure {\*\bkmkstart BMfig_online}8{\*\bkmkend BMfig_online}: Adaptability of the Online Scheduler against Dynamic Network Conditions}{\field{\*\fldinst TC "8 Adaptability of the Online Scheduler against Dynamic Network Conditions" \\f f}{\fldrslt }}\par
}\pard\qj\sl240\slmult1 \sb240 \fi0 In order to evaluate the prototype of our online scheduler, I conducted an experiment in which we change network conditions and observe how well the scheduler learns and adapts to dynamic network conditions. In this experiment, the client and the remote server are directly connected through a wireless router and we controlled the network bandwidth between them using TC. Also, I used Sobelfilter for the offloaded execution kernel. Figure\~{\field{\*\fldinst{\lang1024 REF BMfig_online \\* MERGEFORMAT }}{\fldrslt{?}}} shows the ability to adapt the online scheduler to dynamic network conditions.\par
\pard\qj\sl240\slmult1 \fi360 During the training phase, we setup different network conditions where the scheduler is trained with three network bandwidths: 6.5MB/s, 1MB/s, and 0.5MB/s. After the training phase, the scheduler makes a decision on offloading or the local execution in six different network bandwidths as shown in Figure\~{\field{\*\fldinst{\lang1024 REF BMfig_online \\* MERGEFORMAT }}{\fldrslt{?}}}. As you can observe, the scheduler makes the correct decisions in dynamic network conditions except for 17th, 32nd, and 36th trial. These incorrect decisions are because network bandwidth is changed after the scheduler makes a decision. As a result, the actual cost for data transfer is different with what the scheduler predicts. Furthermore, even at the unseen conditions in the training phase such as 3MB/s or 0.3MB/s, the scheduler works correctly by making right decisions. Consequently, we observed the possibility and the potential benefits of machine learning-based online offloading scheduler in this experiment. \par
\pard\qj\sl240\slmult1 \sb240 \fi0 {\s3\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMscheduler_summary}6{\*\bkmkend BMscheduler_summary}  Summary\par
}\pard\qj\sl240\slmult1 \sb60 \fi0 In this section, I proposed machine learning-based runtime scheduler for mobile offloading framework. Before addressing the scheduling problem of the mobile offloading framework, I performed detailed measurement experiments under various network conditions and mobile applications to show the necessity and efficacy of an adaptive offloading mechanism.\par
\pard\qj\sl240\slmult1 \fi0 In order to examine the feasibility of applying machine learning techniques to the adaptive scheduling problem, I utilized Weka which is a Java-based open source package. To this end, I used computation to communication ratio, which represents the local processing time for the workload, the amount of data transfer, and network bandwidth, as an attribute of the machine learning technique. After investigating the scheduling accuracy of several machine learning algorithms using Weka, I choose a few machine learning algorithms which have relatively high scheduling accuracy to implement an offline offloading scheduler. In the evaluation, I showed that although Instance-Based Learning offloading scheduler is fairly simple and has low overhead, it provides performance advantages over non-adaptive scheduling policy or even another machine learning algorithm-based schedulers such as RandomTree or Rule-Based Learning. In fact, the scheduler based on Instance-Based Learning performed 7% better than RandomTree and 3% better than Rule-Based Learning.\par
\pard\qj\sl240\slmult1 \fi0 Furthermore, by taking the complexity and scheduling performance into account, I selected Instance-Based Learning algorithm for an online scheduler for mobile offloading framework. Using Instance-Based Learning online scheduler, I demonstrated the potential benefits and the ability of the online offloading scheduler to adapt into dynamic network conditions.\par
\page{}\pard\qj\sl240\slmult1 \fi0 {\s2\ql\sb240\sa60\keepn\f13\b\fs40 Chapter {\*\bkmkstart BMchap_summary}6{\*\bkmkend BMchap_summary}\par
\pard\qj\sl240\slmult1 \sb240 \fi0 SUMMARY AND FUTURE WORK\par
\par\par}\pard\qj\sl240\slmult1 \sb60 \fi0 This proposal presents a novel framework which enables remote workload offloading to trusted remote resources through the paradigm of extended hardware-layer heterogeneous computing environment. Thus, the proposed framework broadens the range of heterogeneous computing to remote resources in the network, where offloading services are dynamically discovered. The proposed approach accomplishes this by 1) extending the OpenCL framework to support remote offloading using the TCP/IP networking stack and 2) introducing the concept of Social Device Networks in which trusted peers\rquote  computing resources are aggregated through virtual private networks for the purpose of resource discovery and configuration. More specifically, the proposed framework is implemented as a wrapper library around the OpenCL API with identical interfaces of the original library. Furthermore, the wrapped library is integrated with a customized RPC-based service to support remote offloading. According to the decision from a runtime scheduler, each OpenCL API call can invoke the corresponding original API in the local OpenCL library, or it can be marshalled and sent to external resources for remote execution.\par
\pard\qj\sl240\slmult1 \fi0 Additionally, the proposed approach supports accessing resources beyond the local private network, broadening the accessibility to trusted remote resources across the Internet and the cloud. This is accomplished by utilizing a social peer-to-peer virtual private network, SocialVPN, and creating a social device network in which only trusted social peers\rquote  computing resources are involved. Also, the IP multicast-based resource discovery technique makes it possible for mobile devices to dynamically discover remote resources during runtime and to flexibly utilize computing capabilities of the remote resources. With the proposed framework, therefore, a mobile device dynamically discovers trusted remote resources in a social device network using the IP multicast-based resource discovery mechanism, transparently offloads mobile workloads onto selected remote resources, and efficiently manages its energy consumption.\par
\pard\qj\sl240\slmult1 \fi0 For the second contribution of this dissertation proposal, I analyzed the behavior of the mobile offloading framework in terms of the offloading performance and energy implication of the mobile device with respect to workload and resource characteristics such as the size of data transfer, workload complexity, network conditions or computing capabilities of remote resources. In order to characterize workloads, I utilized Computation to Communication ratio calculated by local processing time divided by time for data transfer. I configured both local and wide area networks in which various computing capabilities are deployed to evaluate the impact of resource characteristics into the offloading performance.\par
\pard\qj\sl240\slmult1 \fi0 According to the analysis, the benefits and costs of remote offloading depend on computation to communication ratio as well as additional communications to setup extra arguments for workload executions. In fact, as computation to communication ratio becomes higher, the performance improvement and the conservation of energy consumption also increase. Interestingly, in cases of hidden Markov model and {\scaps0\b0\i N}-body physics, we observed that additional communications between the client and server are also important to analyze the behavior of remote offloading for mobile platforms.\par
\pard\qj\sl240\slmult1 \fi0 Lastly, I proposed a machine learning-based runtime scheduler for mobile offloading framework. In order to examine the feasibility of applying machine learning techniques to adaptive scheduling problems for mobile offloading framework, I evaluated well-known machine learning algorithms using Weka, a Java-based open source package. To this end, I used computation to communication ratio, which represents the local processing time for the workload, the amount of data transfer, and network bandwidth, as attributes of the machine learning technique. After investigating the scheduling accuracy of several machine learning algorithms, I chose a few machine learning algorithms which have relatively high scheduling accuracy to implement offline offloading schedulers. In the evaluation, I showed that even though Instance-Based Learning offloading scheduler is fairly simple and has low overhead, it provides performance advantages over non-adaptive scheduling policy as well as other machine learning-based schedulers such as RandomTree or Rule-Based Learning. In fact, the scheduler based on Instance-Based Learning performed 7% better than RandomTree and 3% better than Rule-Based Learning. Furthermore, by taking the complexity and scheduling performance into account, I selected Instance-Based Learning algorithm for an online scheduler for mobile offloading framework. Using Instance-Based Learning online scheduler, I demonstrated the potential benefits and the ability of the online offloading scheduler to adapt to dynamic network conditions.\par
\pard\qj\sl240\slmult1 \fi0 Since the resource discovery of the proposed remote offloading framework is carried out on top of social device networks, the discovery scope can be limited within the user-defined virtual private network, and the remote resources, which are discovered through the resource discovery mechanism, enforce security and privacy in communication. Nevertheless, the resource selection among the discovered resources plays a critical role in quality of service for the application layer and user\rquote s mobility. In the current implementation, however, the process to select a remote resource is overly simple by employing only network latency as a main criterion for selecting the best resource. Hence, the current resource discovery technique will be further extended into the consideration of keeping track of a more complex set of conditions of multiple remote computing resources such as network latency, bandwidth, and computing capabilities of remote resources, and the provision of the most appropriate resource in accordance with network conditions and mobile application requirements.\par
\pard\qj\sl240\slmult1 \fi0 Also, the machine learning-based runtime scheduler will be modularized so that it provides well-defined APIs, and the proposed runtime scheduler can be plugged and played for various types of adaptive scheduling problems. As part of the modularization of the machine learning-based runtime scheduler, I am currently working on the online scheduler for Java-based on-demand code offloading system. \par
\pard\qj\sl240\slmult1 \sb240 \fi0 {\s3\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMsummary_selection}1{\*\bkmkend BMsummary_selection}  On-Demand Resource Discovery and Selection\par
}{\pard\qj\sl240\slmult1 \sb300 \fi0 \par
\pard\qc\sl240\slmult1 \fi0 {file=figs/ondemand.eps, width=4.0in} \par
\pard\qc\sl240\slmult1 \fi0 {Figure {\*\bkmkstart BMfig_ondemand}1{\*\bkmkend BMfig_ondemand}: Structure of On-Demand Resource Discovery and Selection Mechanism}{\field{\*\fldinst TC "1 Structure of On-Demand Resource Discovery and Selection Mechanism" \\f f}{\fldrslt }}\par
}\pard\qj\sl240\slmult1 \sb240 \fi0 Service discovery protocols make developers free from taking care of all possible interactions and states between devices at design time by including a layer of indirection to remote offloading framework\~feng. However, it is still challenging to define a discovery scope when the offloading framework is deployed in wide area environments rather than in local area environments such as enterprise networks protected by firewalls and managed by system administrators.\par
\pard\qj\sl240\slmult1 \fi0 The IP multicast-based resource discovery over Social Device Networks, which is accomplished through a peer-to-peer virtual private network, SocialVPN, can limit the discovery scope within the user-defined virtual private network, thus, the discovered remote resources enforce security and privacy in communication. Nevertheless, it is crucial that a proper policy for the resource selection needs to provide the most appropriate resource among the discovered remote resources in accordance with network conditions and application characteristics to offer quality of service for the application layer and the elasticity for the user\rquote s mobility. The default behavior of the current implementation of the resource discovery mechanism is to profile available remote resources and provide the one with the lowest network latency only when a workload is needed to be offloaded. As a result, the process to select a remote resource is overly simple by employing only network latency as a main criterion for selecting the best resource, and it is difficult to capture the sophisticated set of information for remote resources. Furthermore, in the mobile computing environment, as the physical location of mobile hosts can be changed from time to time and network access points might be changed as well, it is essential to take the mobility into account and it is required to have more fine-grained resource profiling and selection mechanism.\par
\pard\qj\sl240\slmult1 \fi0 As a next goal of the future work, I plan on extending the current resource discovery technique into a more fine-grained resource discovery technique in which a more complex set of conditions will be periodically monitored and the most appropriate remote resource will be selected in accordance with application characteristics and requirements. To this end, I propose a fine-grained on-demand resource discovery and selection approach that can automatically make the resource selection which would yield the best performance without any user interventions. Even though there exist many ways to achieve the automatic resource selection, a utility functions-based approach can be used due to its expressiveness of preference and ability of self-optimization. Utility functions provide a natural and advantageous framework for achieving self-optimization in autonomic computing by representing user or application preference\~utility. For that reason, utility functions have been applied to autonomic systems, particularly, to the resource allocation in high performance computing environments\~william. Thus, by implementing utility functions-based on-demand resource selection with various features such as latency, bandwidth, or computing capabilities of remote resources, and by setting different weight values of each feature for utility functions in accordance with application characteristics and requirements, it is expected that the proposed resource discovery technique provides a more flexible capability of selecting the appropriate resource according to application expectations and preference priority.\par
\pard\qj\sl240\slmult1 \fi0 Figure\~{\field{\*\fldinst{\lang1024 REF BMfig_ondemand \\* MERGEFORMAT }}{\fldrslt{?}}} shows the structure of the proposed on-demand resource discovery and selection mechanism. The IP multicast-based resource discovery profiles remote resources involved in a social device network by periodically sending multicast request packets with a certain interval. It is worth noting that the interval for request packets is adaptively changed according to network conditions. If network conditions such as latency or bandwidth have high variance, the request rate increases so that the information for remote resources can be updated sufficiently often. Otherwise, the resource discovery stays in a low request rate to save energy consumption of a mobile client. Whenever the machine learning-based runtime scheduler requests a remote resource, the utility functions based resource selector delivers the remote resource which has the highest utility value to the runtime scheduler along with its information such as network performance or computing capabilities.\par
{\pard\qj\sl240\slmult1 \sb240 \fi360 \par
\pard\qc\sl240\slmult1 \fi0 {file=figs/futurework.eps, width=5.5in} \par
\pard\qc\sl240\slmult1 \fi0 {Figure {\*\bkmkstart BMfig_futurework}2{\*\bkmkend BMfig_futurework}: On-Demand Resource Discovery and Selection}{\field{\*\fldinst TC "2 On-Demand Resource Discovery and Selection" \\f f}{\fldrslt }}\par
}\pard\qj\sl240\slmult1 \sb240 \fi0 Figure\~{\field{\*\fldinst{\lang1024 REF BMfig_futurework \\* MERGEFORMAT }}{\fldrslt{?}}} illustrates a typical use case scenario of the utility functions-based on-demand resource discovery and selection technique. First, Alice considers two different types of mobile applications, first application recognizes the face of human being from a video clip and second application is a Go game in which the mobile user plays the game with virtual competitor. For first application, it is required that large amounts of data should be transferred from the mobile device to the remote resource as the application processes through a series of images. Therefore, it might be most important that network bandwidth between the mobile device and the remote resource should be high enough to support the seamless service of the face recognition. As a result, the on-demand resource selection mechanism will select Alice\rquote s laptop as a target resource which has the highest bandwidth among three remote resources. For the Go game, on the other hand, the mobile device transfers only several hundreds of bytes of data which present current positions of stones and the remote resource calculates the optimal next stone position with certain complex algorithms such as Monte Carlo or pattern matching. Consequently, the resource selection mechanism selects the most powerful computing resource, Alice\rquote s VM on EC2, even though it has the worst network performance.\par
\pard\qj\sl240\slmult1 \fi0 The proposed on-demand resource selection based on utility functions will be evaluated through real deployment into various network configurations which have different network performance in terms of latency and bandwidth. Furthermore, in order to examine the ability of on-demand resource selection, I will utilize different types of mobile applications which have different characteristics and requirements such as image processing, character recognition, and gaming systems.\par
\pard\qj\sl240\slmult1 \sb240 \fi0 {\s3\ql\sb240\sa60\keepn\f13\b\fs32 {\*\bkmkstart BMsummary_scheduler}2{\*\bkmkend BMsummary_scheduler}  Performance Characterization of ML-based Runtime Schedulers\par
}{\pard\qj\sl240\slmult1 \sb300 \fi0 \par
\pard\qc\sl240\slmult1 \fi0 {file=figs/mlscheduler.eps, width=4.0in} \par
\pard\qc\sl240\slmult1 \fi0 {Figure {\*\bkmkstart BMfig_mlscheduler}3{\*\bkmkend BMfig_mlscheduler}: Modularization of the Machine Learning-based Runtime Scheduler}{\field{\*\fldinst TC "3 Modularization of the Machine Learning-based Runtime Scheduler" \\f f}{\fldrslt }}\par
}\pard\qj\sl240\slmult1 \sb240 \fi0 In the evaluation of the machine learning-based runtime scheduler, it has been shown that several machine learning algorithms such as Instance-Based Learning or RandomTree have potential benefits and the ability of the on/offline offloading scheduler to adapt into dynamic network conditions and application requirements. However, the current prototype of the machine learning-based runtime scheduler has been implemented upon the OpenCL-based remote offloading framework and it uses the OpenCL-dependent parameter, the number of the invocations for argument setup API (i.e. clSetkernelArgs({\scaps0\b0\i n{{\field{\*\fldinst{ EQ {\\s\\do5({\fs16 {\i a}{\i r}{\i g}{\i s}{\i e}{\i t}})}}}{\fldrslt }}}})), as an attribute of machine learning algorithms. Therefore, it is impractical to apply the proposed machine learning-based runtime scheduler into different types of remote offloading systems which have a different set of features to represent the scheduling problem. For a next goal for the machine learning-based runtime scheduler, I will generalize and modularize the components of the machine learning-based runtime scheduler which is shown in Figure\~{\field{\*\fldinst{\lang1024 REF BMfig_scheduler \\* MERGEFORMAT }}{\fldrslt{?}}} and provide the well-defined APIs so that various offloading frameworks can easily take the advantages of the machine learning-based runtime scheduler. For example, API, {\scaps0\b0\i getAttributes()} gives developers the functionality which takes the profiled information for network conditions and application characteristics as input parameters and generates a set of attributes used by the machine learning-based classifier to make the final decision to offloading or local execution. Also, it is possible to implement several types of ML-based classifiers with different machine learning algorithms so that the developer can build the attribute-dependent runtime scheduler. Figure\~{\field{\*\fldinst{\lang1024 REF BMfig_mlscheduler \\* MERGEFORMAT }}{\fldrslt{?}}} details the modularization of the machine learning-based runtime scheduler. First, the machine learning-based decision maker takes the information of the selected resource and makes a decision the workload is offloaded or executed in local. Next, the offloading performance will be evaluated by the performance evaluator and the database updater will update the performance database. Based on the updated database, the decision maker updater will update the machine learning-based decision maker.\par
\pard\qj\sl240\slmult1 \fi0 One of the contributions of the modularization of the machine learning-based runtime scheduler is to characterize the performance, benefits and overhead of different types of machine learning algorithms in mobile offloading online scheduler scenario. As presented in section\~{\field{\*\fldinst{\lang1024 REF BMchap_scheduler \\* MERGEFORMAT }}{\fldrslt{?}}}, while the machine learning techniques show the feasibility of applying to the runtime scheduler for mobile offloading framework, different types of machine learning algorithms vary considerably in the performance and costs for the runtime offloading scheduler. It is effective to identify the performance, benefits and costs of different types of machine learning algorithms in online scheduler scenario. To my best knowledge, it is first to consider the characterization of machine learning algorithms for the runtime mobile offloading scheduler.\par
\pard\qj\sl240\slmult1 \fi0 As part of the modularization of the machine learning-based runtime scheduler, I am currently working for transplanting the modules of the machine learning-based runtime scheduler into on-demand Android Java code offloading framework\~dpartner. In the current implementation for on-demand Android Java code offloading framework, the task to scheduler code offloading is completely dependent on a mobile user\rquote s control. In fact, the on-demand Java code offloading system depends on the web-based user interface called RuntimeManager in which a mobile user schedules each offloadable class by dragging and dropping the badges representing offloadable classes into local and remote area in RuntimeManager. As a result, this scheduling policy of the Java on-demand offloading system does not consider network conditions and application requirements at all and the mobile user can make wrong scheduling which might cause poor application performance and unnecessary energy consumption.\par
\pard\qj\sl240\slmult1 \fi0 By placing the module of the machine learning-based runtime scheduler and implementing the online scheduler based on machine learning techniques into the client side of Android Java code offloading framework, it will dynamically profile network conditions and application requirements, and user-transparently decide offloading or local execution in accordance with the previous offloading performance.\par
\page{}\pard\qj\sl240\slmult1 \fi360 { Heungsik Eom was born in Incheon, South Korea in 1980. He attended Dongincheon High School and graduated in 1999. Heungsik Eom received his bachelor\rquote s degree in 2006 and master\rquote s degree in 2008 in Electronic Engineering from Dongguk University, Seoul, South Korea, respectively. After he was accepted to the Department of Electrical and Computer Engineering at University in 2008, he obtained another master\rquote s degree in 2010 and a doctoral degree in 2014.\par
\pard\qj\sl240\slmult1 \fi360 At the University of Florida, he joined the Advanced Computing and Information Systems (ACIS) Laboratory to pursue his doctoral research under the advisement of Dr. Renato Figueiredo. His research interests involve self-clustering systems for networked resources, computation offloading for mobile platforms as well as Peer-to-Peer systems.\par
}}}
